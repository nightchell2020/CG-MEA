{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74515d5-5f25-477b-9a97-ed1f9b6d4c51",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate\n",
    "\n",
    "This notebook evaluates the network trained previous notebooks and analyzes the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2666f-6b4b-4748-9dca-2884347a1f0f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df587e38-b1a9-42c8-9b50-680200a9bec7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb41bf8-1713-4228-b799-0e1c87545a16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pprint\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom package\n",
    "from datasets.caueeg_script import build_dataset_for_train\n",
    "import models\n",
    "from train.evaluate import check_accuracy\n",
    "from train.evaluate import check_accuracy_extended\n",
    "from train.evaluate import check_accuracy_extended_debug\n",
    "from train.evaluate import check_accuracy_multicrop\n",
    "from train.evaluate import check_accuracy_multicrop_extended\n",
    "from train.evaluate import calculate_confusion_matrix\n",
    "from train.visualize import draw_roc_curve\n",
    "from train.visualize import draw_confusion\n",
    "from train.visualize import draw_class_wise_metrics\n",
    "from train.visualize import draw_error_table\n",
    "from train.visualize import annotate_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e3f39-ed55-422c-b47f-376691cbeed6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a62fa4-aa53-49d5-8ab4-2e2aaf7b65d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Load the configuration used during the train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4f853-3cad-4b01-8ee8-44ac9e49adbe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_pool = [{'name': 'lo88puq7'}, {'name': 'gjkysllw'}, \n",
    "              {'name': 'v301o425'}, {'name': 'lm6j0kiz'}, \n",
    "              {'name': 'jjk1coy8'}]\n",
    "\n",
    "for model in model_pool:\n",
    "    ckpt = torch.load(os.path.join(r'E:\\CAUEEG\\checkpoint', model['name'], 'checkpoint.pt'), map_location=device)\n",
    "    \n",
    "    model['model_state'] = ckpt['model_state']\n",
    "    model['config'] = ckpt['config']\n",
    "    model['optimizer_state'] = ckpt['optimizer_state']\n",
    "    model['scheduler_state'] = ckpt['scheduler_state']\n",
    "    \n",
    "    print(model['name'])\n",
    "    # print(model['config'])\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85edd9-772b-4525-8acc-095850716b4e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Load the target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88946aa-6d3c-4b14-869e-b558783b8148",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = config['generator'](**config).to(device)\n",
    "for model in model_pool:\n",
    "    print(model['name'])\n",
    "    model['model'] = hydra.utils.instantiate(model['config']).to(device)\n",
    "\n",
    "    if model['config'].get('ddp', False):\n",
    "        model_state_ddp = deepcopy(model['model_state'])\n",
    "        model_state = OrderedDict()\n",
    "        for k, v in model_state_ddp.items():\n",
    "            name = k[7:] # remove 'module.' of DataParallel/DistributedDataParallel\n",
    "            model_state[name] = v\n",
    "        model['model_state'] = model_state\n",
    "        \n",
    "    model['model'].load_state_dict(model['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751ab89-fe12-4e98-94e1-9a616ba08670",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Evaluate the model and analyze the performance by the crop timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b02db-da4e-4e65-830b-ba4214ff1bd7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa65ab-c9ce-4182-97c7-1adb2ca651d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repeat = 1\n",
    "for model in model_pool:\n",
    "    model['config'].pop('cwd', 0)\n",
    "    model['config']['ddp'] = False\n",
    "    model['config']['crop_timing_analysis'] = True\n",
    "    model['config']['device'] = device\n",
    "\n",
    "    #  model['config']['seq_length'] = 16000 # TODO\n",
    "    #  model['config']['test_crop_multiple'] = 32 # TODO\n",
    "    \n",
    "    repeat = max(repeat, round(200 / model['config']['crop_multiple']))\n",
    "\n",
    "print(repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc5f3a-1321-406c-ad0f-d8cb6f587bd1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ce735-4ffd-49a6-a9d1-51584df94390",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for model in model_pool:\n",
    "    train_loader, val_loader, test_loader, multicrop_test_loader = build_dataset_for_train(model['config'], verbose=True)\n",
    "    \n",
    "    model['train_loader'] = train_loader\n",
    "    model['val_loader'] = val_loader\n",
    "    model['test_loader'] = test_loader\n",
    "    model['multicrop_test_loader'] = multicrop_test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d83768-0b92-41f8-9276-08f444ae3006",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcce1ba-ccb0-4b77-aebc-b6ee966ab28c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for model in model_pool:\n",
    "    _ = check_accuracy_extended_debug(model['model'], model['val_loader'], \n",
    "                                      model['config']['preprocess_test'], model['config'], repeat=repeat)\n",
    "    model['val_acc'] = _[0]\n",
    "    model['val_score'] = _[1]\n",
    "    model['val_target'] = _[2]\n",
    "    model['val_confusion'] = _[3]\n",
    "    model['val_error_table'] = _[4]\n",
    "    model['val_crop_timing'] = _[5]\n",
    "\n",
    "    print(model['val_acc'])\n",
    "    \n",
    "for model in model_pool:\n",
    "    draw_roc_curve(model['val_score'], model['val_target'], model['config']['class_label_to_name'], use_wandb=False)\n",
    "    draw_confusion(model['val_confusion'], model['config']['class_label_to_name'], use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb56c9a-31e2-4bf4-8d19-5aa918390e59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_val_score = np.zeros_like(model['val_score'])\n",
    "\n",
    "for model in model_pool:\n",
    "    ensemble_val_score += model['val_score'] / len(model_pool)\n",
    "\n",
    "# accuracy\n",
    "pred = ensemble_val_score.argmax(axis=-1)\n",
    "ensemble_val_acc = 100.0 * (pred.squeeze() == model['val_target']).sum() / pred.shape[0]\n",
    "print(ensemble_val_acc)\n",
    "\n",
    "# confusion matrix\n",
    "ensemble_val_confusion = calculate_confusion_matrix(pred, model['val_target'], \n",
    "                                                    num_classes=model['config']['out_dims'])\n",
    "\n",
    "# draw\n",
    "draw_roc_curve(ensemble_val_score, model['val_target'], model['config']['class_label_to_name'], use_wandb=False)\n",
    "draw_confusion(ensemble_val_confusion, model['config']['class_label_to_name'], use_wandb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0423d5-d26b-4a02-84a9-b4577b81cf82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572abf6-6c8d-429b-b18b-9eafbbe2b5b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for model in model_pool:\n",
    "    _ = check_accuracy_extended_debug(model['model'], model['test_loader'], \n",
    "                                      model['config']['preprocess_test'], model['config'], repeat=repeat)\n",
    "    model['test_acc'] = _[0]\n",
    "    model['test_score'] = _[1]\n",
    "    model['test_target'] = _[2]\n",
    "    model['test_confusion'] = _[3]\n",
    "    model['test_error_table'] = _[4]\n",
    "    model['test_crop_timing'] = _[5]\n",
    "\n",
    "    print(model['test_acc'])\n",
    "    \n",
    "for model in model_pool:\n",
    "    draw_roc_curve(model['test_score'], model['test_target'], model['config']['class_label_to_name'], use_wandb=False)\n",
    "    draw_confusion(model['test_confusion'], model['config']['class_label_to_name'], use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313d640-f44f-4564-bd0c-cbd20f02ba8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_test_score = np.zeros_like(model['test_score'])\n",
    "\n",
    "for model in model_pool:\n",
    "    ensemble_test_score += model['test_score'] / len(model_pool)\n",
    "\n",
    "# accuracy\n",
    "pred = ensemble_test_score.argmax(axis=-1)\n",
    "ensemble_test_acc = 100.0 * (pred.squeeze() == model['test_target']).sum() / pred.shape[0]\n",
    "print(ensemble_test_acc)\n",
    "\n",
    "# confusion matrix\n",
    "ensemble_test_confusion = calculate_confusion_matrix(pred, model['test_target'], \n",
    "                                                    num_classes=model['config']['out_dims'])\n",
    "\n",
    "# draw\n",
    "draw_roc_curve(ensemble_test_score, model['test_target'], model['config']['class_label_to_name'], use_wandb=False)\n",
    "draw_confusion(ensemble_test_confusion, model['config']['class_label_to_name'], use_wandb=False)\n",
    "draw_class_wise_metrics(ensemble_test_confusion, model['config']['class_label_to_name'], use_wandb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d240c-1264-478a-88c3-5481ac520cd1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test set (with test-time augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e8af7-e6cb-4b25-89b0-c115296be5e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in model_pool:\n",
    "    _ = check_accuracy_extended_debug(model['model'], model['multicrop_test_loader'], \n",
    "                                      model['config']['preprocess_test'], model['config'], repeat=repeat)\n",
    "    model['multicrop_test_acc'] = _[0]\n",
    "    model['multicrop_test_score'] = _[1]\n",
    "    model['multicrop_test_target'] = _[2]\n",
    "    model['multicrop_test_confusion'] = _[3]\n",
    "    model['multicrop_test_error_table'] = _[4]\n",
    "    model['multicrop_test_crop_timing'] = _[5]\n",
    "\n",
    "    print(model['multicrop_test_acc'])\n",
    "    \n",
    "for model in model_pool:\n",
    "    draw_roc_curve(model['multicrop_test_score'], model['multicrop_test_target'], model['config']['class_label_to_name'], use_wandb=False)\n",
    "    draw_confusion(model['multicrop_test_confusion'], model['config']['class_label_to_name'], use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d78977-2a2a-4a49-8802-3884c19aee07",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_multicrop_test_score = np.zeros_like(model['multicrop_test_score'])\n",
    "\n",
    "for model in model_pool:\n",
    "    ensemble_multicrop_test_score += model['multicrop_test_score'] / len(model_pool)\n",
    "\n",
    "# accuracy\n",
    "pred = ensemble_multicrop_test_score.argmax(axis=-1)\n",
    "ensemble_multicrop_test_acc = 100.0 * (pred.squeeze() == model['multicrop_test_target']).sum() / pred.shape[0]\n",
    "print(ensemble_multicrop_test_acc)\n",
    "\n",
    "# confusion matrix\n",
    "ensemble_multicrop_test_confusion = calculate_confusion_matrix(pred, model['multicrop_test_target'], \n",
    "                                                    num_classes=model['config']['out_dims'])\n",
    "\n",
    "# draw\n",
    "draw_roc_curve(ensemble_multicrop_test_score, model['multicrop_test_target'], model['config']['class_label_to_name'], use_wandb=False)\n",
    "draw_confusion(ensemble_multicrop_test_confusion, model['config']['class_label_to_name'], use_wandb=False)\n",
    "draw_class_wise_metrics(ensemble_multicrop_test_confusion, model['config']['class_label_to_name'], use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23811ac5-4a94-4d8a-b4fb-68161557a87d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
