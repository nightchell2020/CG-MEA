{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Finetune Masked-AutoEncoder\n",
    "\n",
    "- Finetune the deep network after pretraining the self-supervised learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GitHub\\eeg_analysis\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict\n",
    "from cycler import cycler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "# custom package\n",
    "from run_train import check_device_env\n",
    "from run_train import set_seed\n",
    "from run_train import compose_dataset\n",
    "from run_train import generate_model\n",
    "from train.ssl_train_script import ssl_train_script\n",
    "from train.train_script import train_script\n",
    "from models.utils import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Specify the dataset, model, and train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_model_path = 'local/checkpoint/'\n",
    "pre_model_name = 'eg6s5fay'  # '5xlos421'\n",
    "device = 'cuda:0'\n",
    "ddp_size = 1\n",
    "\n",
    "############\n",
    "# Artifact #\n",
    "############\n",
    "art_config = dict()\n",
    "art_config['project'] = 'caueeg-mae-artifact'\n",
    "art_config['use_wandb'] = False\n",
    "art_config['pre_model'] = pre_model_name\n",
    "art_config['device'] = device\n",
    "art_config['ddp_size'] = ddp_size\n",
    "\n",
    "art_config[\"art_dropout\"] = 0.1\n",
    "# art_config[\"art_use_age\"] = \"conv\"  # \"conv\", \"embedding\", \"no\" \n",
    "art_config[\"art_out_activation\"] = \"softplus\"  # \"none\", \"relu\", \"softplus\"   ####################################\n",
    "art_config[\"art_filter_list\"] = (3, 3, 3, 3, 3) ################################################################################################################################################\n",
    "art_config[\"art_resnet\"] = True  ################################################################################################################################################\n",
    "art_config[\"art_loss_type\"] = \"mse\"\n",
    "\n",
    "art_config['total_samples'] = 5.0e+6\n",
    "art_config['search_lr'] = False\n",
    "art_config['base_lr'] = 1e-4\n",
    "art_config['lr_scheduler_type'] = 'cosine_decay_with_warmup_half'\n",
    "\n",
    "art_config[\"warmup_min\"] = 150\n",
    "art_config[\"num_history\"] = 50\n",
    "art_config['save_model'] = False\n",
    "\n",
    "##################\n",
    "# Classification #\n",
    "##################\n",
    "finetune_config = dict()\n",
    "finetune_config['project'] = 'caueeg-mae-artifact-finetune'\n",
    "finetune_config['use_wandb'] = True\n",
    "finetune_config['pre_model'] = pre_model_name\n",
    "finetune_config['device'] = device\n",
    "finetune_config['ddp_size'] = ddp_size\n",
    "finetune_config[\"task\"] = \"dementia\"\n",
    "finetune_config[\"out_dims\"] = 3\n",
    "\n",
    "finetune_config[\"global_pool\"] = True\n",
    "finetune_config[\"fc_stages\"] = 2\n",
    "finetune_config[\"dropout\"] = 0.1\n",
    "finetune_config[\"use_age\"] = \"conv\"  \n",
    "# finetune_config[\"mixup\"] = 0.3 ###\n",
    "# finetune_config[\"crop_length\"] = 8192*4  \n",
    "finetune_config[\"criterion\"] = \"multi-bce\"  # \"cross-entropy\", \"multi-bce\"\n",
    "\n",
    "finetune_config[\"tuning_type\"] = \"finetune\"  # \"finetune\", \"fc_stage\"\n",
    "finetune_config[\"layer_wise_lr\"] = True\n",
    "\n",
    "finetune_config['total_samples'] = 2.0e+6  ############################################################\n",
    "# finetune_config['base_lr'] = 1e-3\n",
    "finetune_config['search_lr'] = True\n",
    "finetune_config['lr_search_steps'] = 100\n",
    "finetune_config['lr_scheduler_type'] = 'cosine_decay_with_warmup_half'\n",
    "finetune_config[\"warmup_min\"] = 200\n",
    "finetune_config[\"num_history\"] = 50\n",
    "finetune_config['save_model'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crop_length': 5120, 'minibatch': 192, 'awgn': 0.003, 'awgn_age': 0.003, 'mgn': 0.003, 'mixup': 0.1, 'seed': 1710, 'art_patch_usage': {'type': 'drop_ends', 'value': 0.3}}\n",
      "{'crop_length': 5120, 'minibatch': 192, 'awgn': 0.003, 'awgn_age': 0.003, 'mgn': 0.003, 'mixup': 0.1, 'seed': 7711, 'art_patch_usage': {'type': 'drop_ends', 'value': 0.3}}\n",
      "{'crop_length': 5120, 'minibatch': 192, 'awgn': 0.003, 'awgn_age': 0.003, 'mgn': 0.003, 'mixup': 0.1, 'seed': 7171, 'art_patch_usage': {'type': 'drop_ends', 'value': 0.3}}\n"
     ]
    }
   ],
   "source": [
    "finetune_cycler = cycler(crop_length=[5120]) + cycler(minibatch=[192])\n",
    "finetune_cycler *= cycler(awgn=[0.003]) + cycler(awgn_age=[0.003]) + cycler(mgn=[0.003])\n",
    "finetune_cycler *= cycler(mixup=[0.1])\n",
    "finetune_cycler *= cycler(seed=[1710, 7711, 7171])\n",
    "# finetune_cycler *= cycler(fc_stages=[1, 3])\n",
    "finetune_cycler *= cycler(art_patch_usage=[{\"type\": \"drop_ends\", \"value\": 0.3},\n",
    "                                          ])\n",
    "for cyc in finetune_cycler:\n",
    "    print(cyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cu118\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Uncertainty Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EKG': 'O',\n",
      " '_target_': 'models.ssl.mae_1d.mae_1d_pre_b_e768_d512',\n",
      " 'activation': 'gelu',\n",
      " 'age_mean': tensor([71.3079]),\n",
      " 'age_std': tensor([9.6942]),\n",
      " 'awgn': 0.003,\n",
      " 'awgn_age': 0.001,\n",
      " 'base_lr': 0.001,\n",
      " 'class_label_to_name': ['Normal', 'Abnormal'],\n",
      " 'class_name_to_label': {'Abnormal': 1, 'Normal': 0},\n",
      " 'criterion': 'cross-entropy',\n",
      " 'crop_length': 2560,\n",
      " 'crop_multiple': 8,\n",
      " 'crop_timing_analysis': False,\n",
      " 'cwd': '',\n",
      " 'dataset_name': 'CAUEEG dataset',\n",
      " 'dataset_path': 'local/dataset/caueeg-dataset/',\n",
      " 'ddp': False,\n",
      " 'device': device(type='cuda'),\n",
      " 'draw_result': True,\n",
      " 'file_format': 'memmap',\n",
      " 'in_channels': 20,\n",
      " 'input_norm': 'datapoint',\n",
      " 'iterations': 390625,\n",
      " 'latency': 2000,\n",
      " 'load_event': False,\n",
      " 'lr_scheduler_type': 'cosine_decay_with_warmup_half',\n",
      " 'mask_ratio': 0.75,\n",
      " 'mgn': 0.003,\n",
      " 'minibatch': 256,\n",
      " 'minibatch_3090': 256,\n",
      " 'mixed_precision': True,\n",
      " 'mixup': 0.0,\n",
      " 'model': '1D-MAE-B',\n",
      " 'multi_batch_size': 32,\n",
      " 'norm_pix_loss': True,\n",
      " 'num_history': 500,\n",
      " 'num_params': 123492000,\n",
      " 'num_workers': 4,\n",
      " 'out_dims': 2,\n",
      " 'output_length': 161,\n",
      " 'patch_size': 8,\n",
      " 'photic': 'X',\n",
      " 'preprocess_test': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda'))\n",
      "  (1): EegResample(orig_freq=200, new_freq=100, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([71.3079]), std=tensor([9.6942]), eps=1e-08, std_eps=tensor([9.6942]))\n",
      "  (3): EegNormalizePerSignal(eps=1e-08)\n",
      "),\n",
      " 'preprocess_train': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda'))\n",
      "  (1): EegResample(orig_freq=200, new_freq=100, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([71.3079]), std=tensor([9.6942]), eps=1e-08, std_eps=tensor([9.6942]))\n",
      "  (3): EegAddGaussianNoiseAge(mean=0.0, std=0.001)\n",
      "  (4): EegNormalizePerSignal(eps=1e-08)\n",
      "  (5): EegMultiplicativeGaussianNoise(mean=0.0, std=0.003)\n",
      "  (6): EegAdditiveGaussianNoise(mean=0.0, std=0.003)\n",
      "),\n",
      " 'project': 'caueeg-mae',\n",
      " 'resample': 100,\n",
      " 'run_mode': 'train',\n",
      " 'sampling_rate': 200,\n",
      " 'save_model': True,\n",
      " 'search_lr': False,\n",
      " 'search_multiplier': 1.0,\n",
      " 'seed': 0,\n",
      " 'seq_length': 1280,\n",
      " 'signal_header': ['Fp1-AVG',\n",
      "                   'F3-AVG',\n",
      "                   'C3-AVG',\n",
      "                   'P3-AVG',\n",
      "                   'O1-AVG',\n",
      "                   'Fp2-AVG',\n",
      "                   'F4-AVG',\n",
      "                   'C4-AVG',\n",
      "                   'P4-AVG',\n",
      "                   'O2-AVG',\n",
      "                   'F7-AVG',\n",
      "                   'T3-AVG',\n",
      "                   'T5-AVG',\n",
      "                   'F8-AVG',\n",
      "                   'T4-AVG',\n",
      "                   'T6-AVG',\n",
      "                   'FZ-AVG',\n",
      "                   'CZ-AVG',\n",
      "                   'PZ-AVG',\n",
      "                   'EKG',\n",
      "                   'Photic'],\n",
      " 'signal_length_limit': 10000000,\n",
      " 'task': 'abnormal',\n",
      " 'task_description': 'Classification of [Normal] and [Abnormal] symptoms',\n",
      " 'task_name': 'CAUEEG-Abnormal benchmark',\n",
      " 'test_crop_multiple': 8,\n",
      " 'total_samples': 100000000.0,\n",
      " 'transform': Compose(\n",
      "    EegRandomCrop(crop_length=2560, length_limit=10000000, multiple=8, latency=2000, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegDropChannels(drop_index=[20])\n",
      "    EegToTensor()\n",
      "),\n",
      " 'transform_multicrop': Compose(\n",
      "    EegRandomCrop(crop_length=2560, length_limit=10000000, multiple=8, latency=2000, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegDropChannels(drop_index=[20])\n",
      "    EegToTensor()\n",
      "),\n",
      " 'use_age': 'conv',\n",
      " 'use_wandb': True,\n",
      " 'warmup_min': 3000,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'warmup_steps': 19531,\n",
      " 'watch_model': False,\n",
      " 'weight_decay': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# load pretrained configurations\n",
    "path = os.path.join(pre_model_path, pre_model_name.split(',')[-1], 'checkpoint.pt')\n",
    "try:\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    config = ckpt['config']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(f'- checkpoint cannot be opened: {path}')\n",
    "config[\"cwd\"] = \"\"\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update configuration\n",
    "for k, v in art_config.items():\n",
    "    config[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "if not config.get(\"ddp\", False):\n",
    "    pre_model_state = ckpt[\"ssl_model_state\"]\n",
    "else:\n",
    "    pre_model_state_ddp = deepcopy(ckpt[\"ssl_model_state\"])\n",
    "    pre_model_state = OrderedDict()\n",
    "    for k, v in pre_model_state_ddp.items():\n",
    "        name = k[7:]  # remove 'module.' of DataParallel/DistributedDataParallel\n",
    "        pre_model_state[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EKG': 'O',\n",
      " '_target_': 'models.ssl.mae_1d.mae_1d_pre_b_e768_d512',\n",
      " 'activation': 'gelu',\n",
      " 'age_mean': tensor([71.3079]),\n",
      " 'age_std': tensor([9.6942]),\n",
      " 'art_dropout': 0.1,\n",
      " 'art_filter_list': (3, 3, 3, 3, 3),\n",
      " 'art_loss_type': 'mse',\n",
      " 'art_out_activation': 'softplus',\n",
      " 'art_resnet': True,\n",
      " 'awgn': 0.003,\n",
      " 'awgn_age': 0.001,\n",
      " 'base_lr': 0.0001,\n",
      " 'class_label_to_name': ['Normal', 'Abnormal'],\n",
      " 'class_name_to_label': {'Abnormal': 1, 'Normal': 0},\n",
      " 'criterion': 'cross-entropy',\n",
      " 'crop_length': 2560,\n",
      " 'crop_multiple': 8,\n",
      " 'crop_timing_analysis': False,\n",
      " 'cwd': '',\n",
      " 'dataset_name': 'CAUEEG dataset',\n",
      " 'dataset_path': 'local/dataset/caueeg-dataset/',\n",
      " 'ddp': False,\n",
      " 'ddp_size': 1,\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'draw_result': True,\n",
      " 'file_format': 'memmap',\n",
      " 'in_channels': 20,\n",
      " 'input_norm': 'datapoint',\n",
      " 'iterations': 390625,\n",
      " 'latency': 2000,\n",
      " 'load_event': False,\n",
      " 'lr_scheduler_type': 'cosine_decay_with_warmup_half',\n",
      " 'mask_ratio': 0.75,\n",
      " 'mgn': 0.003,\n",
      " 'minibatch': 256,\n",
      " 'minibatch_3090': 256,\n",
      " 'mixed_precision': True,\n",
      " 'mixup': 0.0,\n",
      " 'model': '1D-MAE-B',\n",
      " 'multi_batch_size': 32,\n",
      " 'norm_pix_loss': True,\n",
      " 'num_history': 50,\n",
      " 'num_params': 123492000,\n",
      " 'num_workers': 4,\n",
      " 'out_dims': 2,\n",
      " 'output_length': 161,\n",
      " 'patch_size': 8,\n",
      " 'photic': 'X',\n",
      " 'pre_model': 'eg6s5fay',\n",
      " 'preprocess_test': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=0))\n",
      "  (1): EegResample(orig_freq=200, new_freq=100, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([71.3079]), std=tensor([9.6942]), eps=1e-08, std_eps=tensor([9.6942]))\n",
      "  (3): EegNormalizePerSignal(eps=1e-08)\n",
      "),\n",
      " 'preprocess_train': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=0))\n",
      "  (1): EegResample(orig_freq=200, new_freq=100, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([71.3079], device='cuda:0'), std=tensor([9.6942]), eps=1e-08, std_eps=tensor([9.6942], device='cuda:0'))\n",
      "  (3): EegAddGaussianNoiseAge(mean=0.0, std=0.001)\n",
      "  (4): EegNormalizePerSignal(eps=1e-08)\n",
      "  (5): EegMultiplicativeGaussianNoise(mean=0.0, std=0.003)\n",
      "  (6): EegAdditiveGaussianNoise(mean=0.0, std=0.003)\n",
      "),\n",
      " 'project': 'caueeg-mae-artifact',\n",
      " 'resample': 100,\n",
      " 'run_mode': 'train',\n",
      " 'sampling_rate': 200,\n",
      " 'save_model': False,\n",
      " 'search_lr': False,\n",
      " 'search_multiplier': 1.0,\n",
      " 'seed': 0,\n",
      " 'seq_length': 1280,\n",
      " 'signal_header': ['Fp1-AVG',\n",
      "                   'F3-AVG',\n",
      "                   'C3-AVG',\n",
      "                   'P3-AVG',\n",
      "                   'O1-AVG',\n",
      "                   'Fp2-AVG',\n",
      "                   'F4-AVG',\n",
      "                   'C4-AVG',\n",
      "                   'P4-AVG',\n",
      "                   'O2-AVG',\n",
      "                   'F7-AVG',\n",
      "                   'T3-AVG',\n",
      "                   'T5-AVG',\n",
      "                   'F8-AVG',\n",
      "                   'T4-AVG',\n",
      "                   'T6-AVG',\n",
      "                   'FZ-AVG',\n",
      "                   'CZ-AVG',\n",
      "                   'PZ-AVG',\n",
      "                   'EKG',\n",
      "                   'Photic'],\n",
      " 'signal_length_limit': 10000000,\n",
      " 'task': 'abnormal',\n",
      " 'task_description': 'Classification of [Normal] and [Abnormal] symptoms',\n",
      " 'task_name': 'CAUEEG-Abnormal benchmark',\n",
      " 'test_crop_multiple': 8,\n",
      " 'total_samples': 5000000.0,\n",
      " 'transform': Compose(\n",
      "    EegRandomCrop(crop_length=2560, length_limit=10000000, multiple=8, latency=2000, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegDropChannels(drop_index=[20])\n",
      "    EegToTensor()\n",
      "),\n",
      " 'transform_multicrop': Compose(\n",
      "    EegRandomCrop(crop_length=2560, length_limit=10000000, multiple=8, latency=2000, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegDropChannels(drop_index=[20])\n",
      "    EegToTensor()\n",
      "),\n",
      " 'use_age': 'conv',\n",
      " 'use_wandb': False,\n",
      " 'warmup_min': 150,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'warmup_steps': 19531,\n",
      " 'watch_model': False,\n",
      " 'weight_decay': 0.01}\n"
     ]
    }
   ],
   "source": [
    "config[\"ddp\"] = False\n",
    "\n",
    "# check the workstation environment and update some configurations\n",
    "check_device_env(config)\n",
    "\n",
    "# compose dataset\n",
    "train_loader, val_loader, test_loader, multicrop_test_loader = compose_dataset(config)\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the model\n",
    "config[\"_target_\"] = config[\"_target_\"].replace('pre', 'pre_art').replace('.mae_1d.', '.mae_1d_artifact.')\n",
    "model = generate_model(config).to(device)\n",
    "\n",
    "# load the model\n",
    "model_state = model.state_dict()\n",
    "for k, v in model_state.items():\n",
    "    if not k.startswith('art') and not k.endswith(\"pos_embed\"):\n",
    "        model_state[k] = pre_model_state[k]\n",
    "\n",
    "pre_model_state = deepcopy(model.state_dict())\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.requires_grad_(False)\n",
    "model = model.eval()\n",
    "\n",
    "model.art_net.requires_grad_(True)\n",
    "for k, v in model._parameters.items():\n",
    "    if k.startswith(\"art\"):\n",
    "        v.requires_grad_(True)\n",
    "model.art_net = model.art_net.train()\n",
    "\n",
    "config[\"num_params\"] = count_parameters(model)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_token                                                                                         \t|\tFalse\n",
      "enc_pos_embed                                                                                       \t|\tFalse\n",
      "mask_token                                                                                          \t|\tFalse\n",
      "dec_pos_embed                                                                                       \t|\tFalse\n",
      "enc_proj.weight                                                                                     \t|\tFalse\n",
      "enc_proj.bias                                                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm1.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm1.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm2.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm2.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm1.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm1.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm2.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm2.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "enc_norm.weight                                                                                     \t|\tFalse\n",
      "enc_norm.bias                                                                                       \t|\tFalse\n",
      "dec_proj.weight                                                                                     \t|\tFalse\n",
      "dec_proj.bias                                                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm1.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm1.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm2.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm2.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm1.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm1.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm2.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm2.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "dec_norm.weight                                                                                     \t|\tFalse\n",
      "dec_norm.bias                                                                                       \t|\tFalse\n",
      "decoder_pred.weight                                                                                 \t|\tFalse\n",
      "decoder_pred.bias                                                                                   \t|\tFalse\n",
      "art_net.0.weight                                                                                    \t|\tTrue\n",
      "art_net.0.bias                                                                                      \t|\tTrue\n",
      "art_net.1.weight                                                                                    \t|\tTrue\n",
      "art_net.1.bias                                                                                      \t|\tTrue\n",
      "art_net.3.conv1.weight                                                                              \t|\tTrue\n",
      "art_net.3.norm1.weight                                                                              \t|\tTrue\n",
      "art_net.3.norm1.bias                                                                                \t|\tTrue\n",
      "art_net.3.conv2.weight                                                                              \t|\tTrue\n",
      "art_net.3.norm2.weight                                                                              \t|\tTrue\n",
      "art_net.3.norm2.bias                                                                                \t|\tTrue\n",
      "art_net.4.conv1.weight                                                                              \t|\tTrue\n",
      "art_net.4.norm1.weight                                                                              \t|\tTrue\n",
      "art_net.4.norm1.bias                                                                                \t|\tTrue\n",
      "art_net.4.conv2.weight                                                                              \t|\tTrue\n",
      "art_net.4.norm2.weight                                                                              \t|\tTrue\n",
      "art_net.4.norm2.bias                                                                                \t|\tTrue\n",
      "art_net.5.conv1.weight                                                                              \t|\tTrue\n",
      "art_net.5.norm1.weight                                                                              \t|\tTrue\n",
      "art_net.5.norm1.bias                                                                                \t|\tTrue\n",
      "art_net.5.conv2.weight                                                                              \t|\tTrue\n",
      "art_net.5.norm2.weight                                                                              \t|\tTrue\n",
      "art_net.5.norm2.bias                                                                                \t|\tTrue\n",
      "art_net.6.conv1.weight                                                                              \t|\tTrue\n",
      "art_net.6.norm1.weight                                                                              \t|\tTrue\n",
      "art_net.6.norm1.bias                                                                                \t|\tTrue\n",
      "art_net.6.conv2.weight                                                                              \t|\tTrue\n",
      "art_net.6.norm2.weight                                                                              \t|\tTrue\n",
      "art_net.6.norm2.bias                                                                                \t|\tTrue\n",
      "art_net.6.downsample.0.weight                                                                       \t|\tTrue\n",
      "art_net.6.downsample.1.weight                                                                       \t|\tTrue\n",
      "art_net.6.downsample.1.bias                                                                         \t|\tTrue\n",
      "art_net.9.weight                                                                                    \t|\tTrue\n",
      "art_net.11.weight                                                                                   \t|\tTrue\n",
      "art_net.11.bias                                                                                     \t|\tTrue\n",
      "art_net.13.weight                                                                                   \t|\tTrue\n",
      "art_net.13.bias                                                                                     \t|\tTrue\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:100}\\t|\\t{param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************    Configurations for Train    ******************************\n",
      "\n",
      "{'EKG': 'O',\n",
      " '_target_': 'models.ssl.mae_1d_artifact.mae_1d_pre_art_b_e768_d512',\n",
      " 'activation': 'gelu',\n",
      " 'age_mean': tensor([71.3079]),\n",
      " 'age_std': tensor([9.6942]),\n",
      " 'art_dropout': 0.1,\n",
      " 'art_filter_list': (3, 3, 3, 3, 3),\n",
      " 'art_loss_type': 'mse',\n",
      " 'art_out_activation': 'softplus',\n",
      " 'art_resnet': True,\n",
      " 'awgn': 0.003,\n",
      " 'awgn_age': 0.001,\n",
      " 'base_lr': 0.0001,\n",
      " 'class_label_to_name': ['Normal', 'Abnormal'],\n",
      " 'class_name_to_label': {'Abnormal': 1, 'Normal': 0},\n",
      " 'criterion': 'cross-entropy',\n",
      " 'crop_length': 2560,\n",
      " 'crop_multiple': 8,\n",
      " 'crop_timing_analysis': False,\n",
      " 'cwd': '',\n",
      " 'dataset_name': 'CAUEEG dataset',\n",
      " 'dataset_path': 'local/dataset/caueeg-dataset/',\n",
      " 'ddp': False,\n",
      " 'ddp_size': 1,\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'draw_result': True,\n",
      " 'file_format': 'memmap',\n",
      " 'in_channels': 20,\n",
      " 'input_norm': 'datapoint',\n",
      " 'iterations': 390625,\n",
      " 'latency': 2000,\n",
      " 'load_event': False,\n",
      " 'lr_scheduler_type': 'cosine_decay_with_warmup_half',\n",
      " 'mask_ratio': 0.75,\n",
      " 'mgn': 0.003,\n",
      " 'minibatch': 256,\n",
      " 'minibatch_3090': 256,\n",
      " 'mixed_precision': True,\n",
      " 'mixup': 0.0,\n",
      " 'model': '1D-MAE-B',\n",
      " 'multi_batch_size': 32,\n",
      " 'norm_pix_loss': True,\n",
      " 'num_history': 50,\n",
      " 'num_params': 109729,\n",
      " 'num_workers': 4,\n",
      " 'out_dims': 2,\n",
      " 'output_length': 161,\n",
      " 'patch_size': 8,\n",
      " 'photic': 'X',\n",
      " 'pre_model': 'eg6s5fay',\n",
      " 'preprocess_test': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=0))\n",
      "  (1): EegResample(orig_freq=200, new_freq=100, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([71.3079]), std=tensor([9.6942]), eps=1e-08, std_eps=tensor([9.6942]))\n",
      "  (3): EegNormalizePerSignal(eps=1e-08)\n",
      "),\n",
      " 'preprocess_train': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=0))\n",
      "  (1): EegResample(orig_freq=200, new_freq=100, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([71.3079], device='cuda:0'), std=tensor([9.6942]), eps=1e-08, std_eps=tensor([9.6942], device='cuda:0'))\n",
      "  (3): EegAddGaussianNoiseAge(mean=0.0, std=0.001)\n",
      "  (4): EegNormalizePerSignal(eps=1e-08)\n",
      "  (5): EegMultiplicativeGaussianNoise(mean=0.0, std=0.003)\n",
      "  (6): EegAdditiveGaussianNoise(mean=0.0, std=0.003)\n",
      "),\n",
      " 'project': 'caueeg-mae-artifact',\n",
      " 'resample': 100,\n",
      " 'run_mode': 'train',\n",
      " 'sampling_rate': 200,\n",
      " 'save_model': False,\n",
      " 'search_lr': False,\n",
      " 'search_multiplier': 1.0,\n",
      " 'seed': 0,\n",
      " 'seq_length': 1280,\n",
      " 'signal_header': ['Fp1-AVG',\n",
      "                   'F3-AVG',\n",
      "                   'C3-AVG',\n",
      "                   'P3-AVG',\n",
      "                   'O1-AVG',\n",
      "                   'Fp2-AVG',\n",
      "                   'F4-AVG',\n",
      "                   'C4-AVG',\n",
      "                   'P4-AVG',\n",
      "                   'O2-AVG',\n",
      "                   'F7-AVG',\n",
      "                   'T3-AVG',\n",
      "                   'T5-AVG',\n",
      "                   'F8-AVG',\n",
      "                   'T4-AVG',\n",
      "                   'T6-AVG',\n",
      "                   'FZ-AVG',\n",
      "                   'CZ-AVG',\n",
      "                   'PZ-AVG',\n",
      "                   'EKG',\n",
      "                   'Photic'],\n",
      " 'signal_length_limit': 10000000,\n",
      " 'task': 'abnormal',\n",
      " 'task_description': 'Classification of [Normal] and [Abnormal] symptoms',\n",
      " 'task_name': 'CAUEEG-Abnormal benchmark',\n",
      " 'test_crop_multiple': 8,\n",
      " 'total_samples': 5000000.0,\n",
      " 'transform': Compose(\n",
      "    EegRandomCrop(crop_length=2560, length_limit=10000000, multiple=8, latency=2000, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegDropChannels(drop_index=[20])\n",
      "    EegToTensor()\n",
      "),\n",
      " 'transform_multicrop': Compose(\n",
      "    EegRandomCrop(crop_length=2560, length_limit=10000000, multiple=8, latency=2000, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegDropChannels(drop_index=[20])\n",
      "    EegToTensor()\n",
      "),\n",
      " 'use_age': 'conv',\n",
      " 'use_wandb': False,\n",
      " 'warmup_min': 150,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'warmup_steps': 19531,\n",
      " 'watch_model': False,\n",
      " 'weight_decay': 0.01}\n",
      "\n",
      "********************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect some garbage\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# fix the seed for reproducibility (a negative seed value means not fixing)\n",
    "set_seed(config, rank=None)\n",
    "\n",
    "# train\n",
    "ssl_train_script(\n",
    "    config,\n",
    "    model,\n",
    "    train_loader,\n",
    "    config[\"preprocess_train\"],\n",
    ")\n",
    "\n",
    "for k, v in model_state.items():\n",
    "    pre_model_state[k] = model_state[k].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_header = [channel.split('-')[0] for i, channel in enumerate(config[\"signal_header\"])]\n",
    "fps = config.get('resample', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plt.style.context(['ieee', 'science', 'default'])\n",
    "def draw_eeg_graph(x, pred, mask, art_out, rec_loss, num=None):\n",
    "    plt.rcParams.update({'font.family': 'Ubuntu'})\n",
    "    N, C, L = x.shape\n",
    "    _, l = mask.shape\n",
    "    patch_size = L // l\n",
    "\n",
    "    art_out = art_out.reshape(N, -1)\n",
    "    rec_loss = rec_loss.reshape(N, -1)\n",
    "\n",
    "    for n in range(N):\n",
    "        if num is not None and num <= n:\n",
    "            break\n",
    "        \n",
    "        fig = plt.figure(num=1, clear=True, figsize=(25.0, 17.0))\n",
    "        fig.subplots_adjust(hspace=0)\n",
    "        \n",
    "        for c in range(C):\n",
    "            ax = fig.add_subplot(C, 1, c + 1)\n",
    "            ax.plot(x[n, c].cpu().numpy(), lw=1, c='tab:red', label='origin')\n",
    "            ax.plot(pred[n, c].cpu().numpy(), lw=1, c='tab:blue', label='pred')\n",
    "\n",
    "            art_mid = art_out[n].median()\n",
    "            rec_mid = art_out[n].median()\n",
    "            for r in range(l):\n",
    "                if r > 0:\n",
    "                    ax.axvline(r*patch_size, color='tab:purple', alpha=0.4)\n",
    "                if mask[n, r]:\n",
    "                    ax.axvspan(r*patch_size, (r + 1)*patch_size, facecolor='tab:purple', alpha=0.2)\n",
    "\n",
    "                if c == 0:\n",
    "                    if art_mid <= art_out[n, r]:\n",
    "                        ax.annotate(f\"{art_out[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, 1), ha='center', va='bottom', color='tab:purple')\n",
    "                    else:\n",
    "                        ax.annotate(f\"{art_out[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, 1), ha='center', va='bottom')\n",
    "                    if rec_mid <= rec_loss[n, r]:\n",
    "                        ax.annotate(f\"{rec_loss[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, -1), ha='center', va='bottom', color='tab:purple')\n",
    "                    else:\n",
    "                        ax.annotate(f\"{rec_loss[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, -1), ha='center', va='bottom')\n",
    "\n",
    "            ax.set_xlim(0, L)\n",
    "            ax.set_ylabel(signal_header[c])\n",
    "            ax.set_xticks(np.arange(round(config[\"seq_length\"] / fps) + 1) * fps)\n",
    "            ax.set_xticklabels([])\n",
    "            # ax.tick_params(axis='x', width=0.1, length=0.1)\n",
    "            ax.set_yticks([0])\n",
    "            ax.set_yticklabels([])\n",
    "        \n",
    "        ax.set_xticks(np.arange(round(config[\"seq_length\"] / fps) + 1) * fps)\n",
    "        ax.set_xticklabels(np.arange(round(config[\"seq_length\"] / fps) + 1))\n",
    "        \n",
    "        ax.set_xlabel('Time (s)')\n",
    "        # fig.savefig(os.path.join(output_folder, 'signal_example.pdf'), transparent=True)\n",
    "        plt.show()\n",
    "        fig.clear()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for target_dataset in tqdm([\"val\"], desc=\"Dataset\", leave=False):\n",
    "        if target_dataset == 'train':\n",
    "            loader = train_loader\n",
    "        elif target_dataset == 'val':\n",
    "            loader = val_loader\n",
    "        elif target_dataset == 'test':\n",
    "            loader = test_loader\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "                \n",
    "        for sample_batched in tqdm(loader, total=len(loader), desc='Batch', leave=False):\n",
    "            print(target_dataset)\n",
    "            config[\"preprocess_test\"](sample_batched)\n",
    "            x = sample_batched[\"signal\"]\n",
    "            age = sample_batched[\"age\"]\n",
    "\n",
    "            pred, mask = model.mask_and_reconstruct(x, age, config[\"mask_ratio\"])\n",
    "            rec_loss = model.compute_reconstruction_loss_without_masking(x, pred)\n",
    "            art_out = model.forward_artifact(x, age)\n",
    "            \n",
    "            pred_eeg = model.unpatchify(pred)\n",
    "            draw_eeg_graph(x, pred_eeg, mask, art_out, rec_loss, 2)\n",
    "\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, ckpt\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cyc in finetune_cycler:\n",
    "    # update configuration\n",
    "    for k, v in finetune_config.items():\n",
    "        config[k] = v\n",
    "    for k, v in cyc.items():\n",
    "        config[k] = v\n",
    "    config[\"ddp\"] = False\n",
    "\n",
    "    # check the workstation environment and update some configurations\n",
    "    check_device_env(config)\n",
    "    \n",
    "    # compose dataset\n",
    "    train_loader, val_loader, test_loader, multicrop_test_loader = compose_dataset(config)\n",
    "    pprint.pprint(config)\n",
    "\n",
    "    # generate the model\n",
    "    config[\"_target_\"] = config[\"_target_\"].replace('.ssl', '').replace('_pre', '')\n",
    "    model = generate_model(config).to(device)\n",
    "    \n",
    "    # load the model\n",
    "    model_state = model.state_dict()\n",
    "    for k, v in model_state.items():\n",
    "        if not k.startswith(\"fc\") and not k.endswith(\"pos_embed\"):\n",
    "            model_state[k] = pre_model_state[k]\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.finetune_mode(config[\"tuning_type\"])\n",
    "    config[\"num_params\"] = count_parameters(model)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name:100}\\t|\\t{param.requires_grad}\")\n",
    "\n",
    "    # collect some garbage\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # fix the seed for reproducibility (a negative seed value means not fixing)\n",
    "    set_seed(config, rank=None)\n",
    "    \n",
    "    # train\n",
    "    train_script(\n",
    "        config,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        multicrop_test_loader,\n",
    "        config[\"preprocess_train\"],\n",
    "        config[\"preprocess_test\"],\n",
    "    )\n",
    "\n",
    "print(\"- END -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
