{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aef1d0b-f084-44c6-a332-127a3d4af1fb",
   "metadata": {},
   "source": [
    "# Occlusion Sensitivity\n",
    "\n",
    "This notebook conducts an experiment for the occlusion sensitivity of our networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b731a-e5d9-429d-9313-4d6ad242a70f",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae52098d-3075-4185-8408-a5bc3a52b31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minjae\\Desktop\\EEG_Project\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18cdea24-1a20-4989-9b69-69c05da32f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import hydra\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cycler import cycler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredDirectionArrows\n",
    "\n",
    "# custom package\n",
    "from datasets.caueeg_script import build_dataset_for_train\n",
    "from datasets.pipeline import EegSpectrogram\n",
    "from datasets.pipeline import eeg_collate_fn\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54f80b1-51d1-4b23-ba5a-97750a8ab3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu113\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa174d-f3f2-4de6-91b6-4e64ab0cd86a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ddbd12-da9a-4373-9775-020ab6a6fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo88puq7 is successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'lo88puq7'\n",
    "\n",
    "# load from disk\n",
    "try:\n",
    "    path = os.path.join(r'E:\\CAUEEG\\checkpoint', model_name, 'checkpoint.pt')\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "model_state = ckpt['model_state']\n",
    "config = ckpt['config']\n",
    "\n",
    "# initiate the model\n",
    "if '_target_' in config:\n",
    "    model = hydra.utils.instantiate(config).to(device)\n",
    "elif type(config['generator']) is str:\n",
    "    config['generator'] = getattr(models, config['generator'].split('.')[-1])\n",
    "    if 'block' in config:\n",
    "        config['block'] = getattr(models, config['block'].split('.')[-1])\n",
    "    model = config['generator'](**config).to(device)\n",
    "else:\n",
    "    if 'block' in config:\n",
    "        if config['block'] == models.resnet_1d.BottleneckBlock1D:\n",
    "            config['block'] = 'bottleneck'\n",
    "        elif config['block'] == models.resnet_2d.Bottleneck2D:\n",
    "            config['block'] = 'bottleneck'\n",
    "        elif config['block'] == models.resnet_1d.BasicBlock1D:\n",
    "            config['block'] = 'basic'\n",
    "        elif config['block'] == models.resnet_2d.BasicBlock2D:\n",
    "            config['block'] = 'basic'\n",
    "\n",
    "    model = config['generator'](**config).to(device)\n",
    "\n",
    "if config.get('ddp', False):\n",
    "    model_state_ddp = deepcopy(model_state)\n",
    "    model_state = OrderedDict()\n",
    "    for k, v in model_state_ddp.items():\n",
    "        name = k[7:]  # remove 'module.' of DataParallel/DistributedDataParallel\n",
    "        model_state[name] = v\n",
    "\n",
    "model.load_state_dict(model_state)\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "print(f'{model_name} is successfully loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119a08f-f9cc-4db4-a8f6-c7030bd08813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dfec22-3837-46a7-8b4d-4588300068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 3\n",
    "minibatch = 1024\n",
    "crop_multiple = 1\n",
    "test_crop_multiple = 1\n",
    "save_fig = True\n",
    "target_dataset = 'val'  # train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713226ad-59c7-4f37-8cf8-3a87da629773",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.pop('cwd', 0)\n",
    "config['ddp'] = False\n",
    "config['minibatch'] = 1\n",
    "config['crop_multiple'] = crop_multiple\n",
    "config['test_crop_multiple'] = test_crop_multiple\n",
    "config['crop_timing_analysis'] = False\n",
    "config['eval'] = True\n",
    "config['device'] = device\n",
    "H, W = config['seq_len_2d']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6cc89-0dca-4e4c-b3aa-4026b8e182fb",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc21b49-f2d2-40ce-8bc3-e6ae0e38c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = build_dataset_for_train(config, verbose=False)\n",
    "train_loader = _[0]\n",
    "val_loader = _[1]\n",
    "test_loader = _[2]\n",
    "multicrop_test_loader = _[3]\n",
    "\n",
    "if target_dataset == 'train':\n",
    "    loader = train_loader\n",
    "elif target_dataset == 'val':\n",
    "    loader = val_loader\n",
    "elif target_dataset == 'test':\n",
    "    loader = test_loader\n",
    "else:\n",
    "    raise ValueError('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99490d-03bd-4bee-8bed-5253ab045b54",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba96a8e-94b1-4ce3-9877-6f767bd6a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_score(model, sample_batched, config):\n",
    "    # compute output embedding\n",
    "    x = sample_batched['signal']\n",
    "    age = sample_batched['age']\n",
    "    output = model.compute_feature_embedding(x, age)\n",
    "    \n",
    "    if config['criterion'] == 'cross-entropy':\n",
    "        s = F.softmax(output, dim=1)\n",
    "    elif config['criterion'] == 'multi-bce':\n",
    "        s = torch.sigmoid(output)\n",
    "    elif config['criterion'] == 'svm':\n",
    "        s = output\n",
    "\n",
    "    # map depending on the loss function\n",
    "    if config['criterion'] == 'cross-entropy':\n",
    "        score = F.softmax(output, dim=1)\n",
    "    elif config['criterion'] == 'multi-bce':\n",
    "        score = torch.sigmoid(output)\n",
    "    elif config['criterion'] == 'svm':\n",
    "        score = output\n",
    "    else:\n",
    "        raise ValueError(f\"estimate_score(): cannot parse config['criterion']={config['criterion']}.\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cafa3d0-d589-47f9-b0ff-ada041b8af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_as_real_to_complex(signal):\n",
    "    N, _, H, W = signal.shape\n",
    "    C = signal.shape[1] // 2\n",
    "\n",
    "    sig_out = torch.zeros((N, C, H, W, 2))\n",
    "    sig_out[..., 0] = signal[:, :C]\n",
    "    sig_out[..., 1] = signal[:, C:]\n",
    "\n",
    "    sig_out = torch.view_as_complex(sig_out)\n",
    "    return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4876d9-9687-431e-bc8e-d6775fb51c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# plt.style.use('fast') # default, ggplot, fivethirtyeight, bmh, dark_background, classic\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'jet' # 'jet', 'nipy_spectral', 'rainbow'\n",
    "\n",
    "# plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams.update({'font.family': 'Arial'})\n",
    "# plt.rcParams[\"font.family\"] = 'DejaVu Sans' # 'NanumGothic' # for Hangul in Windows\n",
    "plt.rcParams[\"savefig.dpi\"] = 1200\n",
    "\n",
    "\n",
    "def draw_stft(sample, config, index=0, log_scale=False, occlusion=None, save_fig=None):\n",
    "    signal = deepcopy(sample['signal'])\n",
    "    signal_f = from_as_real_to_complex(signal)[index].abs().cpu().numpy()\n",
    "    \n",
    "    # always do not consider EKG and Photic channels\n",
    "    C = 19 # signal_f.shape[0]\n",
    "    _, H, W = signal_f.shape\n",
    "    \n",
    "    columns = 7\n",
    "    rows = round(np.ceil(C / columns))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, columns, \n",
    "                           figsize=(22.0, 9.5), constrained_layout=True)\n",
    "    \n",
    "    for k in range(columns * rows):\n",
    "        r = k // columns\n",
    "        c = k % columns\n",
    "        \n",
    "        if k < C:\n",
    "            im = ax[r, c].imshow(np.log(signal_f[k, ::-1] + 1e-8) if log_scale else signal_f[k, ::-1],\n",
    "                                 interpolation='nearest',\n",
    "                                 extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                                 aspect=(config['seq_length']/200.0) / (200/2.0))\n",
    "            ax[r, c].set_title(config['signal_header'][k].split('-')[0], \n",
    "                               fontsize=18, fontweight='bold', color='darkred')\n",
    "            ax[r, c].set_xlabel('Time (s)', fontsize=13)\n",
    "            ax[r, c].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "            # ax[r, c].invert_yaxis()\n",
    "            \n",
    "            if occlusion and occlusion['c'] == k:\n",
    "                bb = mtransforms.Bbox([[occlusion['x'] / W * config['seq_length']/200.0, \n",
    "                                        occlusion['y'] / H * 200/2.0], \n",
    "                                       [(occlusion['x'] + occlusion['w']) / W * config['seq_length']/200.0, \n",
    "                                        (occlusion['y'] + occlusion['h']) / H * 200/2.0]])\n",
    "                fancy = FancyBboxPatch(bb.p0, bb.width, bb.height, boxstyle=\"square,pad=0\")\n",
    "                fancy.set(edgecolor=\"none\", facecolor=\"gray\", zorder=10)\n",
    "                ax[r, c].add_patch(fancy)\n",
    "            \n",
    "        elif k == C:\n",
    "            ax[r, c].axis('off')\n",
    "            axins = ax[r, c]\n",
    "        else:\n",
    "            ax[r, c].axis('off')\n",
    "        \n",
    "    fig.suptitle('Time-Frequency Representation', fontsize=20, fontweight='semibold')\n",
    "    cax = inset_axes(axins,\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=axins.transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax)\n",
    "    cbar.ax.set_xlabel('Magnitude in log-scale' if log_scale else 'Magnitude', fontsize=13) \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_fig, 'draw_stft.pdf'), transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def draw_stft_origin(sample, config, index=0, log_scale=False, occlusion=None, save_fig=None):\n",
    "    sample = deepcopy(sample)\n",
    "    EegSpectrogram(**config['stft_params'])(sample)\n",
    "    signal_f = from_as_real_to_complex(sample['signal'])[index].abs().cpu().numpy()\n",
    "    \n",
    "    # always do not consider EKG and Photic channels\n",
    "    C = 19 # signal_f.shape[0]\n",
    "    _, H, W = signal_f.shape\n",
    "    \n",
    "    columns = 7\n",
    "    rows = round(np.ceil(C / columns))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, columns, \n",
    "                           figsize=(22.0, 9.5), constrained_layout=True)\n",
    "    \n",
    "    for k in range(columns * rows):\n",
    "        r = k // columns\n",
    "        c = k % columns\n",
    "        \n",
    "        if k < C:\n",
    "            im = ax[r, c].imshow(np.log(signal_f[k, ::-1] + 1e-8) if log_scale else signal_f[k, ::-1],\n",
    "                                 interpolation='nearest',\n",
    "                                 extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                                 aspect=(config['seq_length']/200.0) / (200/2.0))\n",
    "            ax[r, c].set_title(config['signal_header'][k].split('-')[0], \n",
    "                               fontsize=18, fontweight='bold', color='darkred')\n",
    "            ax[r, c].set_xlabel('Time (s)', fontsize=13)\n",
    "            ax[r, c].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "            # ax[r, c].invert_yaxis()\n",
    "            \n",
    "            if occlusion and occlusion['c'] == k:\n",
    "                bb = mtransforms.Bbox([[occlusion['x'] / W * config['seq_length']/200.0, \n",
    "                                        occlusion['y'] / H * 200/2.0], \n",
    "                                       [(occlusion['x'] + occlusion['w']) / W * config['seq_length']/200.0, \n",
    "                                        (occlusion['y'] + occlusion['h']) / H * 200/2.0]])\n",
    "                fancy = FancyBboxPatch(bb.p0, bb.width, bb.height, boxstyle=\"square,pad=0\")\n",
    "                fancy.set(edgecolor=\"none\", facecolor=\"gray\", zorder=10)\n",
    "                ax[r, c].add_patch(fancy)\n",
    "            \n",
    "        elif k == C:\n",
    "            ax[r, c].axis('off')\n",
    "            axins = ax[r, c]\n",
    "        else:\n",
    "            ax[r, c].axis('off')\n",
    "        \n",
    "    fig.suptitle('Time-Frequency Representation', fontsize=20, fontweight='semibold')\n",
    "    cax = inset_axes(axins,\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=axins.transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax)\n",
    "    cbar.ax.set_xlabel('Magnitude in log-scale' if log_scale else 'Magnitude', fontsize=13) \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_fig, 'draw_stft_origin.pdf'), transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def draw_occlusion_sensitivity(occlusion_score, config, save_fig=None):\n",
    "    # always do not consider EKG and Photic channels\n",
    "    C, H, W = occlusion_score.shape\n",
    "    \n",
    "    columns = 7\n",
    "    rows = round(np.ceil(C / columns))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, columns, \n",
    "                           figsize=(22.0, 9.5), constrained_layout=True)\n",
    "    \n",
    "    for k in range(columns * rows):\n",
    "        r = k // columns\n",
    "        c = k % columns\n",
    "        \n",
    "        if k < C:\n",
    "            im = ax[r, c].imshow(occlusion_score[k, ::-1], interpolation='nearest',\n",
    "                                 extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                                 aspect=(config['seq_length']/200.0) / (200/2.0))\n",
    "            ax[r, c].set_title(config['signal_header'][k].split('-')[0], \n",
    "                               fontsize=18, fontweight='bold', color='darkred')\n",
    "            ax[r, c].set_xlabel('Time (s)', fontsize=13)\n",
    "            ax[r, c].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "            # ax[r, c].invert_yaxis()\n",
    "        elif k == C:\n",
    "            ax[r, c].axis('off')\n",
    "            axins = ax[r, c]\n",
    "        else:\n",
    "            ax[r, c].axis('off')\n",
    "        \n",
    "    fig.suptitle('Occlusion Sensitivity', fontsize=20, fontweight='semibold')\n",
    "    # colorbar\n",
    "    cax = inset_axes(axins,\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=axins.transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax)\n",
    "    cbar.ax.set_xlabel('True class score', fontsize=13) \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_fig, 'draw_occlusion_sensitivity.pdf'), transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def draw_occlusion_sensitivity_all_channels(occlusion_score, config, save_fig=None):\n",
    "    # always do not consider EKG and Photic channels\n",
    "    H, W = occlusion_score.shape\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6.5, 4.0), constrained_layout=True)\n",
    "    im = ax[0].imshow(occlusion_score[::-1], interpolation='nearest',\n",
    "                      extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                      aspect=(config['seq_length']/200.0) / (200/2.0))\n",
    "    ax[0].set_xlabel('Time (s)', fontsize=13)\n",
    "    ax[0].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "    fig.suptitle('Occlusion Sensitivity (All Channels)', fontsize=20, fontweight='semibold')\n",
    "    \n",
    "    # colorbar\n",
    "    ax[1].axis('off')    \n",
    "    cax = inset_axes(ax[1],\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=ax[1].transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax, shrink=0.9)\n",
    "    cbar.ax.set_xlabel('True class score', fontsize=13) \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_fig, 'draw_occlusion_sensitivity_all_channels.pdf'), transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008b6086-c40f-4377-92a3-6ed9cd957a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occlusion_sensitivity_map(occlusion_sensitivity):\n",
    "    coeff = np.zeros((19, H, W))\n",
    "    score = np.zeros((19, H, W))\n",
    "\n",
    "    for result in occlusion_sensitivity:\n",
    "        s = result['score']\n",
    "        c = result['c']\n",
    "        x = result['x']\n",
    "        y = result['y']\n",
    "        w = result['w']\n",
    "        h = result['h']\n",
    "\n",
    "        score[c, y:y+h, x:x+w] += s\n",
    "        coeff[c, y:y+h, x:x+w] += 1\n",
    "\n",
    "    score = score / coeff\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_occlusion_sensitivity_all_channels_map(occlusion_sensitivity):\n",
    "    coeff = np.zeros((H, W))\n",
    "    score = np.zeros((H, W))\n",
    "\n",
    "    for result in occlusion_sensitivity:\n",
    "        s = result['score']\n",
    "        x = result['x']\n",
    "        y = result['y']\n",
    "        w = result['w']\n",
    "        h = result['h']\n",
    "\n",
    "        score[y:y+h, x:x+w] += s\n",
    "        coeff[y:y+h, x:x+w] += 1\n",
    "\n",
    "    score = score / coeff\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e085b-fadf-40c4-a37b-1913c4f8a9c6",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1aefd-a99a-4113-acce-026e63491766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in loader:\n",
    "    sample_origin = deepcopy(sample)\n",
    "    config['preprocess_test'](sample)\n",
    "\n",
    "    occlusion_sensitivity = []\n",
    "    occ_cycler = cycler(x=np.arange(W)) * cycler(y=np.arange(H)) * \\\n",
    "                cycler(w=[round(W/4)]) * cycler(h=[round(H/4)]) * \\\n",
    "                cycler(c=np.arange(19))\n",
    "\n",
    "    sample_batched = []\n",
    "    for occ in tqdm(occ_cycler):\n",
    "        # boundary condition\n",
    "        if W < occ['x'] + occ['w'] or H < occ['y'] + occ['h']:\n",
    "            continue\n",
    "\n",
    "        # occlude\n",
    "        sb_temp = deepcopy(sample)\n",
    "        sb_temp['occlusion'] = occ\n",
    "        C = sb_temp['signal'].shape[1]\n",
    "        sb_temp['signal'][:, [occ['c'], C//2 + occ['c']], \n",
    "                          occ['y']:occ['y'] + occ['h'], \n",
    "                          occ['x']:occ['x'] + occ['w']] = 0\n",
    "        sample_batched.append(sb_temp)\n",
    "\n",
    "        # test\n",
    "        if len(sample_batched) == minibatch:\n",
    "            N = len(sample_batched)\n",
    "            sample_batched = eeg_collate_fn(sample_batched)\n",
    "            sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "            sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "            sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "            s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "            for i in range(N):\n",
    "                result = {**sample_batched['occlusion'][i], \n",
    "                          'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "                occlusion_sensitivity.append(result)\n",
    "            sample_batched = []\n",
    "\n",
    "    # test\n",
    "    if len(sample_batched) > 0:\n",
    "        N = len(sample_batched)\n",
    "        sample_batched = eeg_collate_fn(sample_batched)\n",
    "        sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "        sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "        sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "        s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "        for i in range(N):\n",
    "            result = {**sample_batched['occlusion'][i], \n",
    "                      'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "            occlusion_sensitivity.append(result)\n",
    "        sample_batched = []        \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce59b2c-4660-4842-8085-c6b5196d2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_stft_origin(sample_origin, config, log_scale=True)\n",
    "# draw_stft_origin(sample_origin, config, log_scale=False)\n",
    "# draw_stft(sample, config, log_scale=True)\n",
    "# draw_stft(sample, config, log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4082a3-07a5-46b6-b9dd-083e8080dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generate_occlusion_sensitivity_map(occlusion_sensitivity)\n",
    "draw_occlusion_sensitivity(score, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4f193-0431-444c-bcfc-e20970f0e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion_sensitivity_all_channels = []\n",
    "occ_cycler = cycler(x=np.arange(W)) * cycler(y=np.arange(H)) * \\\n",
    "            cycler(w=[round(W/4)]) * cycler(h=[round(H/4)])\n",
    "\n",
    "sample_batched = []\n",
    "for occ in tqdm(occ_cycler):\n",
    "    # boundary condition\n",
    "    if W < occ['x'] + occ['w'] or H < occ['y'] + occ['h']:\n",
    "        continue\n",
    "\n",
    "    # occlude\n",
    "    sb_temp = deepcopy(sample)\n",
    "    sb_temp['occlusion'] = occ\n",
    "    C = sb_temp['signal'].shape[1]\n",
    "    sb_temp['signal'][:, :, \n",
    "                      occ['y']:occ['y'] + occ['h'], \n",
    "                      occ['x']:occ['x'] + occ['w']] = 0\n",
    "    sample_batched.append(sb_temp)\n",
    "\n",
    "    # test\n",
    "    if len(sample_batched) == minibatch:\n",
    "        N = len(sample_batched)\n",
    "        sample_batched = eeg_collate_fn(sample_batched)\n",
    "        sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "        sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "        sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "        s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "        for i in range(N):\n",
    "            result = {**sample_batched['occlusion'][i], \n",
    "                      'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "            occlusion_sensitivity_all_channels.append(result)\n",
    "        sample_batched = []\n",
    "\n",
    "# test\n",
    "if len(sample_batched) > 0:\n",
    "    N = len(sample_batched)\n",
    "    sample_batched = eeg_collate_fn(sample_batched)\n",
    "    sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "    sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "    sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "    s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "    for i in range(N):\n",
    "        result = {**sample_batched['occlusion'][i], \n",
    "                  'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "        occlusion_sensitivity_all_channels.append(result)\n",
    "    sample_batched = []        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edf972-bb42-49c1-8f2b-c7b684892427",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = generate_occlusion_sensitivity_all_channels_map(occlusion_sensitivity)\n",
    "draw_occlusion_sensitivity_all_channels(score, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ea1c7-644b-4fed-a86b-141109e4c499",
   "metadata": {},
   "source": [
    "## All pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca0f95-ed61-474c-9075-ae624342d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c80c5272d94361a910df5a3b1c2eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample in tqdm(loader):\n",
    "    sample_origin = deepcopy(sample)\n",
    "    config['preprocess_test'](sample)\n",
    "\n",
    "    occlusion_sensitivity = []\n",
    "    occ_cycler = cycler(x=np.arange(W)) * cycler(y=np.arange(H)) * \\\n",
    "                cycler(w=[round(W/4)]) * cycler(h=[round(H/4)]) * \\\n",
    "                cycler(c=np.arange(19))\n",
    "\n",
    "    sample_batched = []\n",
    "    for occ in occ_cycler:\n",
    "        # boundary condition\n",
    "        if W < occ['x'] + occ['w'] or H < occ['y'] + occ['h']:\n",
    "            continue\n",
    "\n",
    "        # occlude and gather minibatch\n",
    "        sb_temp = deepcopy(sample)\n",
    "        sb_temp['occlusion'] = occ\n",
    "        C = sb_temp['signal'].shape[1]\n",
    "        sb_temp['signal'][:, [occ['c'], C//2 + occ['c']], \n",
    "                          occ['y']:occ['y'] + occ['h'], \n",
    "                          occ['x']:occ['x'] + occ['w']] = 0\n",
    "        sample_batched.append(sb_temp)\n",
    "\n",
    "        # test\n",
    "        if len(sample_batched) == minibatch:\n",
    "            N = len(sample_batched)\n",
    "            sample_batched = eeg_collate_fn(sample_batched)\n",
    "            sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "            sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "            sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "            s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "            for i in range(N):\n",
    "                result = {**sample_batched['occlusion'][i], \n",
    "                          'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "                occlusion_sensitivity.append(result)\n",
    "            sample_batched = []\n",
    "\n",
    "    # test\n",
    "    if len(sample_batched) > 0:\n",
    "        N = len(sample_batched)\n",
    "        sample_batched = eeg_collate_fn(sample_batched)\n",
    "        sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "        sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "        sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "        s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "        for i in range(N):\n",
    "            result = {**sample_batched['occlusion'][i], \n",
    "                      'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "            occlusion_sensitivity.append(result)\n",
    "        sample_batched = []        \n",
    "\n",
    "\n",
    "    # draw and save\n",
    "    class_name = config['class_label_to_name'][sample[\"class_label\"][0]]\n",
    "    path = f'local/output/occlusion_exp/{target_dataset}/{class_name}/{sample[\"serial\"][0]}'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    draw_stft_origin(sample_origin, config, log_scale=True, save_fig=path)\n",
    "\n",
    "    score = generate_occlusion_sensitivity_map(occlusion_sensitivity)\n",
    "    with open(os.path.join(path, 'occlusion_sensitivity_map.npy'), 'wb') as f:\n",
    "        np.save(f, score)\n",
    "    draw_occlusion_sensitivity(score, config, save_fig=path)\n",
    "\n",
    "    ################\n",
    "    # All Channels #\n",
    "    ################\n",
    "    occlusion_sensitivity_all_channels = []\n",
    "    occ_cycler = cycler(x=np.arange(W)) * cycler(y=np.arange(H)) * \\\n",
    "                cycler(w=[round(W/4)]) * cycler(h=[round(H/4)])\n",
    "\n",
    "    sample_batched = []\n",
    "    for occ in occ_cycler:\n",
    "        # boundary condition\n",
    "        if W < occ['x'] + occ['w'] or H < occ['y'] + occ['h']:\n",
    "            continue\n",
    "\n",
    "        # occlude and gather minibatch\n",
    "        sb_temp = deepcopy(sample)\n",
    "        sb_temp['occlusion'] = occ\n",
    "        C = sb_temp['signal'].shape[1]\n",
    "        sb_temp['signal'][:, :, \n",
    "                          occ['y']:occ['y'] + occ['h'], \n",
    "                          occ['x']:occ['x'] + occ['w']] = 0\n",
    "        sample_batched.append(sb_temp)\n",
    "\n",
    "        # test\n",
    "        if len(sample_batched) == minibatch:\n",
    "            N = len(sample_batched)\n",
    "            sample_batched = eeg_collate_fn(sample_batched)\n",
    "            sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "            sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "            sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "            s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "            for i in range(N):\n",
    "                result = {**sample_batched['occlusion'][i], \n",
    "                          'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "                occlusion_sensitivity_all_channels.append(result)\n",
    "            sample_batched = []\n",
    "\n",
    "    # test\n",
    "    if len(sample_batched) > 0:\n",
    "        N = len(sample_batched)\n",
    "        sample_batched = eeg_collate_fn(sample_batched)\n",
    "        sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "        sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "        sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "        s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "        for i in range(N):\n",
    "            result = {**sample_batched['occlusion'][i], \n",
    "                      'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "            occlusion_sensitivity_all_channels.append(result)\n",
    "        sample_batched = []              \n",
    "\n",
    "    score = generate_occlusion_sensitivity_all_channels_map(occlusion_sensitivity)\n",
    "    with open(os.path.join(path, 'occlusion_sensitivity_all_channels_map.npy'), 'wb') as f:\n",
    "        np.save(f, score)\n",
    "    draw_occlusion_sensitivity_all_channels(score, config, save_fig=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7f762-d710-4bcf-bb6b-1a7c843bfe72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
