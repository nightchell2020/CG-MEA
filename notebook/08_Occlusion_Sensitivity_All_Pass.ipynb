{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aef1d0b-f084-44c6-a332-127a3d4af1fb",
   "metadata": {},
   "source": [
    "# Occlusion Sensitivity\n",
    "\n",
    "This notebook conducts an experiment for the occlusion sensitivity of our networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b731a-e5d9-429d-9313-4d6ad242a70f",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae52098d-3075-4185-8408-a5bc3a52b31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minjae\\Desktop\\EEG_Project\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18cdea24-1a20-4989-9b69-69c05da32f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import hydra\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cycler import cycler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredDirectionArrows\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# custom package\n",
    "from datasets.caueeg_script import build_dataset_for_train\n",
    "from datasets.pipeline import EegSpectrogram\n",
    "from datasets.pipeline import eeg_collate_fn\n",
    "import models\n",
    "from train.evaluate import estimate_class_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54f80b1-51d1-4b23-ba5a-97750a8ab3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu117\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa174d-f3f2-4de6-91b6-4e64ab0cd86a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ddbd12-da9a-4373-9775-020ab6a6fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'lo88puq7',\n",
    "    'ruqd8r7g'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119a08f-f9cc-4db4-a8f6-c7030bd08813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dfec22-3837-46a7-8b4d-4588300068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_range = range(5, 7)\n",
    "minibatch = 1024\n",
    "crop_multiple = 1\n",
    "test_crop_multiple = 1\n",
    "\n",
    "box_size_ratio_min = 0.2\n",
    "box_size_ratio_max = 0.25\n",
    "\n",
    "target_datasets = [\n",
    "   'train',\n",
    "    'val',\n",
    "#    'test',\n",
    "]\n",
    "\n",
    "save_fig = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99490d-03bd-4bee-8bed-5253ab045b54",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4876d9-9687-431e-bc8e-d6775fb51c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# plt.style.use('fast') # default, ggplot, fivethirtyeight, bmh, dark_background, classic\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'jet' # 'jet', 'nipy_spectral', 'rainbow'\n",
    "\n",
    "# plt.rcParams.update({'font.size': 11})\n",
    "plt.rcParams.update({'font.family': 'Roboto Slab'})\n",
    "# plt.rcParams[\"font.family\"] = 'DejaVu Sans' # 'NanumGothic' # for Hangul in Windows\n",
    "plt.rcParams[\"savefig.dpi\"] = 1200\n",
    "\n",
    "\n",
    "def from_as_real_to_complex(signal):\n",
    "    N, _, H, W = signal.shape\n",
    "    C = signal.shape[1] // 2\n",
    "\n",
    "    sig_out = torch.zeros((N, C, H, W, 2))\n",
    "    sig_out[..., 0] = signal[:, :C]\n",
    "    sig_out[..., 1] = signal[:, C:]\n",
    "\n",
    "    sig_out = torch.view_as_complex(sig_out)\n",
    "    return sig_out\n",
    "\n",
    "\n",
    "def draw_stft(sample, config, index=0, log_scale=False, occlusion=None, save_fig=None):\n",
    "    signal = deepcopy(sample['signal'])\n",
    "    signal_f = from_as_real_to_complex(signal)[index].abs().cpu().numpy()\n",
    "    \n",
    "    # always do not consider EKG and Photic channels\n",
    "    C = 19 # signal_f.shape[0]\n",
    "    _, H, W = signal_f.shape\n",
    "    \n",
    "    columns = 7\n",
    "    rows = round(np.ceil(C / columns))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, columns, \n",
    "                           figsize=(22.0, 9.5), constrained_layout=True)\n",
    "    normalizer = Normalize()\n",
    "    \n",
    "    for k in range(columns * rows):\n",
    "        r = k // columns\n",
    "        c = k % columns\n",
    "        \n",
    "        if k < C:\n",
    "            im = ax[r, c].imshow(np.log(signal_f[k, ::-1] + 1e-8) if log_scale else signal_f[k, ::-1],\n",
    "                                 interpolation='nearest',\n",
    "                                 extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                                 aspect=(config['seq_length']/200.0) / (200/2.0), norm=normalizer)\n",
    "            ax[r, c].set_title(config['signal_header'][k].split('-')[0], \n",
    "                               fontsize=18, fontweight='bold', color='darkred')\n",
    "            ax[r, c].set_xlabel('Time (s)', fontsize=13)\n",
    "            ax[r, c].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "            # ax[r, c].invert_yaxis()\n",
    "            \n",
    "            if occlusion and occlusion['c'] == k:\n",
    "                bb = mtransforms.Bbox([[occlusion['x'] / W * config['seq_length']/200.0, \n",
    "                                        occlusion['y'] / H * 200/2.0], \n",
    "                                       [(occlusion['x'] + occlusion['w']) / W * config['seq_length']/200.0, \n",
    "                                        (occlusion['y'] + occlusion['h']) / H * 200/2.0]])\n",
    "                fancy = FancyBboxPatch(bb.p0, bb.width, bb.height, boxstyle=\"square,pad=0\")\n",
    "                fancy.set(edgecolor=\"none\", facecolor=\"gray\", zorder=10)\n",
    "                ax[r, c].add_patch(fancy)\n",
    "            \n",
    "        elif k == C:\n",
    "            ax[r, c].axis('off')\n",
    "            axins = ax[r, c]\n",
    "        else:\n",
    "            ax[r, c].axis('off')\n",
    "        \n",
    "    fig.suptitle('Time-Frequency Representation', fontsize=20, fontweight='semibold')\n",
    "    cax = inset_axes(axins,\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=axins.transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax)\n",
    "    cbar.ax.set_xlabel('Magnitude in log-scale' if log_scale else 'Magnitude', fontsize=13) \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_fig, 'draw_stft.pdf'), transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def draw_stft_origin(sample, config, index=0, log_scale=False, occlusion=None, save_fig=None):\n",
    "    sample = deepcopy(sample)\n",
    "    EegSpectrogram(**config['stft_params'])(sample)\n",
    "    signal_f = from_as_real_to_complex(sample['signal'])[index].abs().cpu().numpy()\n",
    "    \n",
    "    # always do not consider EKG and Photic channels\n",
    "    C = 19 # signal_f.shape[0]\n",
    "    _, H, W = signal_f.shape\n",
    "    \n",
    "    columns = 7\n",
    "    rows = round(np.ceil(C / columns))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, columns, \n",
    "                           figsize=(22.0, 9.5), constrained_layout=True)\n",
    "    normalizer = Normalize()\n",
    "    \n",
    "    for k in range(columns * rows):\n",
    "        r = k // columns\n",
    "        c = k % columns\n",
    "        \n",
    "        if k < C:\n",
    "            im = ax[r, c].imshow(np.log(signal_f[k, ::-1] + 1e-8) if log_scale else signal_f[k, ::-1],\n",
    "                                 interpolation='nearest',\n",
    "                                 extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                                 aspect=(config['seq_length']/200.0) / (200/2.0), norm=normalizer)\n",
    "            ax[r, c].set_title(config['signal_header'][k].split('-')[0], \n",
    "                               fontsize=18, fontweight='bold', color='darkred')\n",
    "            ax[r, c].set_xlabel('Time (s)', fontsize=13)\n",
    "            ax[r, c].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "            # ax[r, c].invert_yaxis()\n",
    "            \n",
    "            if occlusion and occlusion['c'] == k:\n",
    "                bb = mtransforms.Bbox([[occlusion['x'] / W * config['seq_length']/200.0, \n",
    "                                        occlusion['y'] / H * 200/2.0], \n",
    "                                       [(occlusion['x'] + occlusion['w']) / W * config['seq_length']/200.0, \n",
    "                                        (occlusion['y'] + occlusion['h']) / H * 200/2.0]])\n",
    "                fancy = FancyBboxPatch(bb.p0, bb.width, bb.height, boxstyle=\"square,pad=0\")\n",
    "                fancy.set(edgecolor=\"none\", facecolor=\"gray\", zorder=10)\n",
    "                ax[r, c].add_patch(fancy)\n",
    "            \n",
    "        elif k == C:\n",
    "            ax[r, c].axis('off')\n",
    "            axins = ax[r, c]\n",
    "        else:\n",
    "            ax[r, c].axis('off')\n",
    "        \n",
    "    fig.suptitle('Time-Frequency Representation', fontsize=20, fontweight='semibold')\n",
    "    cax = inset_axes(axins,\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=axins.transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax)\n",
    "    cbar.ax.set_xlabel('Magnitude in log-scale' if log_scale else 'Magnitude', fontsize=13) \n",
    "    if save_fig:\n",
    "        fig.savefig(os.path.join(save_fig, 'draw_stft_origin.pdf'), transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def draw_occlusion_sensitivity(occlusion_score, config, suptitle=None, save_fig=None):\n",
    "    # always do not consider EKG and Photic channels\n",
    "    C, H, W = occlusion_score.shape\n",
    "    \n",
    "    columns = 7\n",
    "    rows = round(np.ceil(C / columns))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, columns, \n",
    "                           figsize=(22.0, 9.5), constrained_layout=True)\n",
    "    normalizer = Normalize()  # (0, 1)\n",
    "\n",
    "    for k in range(columns * rows):\n",
    "        r = k // columns\n",
    "        c = k % columns\n",
    "        \n",
    "        if k < C:\n",
    "            im = ax[r, c].imshow(occlusion_score[k, ::-1], interpolation='nearest',\n",
    "                                 extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                                 aspect=(config['seq_length']/200.0) / (200/2.0), norm=normalizer)\n",
    "            ax[r, c].set_title(config['signal_header'][k].split('-')[0], \n",
    "                               fontsize=18, fontweight='bold', color='darkred')\n",
    "            ax[r, c].set_xlabel('Time (s)', fontsize=13)\n",
    "            ax[r, c].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "            # ax[r, c].invert_yaxis()\n",
    "        elif k == C:\n",
    "            ax[r, c].axis('off')\n",
    "            axins = ax[r, c]\n",
    "        else:\n",
    "            ax[r, c].axis('off')\n",
    "    \n",
    "    if suptitle is None:\n",
    "        suptitle = 'Occlusion Sensitivity'\n",
    "    fig.suptitle(suptitle, fontsize=20, fontweight='semibold')\n",
    "    # colorbar\n",
    "    cax = inset_axes(axins,\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"80%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=axins.transAxes,\n",
    "                     borderpad=0,\n",
    "                     )\n",
    "    cbar = fig.colorbar(im, ax=ax.ravel().tolist(), cax=cax)\n",
    "    cbar.ax.set_xlabel('True class score', fontsize=13)\n",
    "    if save_fig:\n",
    "        fig.savefig(save_fig, transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008b6086-c40f-4377-92a3-6ed9cd957a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occlusion_sensitivity_map(occlusion_sensitivity):\n",
    "    coeff = np.zeros((19, H, W))\n",
    "    score = np.zeros((19, H, W))\n",
    "\n",
    "    for result in occlusion_sensitivity:\n",
    "        s = result['score']\n",
    "        c = result['c']\n",
    "        x = result['x']\n",
    "        y = result['y']\n",
    "        w = result['w']\n",
    "        h = result['h']\n",
    "\n",
    "        score[c, y:y+h, x:x+w] += s\n",
    "        coeff[c, y:y+h, x:x+w] += 1\n",
    "\n",
    "    score = score / coeff\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_occlusion_sensitivity_all_channels_map(occlusion_sensitivity):\n",
    "    coeff = np.zeros((H, W))\n",
    "    score = np.zeros((H, W))\n",
    "    ideal_score = 0.0\n",
    "    ideal_count = 0\n",
    "\n",
    "    for result in occlusion_sensitivity:\n",
    "        s = result['score']\n",
    "        x = result['x']\n",
    "        y = result['y']\n",
    "        w = result['w']\n",
    "        h = result['h']\n",
    "\n",
    "        # no occlussion case\n",
    "        if x == 0 and y == 0 and w == 0 and h == 0:\n",
    "            ideal_score += s\n",
    "            ideal_count += 1\n",
    "        else:\n",
    "            score[y:y+h, x:x+w] += s\n",
    "            coeff[y:y+h, x:x+w] += 1\n",
    "\n",
    "    score = score / coeff\n",
    "    ideal_score = ideal_score / ideal_count\n",
    "    return score, ideal_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e085b-fadf-40c4-a37b-1913c4f8a9c6",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca0f95-ed61-474c-9075-ae624342d270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in tqdm(model_list, desc='Model', leave=False):\n",
    "    # load from disk\n",
    "    try:\n",
    "        path = os.path.join(r'E:\\CAUEEG\\checkpoint', model_name, 'checkpoint.pt')\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    model_state = ckpt['model_state']\n",
    "    config = ckpt['config']\n",
    "\n",
    "    path = f'local/output/occlusion_exp/{model_name}/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(os.path.join(path, 'config'), 'wb') as f:\n",
    "        torch.save(config, f)\n",
    "        \n",
    "    # initiate the model\n",
    "    if '_target_' in config:\n",
    "        model = hydra.utils.instantiate(config).to(device)\n",
    "    elif type(config['generator']) is str:\n",
    "        config['generator'] = getattr(models, config['generator'].split('.')[-1])\n",
    "        if 'block' in config:\n",
    "            config['block'] = getattr(models, config['block'].split('.')[-1])\n",
    "        model = config['generator'](**config).to(device)\n",
    "    else:\n",
    "        if 'block' in config:\n",
    "            if config['block'] == models.resnet_1d.BottleneckBlock1D:\n",
    "                config['block'] = 'bottleneck'\n",
    "            elif config['block'] == models.resnet_2d.Bottleneck2D:\n",
    "                config['block'] = 'bottleneck'\n",
    "            elif config['block'] == models.resnet_1d.BasicBlock1D:\n",
    "                config['block'] = 'basic'\n",
    "            elif config['block'] == models.resnet_2d.BasicBlock2D:\n",
    "                config['block'] = 'basic'\n",
    "    \n",
    "        model = config['generator'](**config).to(device)\n",
    "    \n",
    "    if config.get('ddp', False):\n",
    "        model_state_ddp = deepcopy(model_state)\n",
    "        model_state = OrderedDict()\n",
    "        for k, v in model_state_ddp.items():\n",
    "            name = k[7:]  # remove 'module.' of DataParallel/DistributedDataParallel\n",
    "            model_state[name] = v\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    task = config['task']\n",
    "    config.pop('cwd', 0)\n",
    "    config['ddp'] = False\n",
    "    config['minibatch'] = 1\n",
    "    config['crop_multiple'] = crop_multiple\n",
    "    config['test_crop_multiple'] = test_crop_multiple\n",
    "    config['crop_timing_analysis'] = False\n",
    "    config['eval'] = True\n",
    "    config['device'] = device\n",
    "    H, W = config['seq_len_2d']\n",
    "    \n",
    "    if '220419' in config['dataset_path']:\n",
    "        config['dataset_path'] = './local/dataset/caueeg-dataset/'\n",
    "    \n",
    "    _ = build_dataset_for_train(config, verbose=False)\n",
    "    train_loader = _[0]\n",
    "    val_loader = _[1]\n",
    "    test_loader = _[2]\n",
    "    multicrop_test_loader = _[3]\n",
    "    \n",
    "    for target_dataset in tqdm(target_datasets, desc=\"Dataset\", leave=False):\n",
    "        if target_dataset == 'train':\n",
    "            loader = train_loader\n",
    "        elif target_dataset == 'val':\n",
    "            loader = val_loader\n",
    "        elif target_dataset == 'test':\n",
    "            loader = test_loader\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "                \n",
    "        for k in tqdm(repeat_range, desc='Repeat Round', leave=False):\n",
    "            for sample in tqdm(loader, total=len(loader), desc='Batch', leave=False):\n",
    "                sample_origin = deepcopy(sample)\n",
    "                config['preprocess_test'](sample)\n",
    "\n",
    "                ################\n",
    "                # All Channels #\n",
    "                ################\n",
    "                occlusion_sensitivity_all_channels = []\n",
    "                \n",
    "                # occluding box size\n",
    "                occ_cycler = cycler(x=np.arange(W)) * cycler(y=np.arange(H))\n",
    "                occ_cycler *= cycler(w=np.arange(round(W * box_size_ratio_min), \n",
    "                                                 round(W * box_size_ratio_max) + 1))\n",
    "                occ_cycler *= cycler(h=np.arange(round(H * box_size_ratio_min), \n",
    "                                                 round(H * box_size_ratio_max) + 1))\n",
    "\n",
    "                # no occlusion case\n",
    "                sample_batched = []\n",
    "                sb_temp = deepcopy(sample)\n",
    "                sb_temp['occlusion'] = {'x': 0, 'y': 0, 'w': 0, 'h': 0}\n",
    "                sample_batched.append(sb_temp)\n",
    "\n",
    "                # occlusion case\n",
    "                for occ in occ_cycler:\n",
    "                    # boundary condition\n",
    "                    if W < occ['x'] + occ['w'] or H < occ['y'] + occ['h']:\n",
    "                        continue\n",
    "\n",
    "                    # occlude and gather minibatch\n",
    "                    sb_temp = deepcopy(sample)\n",
    "                    sb_temp['occlusion'] = occ\n",
    "                    sb_temp['signal'][:, :, \n",
    "                                      occ['y']:occ['y'] + occ['h'], \n",
    "                                      occ['x']:occ['x'] + occ['w']] = 0\n",
    "                    C = sb_temp['signal'].shape[1]\n",
    "                    sample_batched.append(sb_temp)\n",
    "\n",
    "                    # gather data until it becomes minibatch size\n",
    "                    if len(sample_batched) == minibatch:\n",
    "                        N = len(sample_batched)\n",
    "                        sample_batched = eeg_collate_fn(sample_batched)\n",
    "                        sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "                        sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "                        sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "                        s = estimate_class_score(model, sample_batched, config)\n",
    "\n",
    "                        for i in range(N):\n",
    "                            result = {**sample_batched['occlusion'][i], \n",
    "                                      'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "                            occlusion_sensitivity_all_channels.append(result)\n",
    "                        sample_batched = []\n",
    "\n",
    "                # the rest data\n",
    "                if len(sample_batched) > 0:\n",
    "                    N = len(sample_batched)\n",
    "                    sample_batched = eeg_collate_fn(sample_batched)\n",
    "                    sample_batched['signal'] = sample_batched['signal'].reshape(N, C, H, W)\n",
    "                    sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "                    sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "                    s = estimate_class_score(model, sample_batched, config)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        result = {**sample_batched['occlusion'][i], \n",
    "                                  'score': s[i, sample_batched['class_label'][i]].item()}\n",
    "                        occlusion_sensitivity_all_channels.append(result)\n",
    "\n",
    "                score, ideal_score = generate_occlusion_sensitivity_all_channels_map(occlusion_sensitivity_all_channels)\n",
    "                class_name = config['class_label_to_name'][sample[\"class_label\"][0]]\n",
    "                path = f'local/output/occlusion_exp/{model_name}/{target_dataset}/{class_name}/{sample[\"serial\"][0]}_{k:02d}'\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                with open(os.path.join(path, 'occlusion_sensitivity_all_channels_map.npy'), 'wb') as f:\n",
    "                    np.save(f, score)\n",
    "                with open(os.path.join(path, 'occlusion_sensitivity_all_channels_map_ideal_score.npy'), 'wb') as f:\n",
    "                    np.save(f, ideal_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd41b9-bbb7-462f-a3ba-3b6a7eee4da8",
   "metadata": {},
   "source": [
    "## Post analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05856919-a4b0-482c-8a72-d87aa262d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r''\n",
    "# score = np.load(os.path.join(path, 'occlusion_sensitivity_all_channels_map.npy'))\n",
    "# ideal_score = np.load(os.path.join(path, 'occlusion_sensitivity_all_channels_map_ideal_score.npy'))\n",
    "# draw_occlusion_sensitivity_all_channels(score, config, \n",
    "#                                         save_fig=os.path.join(path, 'draw_occlusion_sensitivity_all_channels.pdf'), \n",
    "#                                         suptitle=f\"Ideal score: {ideal_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8c0aa6-117f-47fc-b299-fa337c206f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for target_dataset in ['train', 'test', 'val']:\n",
    "#     for root in glob.glob(f'local/output/occlusion_exp/dementia/{target_dataset}/*'):\n",
    "#         taget_class = os.path.basename(root)\n",
    "        \n",
    "#         all_channel_count = 0\n",
    "#         count = 0\n",
    "        \n",
    "#         for file in glob.glob(os.path.join(root, '*/*.npy')):\n",
    "#             if os.path.basename(file) == 'occlusion_sensitivity_map.npy':\n",
    "#                 if count == 0:\n",
    "#                     occlusion_sensitivity_map = np.load(file)\n",
    "#                 else:\n",
    "#                     occlusion_sensitivity_map += np.load(file)\n",
    "                    \n",
    "#                 count += 1\n",
    "                \n",
    "#             elif os.path.basename(file) == 'occlusion_sensitivity_all_channels_map.npy':\n",
    "#                 if all_channel_count == 0:\n",
    "#                     occlusion_sensitivity_all_channels_map = np.load(file)\n",
    "#                 else:\n",
    "#                     occlusion_sensitivity_all_channels_map += np.load(file)\n",
    "                \n",
    "#                 all_channel_count += 1\n",
    "\n",
    "#             else:\n",
    "#                 raise FileNotFoundError('')\n",
    "    \n",
    "#         occlusion_sensitivity_all_channels_map = occlusion_sensitivity_all_channels_map / count\n",
    "#         occlusion_sensitivity_map = occlusion_sensitivity_map / count\n",
    "\n",
    "        \n",
    "#         if target_dataset == 'train':\n",
    "#             dataset_name = 'Training Set'\n",
    "#         elif target_dataset == 'val':\n",
    "#             dataset_name = 'Validation Set'\n",
    "#         elif target_dataset == 'test':\n",
    "#             dataset_name = 'Test Set'\n",
    "        \n",
    "#         # path = f'local/output/occlusion_exp/dementia/{target_dataset}/{taget_class}/'\n",
    "#         path = r'C:\\Users\\Minjae\\Desktop\\occlusion_exp/'\n",
    "#         os.makedirs(path, exist_ok=True)\n",
    "#         print(target_dataset, taget_class, count, all_channel_count, path)\n",
    "        \n",
    "#         save_path = path + f'/occlusion-exp-dementia-{target_dataset}-{taget_class}-channel-wise.pdf'\n",
    "#         draw_occlusion_sensitivity(occlusion_sensitivity_map, config, save_fig=save_path,\n",
    "#                                    # suptitle=f'Occlusion Sensitivity ({dataset_name}, {taget_class})')\n",
    "#                                    suptitle=f'')\n",
    "\n",
    "#         save_path = path + f'/occlusion-exp-dementia-{target_dataset}-{taget_class}-channel-agnostic.pdf'\n",
    "#         draw_occlusion_sensitivity_all_channels(occlusion_sensitivity_all_channels_map, config, save_fig=save_path,\n",
    "#                                                 # suptitle=f'Occlusion Sensitivity \\n(All Channels, {dataset_name}, {taget_class})')\n",
    "#                                                 suptitle=f'')\n",
    "        \n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2b5d8a8-b89a-4e42-a4d6-75059b82c63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_occlusion_sensitivity_all_channels(occlusion_score, config, original_score=None, suptitle=None, save_fig=None):\n",
    "    # always do not consider EKG and Photic channels\n",
    "    H, W = occlusion_score.shape\n",
    "    \n",
    "    if suptitle:\n",
    "        #suptitle = 'Occlusion Sensitivity (All Channels)'\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(6.5, 4.0), constrained_layout=True)\n",
    "        fig.suptitle(suptitle, fontsize=20, fontweight='semibold')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(6.5, 3.5), constrained_layout=True)\n",
    "        \n",
    "    im = ax[0].imshow(occlusion_score[::-1], interpolation='nearest',\n",
    "                      extent=[0, config['seq_length']/200.0, 0, 200/2.0], \n",
    "                      aspect=(config['seq_length']/200.0) / (200/2.0))\n",
    "    ax[0].set_xlabel('Time (s)', fontsize=13)\n",
    "    ax[0].set_ylabel('Frequency (Hz)', fontsize=13)\n",
    "        \n",
    "    # colorbar\n",
    "    ax[1].axis('off')    \n",
    "    cax = inset_axes(ax[1],\n",
    "                     width=\"10%\",  # width = 10% of parent_bbox width\n",
    "                     height=\"95%\",  # height : 50%\n",
    "                     loc='center',\n",
    "                     bbox_to_anchor=(0., 0., 1, 1),\n",
    "                     bbox_transform=ax[1].transAxes,\n",
    "                     borderpad=0,\n",
    "                    )\n",
    "    \n",
    "    if original_score:\n",
    "        cbar = plt.colorbar(im, ax=ax.ravel().tolist(), cax=cax, shrink=0.95)\n",
    "        cbar.mappable.set_clim(min(original_score - 0.05, np.min(occlusion_score)), \n",
    "                               max(original_score + 0.05, np.max(occlusion_score)))\n",
    "        \n",
    "        # Calculate arrow position\n",
    "        colorbar_min, colorbar_max = cbar.mappable.get_clim()\n",
    "        arrow_pos = (original_score - colorbar_min) / (colorbar_max - colorbar_min) * colorbar_max * 0.95\n",
    "        \n",
    "        cbar.ax.annotate(f\"Unoccluded\\nscore: {original_score:.2f}\", fontsize=10,\n",
    "                       xy=(0.1, original_score), \n",
    "                       xytext=(-2.5, original_score),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                       ha='center', va='center', color='red')\n",
    "\n",
    "    else:\n",
    "        cbar = plt.colorbar(im, ax=ax.ravel().tolist(), cax=cax, shrink=0.95)\n",
    "    \n",
    "    cbar.ax.set_xlabel('Class score \\nfor true class', fontsize=13)\n",
    "    \n",
    "    if save_fig:\n",
    "        fig.savefig(save_fig, transparent=True, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68f617f0-ff24-47cc-b528-597f5ae86494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Loop:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Loop:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inner Loop:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for model_name in tqdm(model_list, desc='Model', leave=False):\n",
    "    for target_dataset in tqdm(target_datasets, desc=\"Dataset\", leave=False):\n",
    "        for root in tqdm(glob.glob(f'local/output/occlusion_exp/{model_name}/{target_dataset}/*'), desc='Inner Loop', leave=False):\n",
    "            target_class = os.path.basename(root)\n",
    "            config = torch.load(f'local/output/occlusion_exp/{model_name}/config')\n",
    "            task = config['task']\n",
    "            \n",
    "            all_channel_count = 0\n",
    "            ideal_count = 0\n",
    "            \n",
    "            for file in glob.glob(os.path.join(root, '*/*.npy')):\n",
    "                if os.path.basename(file) == 'occlusion_sensitivity_all_channels_map.npy':\n",
    "                    if all_channel_count == 0:\n",
    "                        occlusion_sensitivity_all_channels_map = np.load(file)\n",
    "                    else:\n",
    "                        occlusion_sensitivity_all_channels_map += np.load(file)\n",
    "                    all_channel_count += 1\n",
    "                    \n",
    "                elif os.path.basename(file) == 'occlusion_sensitivity_all_channels_map_ideal_score.npy':\n",
    "                    if ideal_count == 0:\n",
    "                        occlusion_sensitivity_ideal_score = np.load(file)\n",
    "                    else:\n",
    "                        occlusion_sensitivity_ideal_score += np.load(file)\n",
    "                    ideal_count += 1\n",
    "    \n",
    "                else:\n",
    "                    raise FileNotFoundError('')\n",
    "        \n",
    "            occlusion_sensitivity_all_channels_map = occlusion_sensitivity_all_channels_map / all_channel_count\n",
    "            occlusion_sensitivity_ideal_score = occlusion_sensitivity_ideal_score / ideal_count\n",
    "            \n",
    "            if target_dataset == 'train':\n",
    "                dataset_name = 'Training Set'\n",
    "            elif target_dataset == 'val':\n",
    "                dataset_name = 'Validation Set'\n",
    "            elif target_dataset == 'test':\n",
    "                dataset_name = 'Test Set'\n",
    "            \n",
    "            path = f'./local/output/occlusion_exp/{model_name}'\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "                \n",
    "            save_path = os.path.join(path, f'{target_dataset}-{target_class}-channel-agnostic.pdf')\n",
    "            draw_occlusion_sensitivity_all_channels(occlusion_sensitivity_all_channels_map, config, \n",
    "                                                    original_score=occlusion_sensitivity_ideal_score,\n",
    "                                                    save_fig=save_path,\n",
    "                                                    # suptitle=f'Occlusion sensitivity: {target_class}'\n",
    "                                                   )\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8077da57-1922-497a-94c8-f0e968349c30",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
