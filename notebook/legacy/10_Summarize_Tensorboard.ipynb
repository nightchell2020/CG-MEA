{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d9ee339-4a34-416b-a462-1103b40a26e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root_folder = 'checkpoint/*'\n",
    "\n",
    "with SummaryWriter(log_dir='runs/hparam') as w:\n",
    "    for folder in glob.glob(root_folder):\n",
    "        if os.path.isdir(folder):\n",
    "            exp_name = folder[folder.find('\\\\') + 1:]\n",
    "            \n",
    "            for log in glob.glob(os.path.join(folder, '*')):\n",
    "                if log.endswith('log'):\n",
    "                    log_dict = torch.load(log)\n",
    "                    \n",
    "                    if 'steps' not in log_dict:\n",
    "                        log_dict['steps'] = len(log_dict['losses']) if len(log_dict['losses']) % 100 == 0 else len(log_dict['losses']) + 1\n",
    "                    \n",
    "                    hparam_dict = {\n",
    "                        'model': log_dict['model'],\n",
    "                        'num_params': log_dict.get('num_params', 0),\n",
    "                        'crop_length': log_dict['crop_length'],\n",
    "                        'pooling': log_dict['pooling'],\n",
    "                        'starting_lr': log_dict['starting_lr'],\n",
    "                        'steps': log_dict['steps'],\n",
    "                        'input_norm': log_dict['input_norm'],\n",
    "                        'weight_decay': log_dict['weight_decay'],\n",
    "                        'mixup': log_dict['mixup'],\n",
    "                        'awgn': log_dict['awgn_std'],\n",
    "                        'loss_type': log_dict['loss_type'],\n",
    "                    }\n",
    "                    metric_dict = {\n",
    "                        'train_accuracy': log_dict['train_acc_history'][-1],\n",
    "                        'test_accuracy': max(log_dict['best_test_accuracy'], log_dict['last_test_accuracy']),\n",
    "                    }\n",
    "                    \n",
    "#                     if '1D-ResNet' not in log_dict['model']:\n",
    "#                         continue\n",
    "\n",
    "                    w.add_hparams(hparam_dict, metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f37c1-5090-4f8c-9b89-b0903a752c63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaf13c-d84a-4129-b453-9f82403fdb62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "797e5a4c-a53d-4d23-b088-604f94b0bc3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fcaa605-67e6-40da-ab6f-18978567881c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history_temp\\05_Train_1D_001_Mixup0.3_10s_new_normal_wd1e-1_awgn3e-1\\1D-ResNet_log                                      : max\n",
      "history_temp\\05_Train_1D_001_Mixup0.3_10s_new_normal_wd1e-1_awgn3e-1\\M5_log                                             : max\n",
      "history_temp\\05_Train_1D_001_wandb_10s_new_normal_wd1e-1_awgn3e-1_wandbtest\\1D-ResNet-18_log                            : max\n",
      "history_temp\\05_Train_1D_001_wandb_10s_new_normal_wd1e-1_awgn3e-1_wandbtest\\1D-ResNeXt-50_log                           : max\n",
      "history_temp\\05_Train_1D_001_wandb_10s_new_normal_wd1e-1_awgn3e-1_wandbtest\\M5_log                                      : max\n"
     ]
    }
   ],
   "source": [
    "root_folder = 'checkpoint/*'\n",
    "\n",
    "for folder in glob.glob(root_folder):\n",
    "    if os.path.isdir(folder):\n",
    "        exp_name = folder[folder.find('\\\\') + 1:].lower()\n",
    "\n",
    "        for log in glob.glob(os.path.join(folder, '*')):\n",
    "            if log.endswith('log'):\n",
    "                log_dict = torch.load(log)\n",
    "\n",
    "#                 if log_dict['model'] == '1D-ResNet':\n",
    "#                     log_dict['model'] = '1D-ResNet-18'\n",
    "\n",
    "#                 if 'awgn' in exp_name and 'awgn_std' not in log_dict:\n",
    "#                     awgn = exp_name[exp_name.find('awgn') + 4:]\n",
    "#                     awgn = awgn if awgn.find('_') == -1 else awgn[:awgn.find('_')]\n",
    "#                     log_dict['awgn_std'] = float(awgn)\n",
    "#                 else:\n",
    "#                     log_dict['awgn_std'] = log_dict.get('awgn_std', 0.0)\n",
    "\n",
    "#                 if 'wd' in exp_name and 'weight_decay' not in log_dict:\n",
    "#                     weight_decay = exp_name[exp_name.find('wd') + 2:]\n",
    "#                     weight_decay = weight_decay if weight_decay.find('_') == -1 else weight_decay[:weight_decay.find('_')]\n",
    "#                     log_dict['weight_decay'] = float(weight_decay)\n",
    "#                 else:\n",
    "#                     log_dict['weight_decay'] = log_dict.get('weight_decay', 1e-4)\n",
    "\n",
    "#                 if log_dict['final_pool'].lower() == 'avg':\n",
    "#                     log_dict['final_pool'] = 'max-avg'          \n",
    "#                 log_dict['pooling'] = log_dict['final_pool']\n",
    "#                 log_dict.pop('final_pool', None)\n",
    "                    \n",
    "#                 if 'new_normal' in exp_name and 'input_norm' not in log_dict:\n",
    "#                     log_dict['input_norm'] = 'dataset'\n",
    "#                 else:\n",
    "#                     log_dict['input_norm'] = 'datapoint'\n",
    "                    \n",
    "#                 if 'mixup' in exp_name and 'mixup' not in log_dict:\n",
    "#                     log_dict['mixup'] = 0.3\n",
    "#                 else:\n",
    "#                     log_dict['mixup'] = log_dict.get('mixup', 0.0)\n",
    "\t\t\t\t\n",
    "# \t\t\t\tif 'bce' in exp_name and 'loss_type' not in log_dict:\n",
    "#                     log_dict['loss_type'] = 'multi-BCE'\n",
    "#                 else:\n",
    "#                     log_dict['loss_type'] = log_dict.get('loss_type', 'cross-entropy')\n",
    "\n",
    "#                 p1 = re.compile('_[0-9.]+s')\n",
    "#                 p2 = re.compile('_[0-9]+ms')\n",
    "#                 p3 = re.compile('_[0-9]+m')\n",
    "#                 m1 = p1.search(exp_name)\n",
    "#                 m2 = p2.search(exp_name)\n",
    "#                 m3 = p3.search(exp_name)\n",
    "#                 if m1 and m2:\n",
    "#                     pass\n",
    "#                     # print(f'crop_length - dulplicated matched patterns: {exp_name}')\n",
    "#                 elif m1:\n",
    "#                     log_dict['crop_length'] = int(float(m1.group(0)[1:-1]) * 200)\n",
    "#                 elif m2:\n",
    "#                     log_dict['crop_length'] = int(float(m2.group(0)[1:-2]) / 1000 * 200)\n",
    "#                 elif m3:\n",
    "#                     log_dict['crop_length'] = int(float(m3.group(0)[1:-1]) * 200 * 60)\n",
    "#                 else:\n",
    "#                     log_dict['crop_length'] = 12000\n",
    "#                     # print(f'crop_length - no matched pattern: {exp_name}')    \n",
    "#                 if '211029_05_Train_1D_001_Remodel_10s_new_normal_wd1e-2_awgn1.5e-1'.lower() in exp_name:\n",
    "#                     log_dict['final_pool'] = 'max'\n",
    "\n",
    "                print(f'{log:<120}: {log_dict.get(\"pooling\", \"NOTHING\")}')\n",
    "                    \n",
    "                # torch.save(log_dict, log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924517d5-cb23-43f8-a7b2-723bdfddf845",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## synthesize the log file from spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "937a260e-8cea-49e2-a25f-4cf2a3c8e7da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook, styles\n",
    "\n",
    "log_file = os.path.join(r'C:\\Users\\IPIS-Minjae\\Desktop\\EEG classifier training schedule.xlsx')\n",
    "ws = load_workbook(log_file, data_only=True)['Stage1']\n",
    "\n",
    "num = 2\n",
    "while True:\n",
    "    if ws.cell(row=num, column=1).value is None:\n",
    "        break\n",
    "        \n",
    "    log_dict = {}\n",
    "    \n",
    "    # field 6: pre_processing\n",
    "    pre_process = ws.cell(row=num, column=6).value.lower()\n",
    "    if 'norm-per-data' in pre_process:\n",
    "        log_dict['input_norm'] = 'datapoint'\n",
    "    elif 'norm-dataset' in pre_process:\n",
    "        log_dict['input_norm'] = 'datapoint'\n",
    "        \n",
    "    # field 7: age\n",
    "    log_dict['age'] = ws.cell(row=num, column=7).value\n",
    "    \n",
    "    # field 8: network architecture\n",
    "    log_dict['model'] = ws.cell(row=num, column=8).value\n",
    "    if log_dict['age'].lower() == 'x':\n",
    "        log_dict['model'] += '-no-age'\n",
    "    \n",
    "    # field 9: pooling\n",
    "    pooling = ws.cell(row=num, column=9).value\n",
    "    log_dict['pooling'] = pooling\n",
    "    \n",
    "    # fiend 10: #params\n",
    "    log_dict['num_params'] = ws.cell(row=num, column=10).value\n",
    "    \n",
    "    # fiend 11: weight decay\n",
    "    log_dict['weight_decay'] = float(ws.cell(row=num, column=11).value)\n",
    "    \n",
    "    # field 12: learning rate\n",
    "    log_dict['starting_lr'] = float(ws.cell(row=num, column=12).value)\n",
    "    \n",
    "    # field 13: iterations\n",
    "    log_dict['steps'] = ws.cell(row=num, column=13).value\n",
    "    \n",
    "    # field 14: LR decay\n",
    "    log_dict['lr_decay_step'] = ws.cell(row=num, column=14).value\n",
    "    \n",
    "    # field 15: final train acc\n",
    "    if ws.cell(row=num, column=15).value <= 1.0:\n",
    "        log_dict['train_acc_history'] = [ws.cell(row=num, column=15).value * 100]\n",
    "    else:\n",
    "        log_dict['train_acc_history'] = [ws.cell(row=num, column=15).value]\n",
    "\n",
    "    # field 16: test accuracy\n",
    "    test_accuracy = ws.cell(row=num, column=16).value\n",
    "    if type(test_accuracy) is float:\n",
    "        log_dict['best_test_accuracy'] = test_accuracy if test_accuracy > 1.0 else test_accuracy * 100\n",
    "        log_dict['last_test_accuracy'] = test_accuracy if test_accuracy > 1.0 else test_accuracy * 100\n",
    "    else:\n",
    "        log_dict['best_test_accuracy'] = float(test_accuracy.split('/')[0].strip(' %'))\n",
    "        log_dict['last_test_accuracy'] = float(test_accuracy.split('/')[1].strip(' %'))\n",
    "    \n",
    "    # field 17: log\n",
    "    exp_name = ws.cell(row=num, column=17).value\n",
    "    \n",
    "    if 'awgn' in exp_name and 'awgn_std' not in log_dict:\n",
    "        awgn = exp_name[exp_name.find('awgn') + 4:]\n",
    "        awgn = awgn if awgn.find('_') == -1 else awgn[:awgn.find('_')]\n",
    "        log_dict['awgn_std'] = float(awgn)\n",
    "    else:\n",
    "        log_dict['awgn_std'] = log_dict.get('awgn_std', 0.0)\n",
    "        \n",
    "    if 'new_normal' in exp_name and 'input_norm' not in log_dict:\n",
    "        log_dict['input_norm'] = 'dataset'\n",
    "    else:\n",
    "        log_dict['input_norm'] = 'datapoint'\n",
    "        \n",
    "    if 'bce' in exp_name and 'loss_type' not in log_dict:\n",
    "        log_dict['loss_type'] = 'multi-BCE'\n",
    "    else:\n",
    "        log_dict['loss_type'] = log_dict.get('loss_type', 'cross-entropy')\n",
    "        \n",
    "    if 'mixup' in exp_name and 'mixup' not in log_dict:\n",
    "        log_dict['mixup'] = 0.3\n",
    "    else:\n",
    "        log_dict['mixup'] = log_dict.get('mixup', 0.0)\n",
    "                    \n",
    "    p1 = re.compile('_[0-9.]+s')\n",
    "    p2 = re.compile('_[0-9]+ms')\n",
    "    p3 = re.compile('_[0-9]+m')\n",
    "    m1 = p1.search(exp_name)\n",
    "    m2 = p2.search(exp_name)\n",
    "    m3 = p3.search(exp_name)\n",
    "    if m1 and m2:\n",
    "        pass\n",
    "        # print(f'crop_length - dulplicated matched patterns: {exp_name}')\n",
    "    elif m1:\n",
    "        log_dict['crop_length'] = int(float(m1.group(0)[1:-1]) * 200)\n",
    "    elif m2:\n",
    "        log_dict['crop_length'] = int(float(m2.group(0)[1:-2]) / 1000 * 200)\n",
    "    elif m3:\n",
    "        log_dict['crop_length'] = int(float(m3.group(0)[1:-1]) * 200 * 60)\n",
    "    else:\n",
    "        log_dict['crop_length'] = 12000\n",
    "        # print(f'crop_length - no matched pattern: {exp_name}')\n",
    "    \n",
    "    if os.path.isdir(os.path.join('checkpoint/', exp_name)) is False:        \n",
    "        root_folder = r'C:\\Users\\IPIS-Minjae\\Desktop\\others'\n",
    "        os.makedirs(root_folder, exist_ok=True)\n",
    "        path = os.path.join(root_folder, exp_name + '_' + log_dict['model'] + '_log')\n",
    "        torch.save(log_dict, path)\n",
    "    \n",
    "    # move the pivot row\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cf605-5552-43d7-88bc-a76a731f56ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630963b-1b15-4e31-8f7c-f97139e8a448",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}