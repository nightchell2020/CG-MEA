{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EEG Data Statistics\n",
    "\n",
    "`01_Data_Curation1`과 `02_Data_Curation2`에서 저장한 EEG 데이터의 분포를 전반적으로 살펴보는 노트북."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minjae\\Desktop\\EEG_Project\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import statistics\n",
    "\n",
    "import pprint\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# custom package\n",
    "from datasets.caueeg_dataset import CauEegDataset\n",
    "from datasets.caueeg_data_curation import MultiEegLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Curated Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data file path\n",
    "curate_folder = r'local/dataset/caueeg-dataset'\n",
    "output_folder = r'local/output/imgs'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'age': 78, 'birth': '1940-06-02', 'dx1': 'mci_rf', 'edfname': '00001809_261018', 'record': '2018-10-26T15:46:26', 'serial': '00001', 'symptom': ['mci', 'mci_amnestic', 'mci_amnestic_rf']},\n",
      "          {'age': 56, 'birth': '1960-12-04', 'dx1': 'smi', 'edfname': '00029426_020817', 'record': '2017-08-02T16:14:56', 'serial': '00002', 'symptom': ['normal', 'smi']},\n",
      "          {'age': 93, 'birth': '1924-10-19', 'dx1': 'vascular mci', 'edfname': '00047327_090718', 'record': '2018-07-09T15:29:10', 'serial': '00003', 'symptom': ['mci', 'mci_vascular']},\n",
      "          {'age': 78, 'birth': '1941-03-16', 'dx1': 'load', 'edfname': '00048377_070819', 'record': '2019-08-07T13:55:25', 'serial': '00004', 'symptom': ['dementia', 'ad', 'load']},\n",
      "          {'age': 75, 'birth': '1941-03-16', 'dx1': 'mci (ef) multi-domain', 'edfname': '00048377_070916', 'record': '2016-09-07T10:36:01', 'serial': '00005', 'symptom': ['mci', 'mci_amnestic', 'mci_amnestic_ef', 'mci_multi_domain']}],\n",
      " 'dataset_name': 'CAUEEG dataset',\n",
      " 'signal_header': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(curate_folder, 'annotation_debug.json'), 'r') as json_file:\n",
    "    annotation = json.load(json_file)\n",
    "\n",
    "pprint.pprint({k: (v if k != 'data' else v[:5]) for (k, v) in annotation.items()}, width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAUEEG-Dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $\\leftrightarrow$ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "28\n",
      "\n",
      "min delta: 10\n",
      "mean delta: 370\n",
      "median delta: 178\n",
      "max delta: 1425\n",
      "\n",
      "symp_diff_count: 6\n",
      "symp_diff_count2: 11\n",
      "[({'load', 'ad', 'dementia'}, {'mci_amnestic', 'mci_amnestic_ef', 'mci'}),\n",
      " ({'mci_amnestic', 'mci_amnestic_ef', 'mci'}, {'eoad', 'ad', 'dementia'}),\n",
      " ({'mci_amnestic', 'mci_amnestic_ef', 'mci'}, {'load', 'ad', 'dementia'}),\n",
      " ({'mci_amnestic', 'mci_amnestic_ef', 'mci'}, {'load', 'ad', 'dementia'}),\n",
      " ({'mci', 'mci_vascular'}, {'vd', 'dementia', 'sivd'}),\n",
      " ({'mci_multi_domain', 'mci_amnestic', 'mci_amnestic_ef', 'mci'},\n",
      "  {'normal', 'smi'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('local/dataset/caueeg-dataset/dementia.json', 'r') as json_file:\n",
    "    dementia_dataset = json.load(json_file)\n",
    "\n",
    "# find edf information from annotation\n",
    "for train_data in dementia_dataset['train_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if train_data['serial'] == anno_data['serial']:\n",
    "            train_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            train_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "    \n",
    "for test_data in dementia_dataset['test_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if test_data['serial'] == anno_data['serial']:\n",
    "            test_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            test_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "\n",
    "# find duplicates\n",
    "duplicates = []\n",
    "for test_data in dementia_dataset['test_split']:\n",
    "    if test_data['edfname'] in [d['edfname'] for d in duplicates]:\n",
    "        for dup_data in duplicates:\n",
    "            if dup_data['edfname'] == test_data['edfname']:\n",
    "                dup_data['test_serials'].add(test_data['serial'])\n",
    "                dup_data['test_records'].add(test_data['record'])\n",
    "                break\n",
    "    else:\n",
    "        for train_data in dementia_dataset['train_split']:\n",
    "            if test_data['edfname'] == train_data['edfname']:\n",
    "                dup_data = {'edfname': test_data['edfname'], \n",
    "                            'test_serials': set([test_data['serial']]), \n",
    "                            'test_records': set([test_data['record']]),\n",
    "                            'test_symptoms': set([*test_data['symptom']]),\n",
    "                            'train_serials': set([train_data['serial']]), \n",
    "                            'train_records': set([train_data['record']]),\n",
    "                            'train_symptoms': set([*train_data['symptom']])\n",
    "                           }\n",
    "                duplicates.append(dup_data)\n",
    "                break\n",
    "\n",
    "for train_data in dementia_dataset['train_split']:\n",
    "    for dup_data in duplicates:\n",
    "        if dup_data['edfname'] == train_data['edfname']:\n",
    "            dup_data['train_serials'].add(train_data['serial'])\n",
    "            dup_data['train_records'].add(train_data['record'])\n",
    "            break\n",
    "print(len(dementia_dataset['test_split']))\n",
    "print(len(duplicates))\n",
    "print()\n",
    "\n",
    "delta_list = []\n",
    "symp_diff_count = 0\n",
    "symp_diff_count2 = 0\n",
    "symp_diff_list = []\n",
    "minimum_case = None\n",
    "\n",
    "for i, dup_data in enumerate(duplicates):\n",
    "    for test_record in dup_data['test_records']:\n",
    "        for train_record in dup_data['train_records']:\n",
    "            delta = abs(test_record - train_record)\n",
    "            delta_list.append(delta)\n",
    "    \n",
    "    if dup_data['train_symptoms'] != dup_data['test_symptoms']:\n",
    "        symp_diff_list.append((dup_data['train_symptoms'], dup_data['test_symptoms']))\n",
    "        symp_diff_count += len(dup_data['test_serials'])\n",
    "        symp_diff_count2 += len(dup_data['train_serials'])\n",
    "            \n",
    "sum_delta = datetime.timedelta(days=0)\n",
    "for delta in delta_list:\n",
    "    sum_delta += delta\n",
    "\n",
    "print('min delta:', round(min(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('mean delta:', round(sum_delta / len(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('median delta:', round(statistics.median(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('max delta:', round(max(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print()\n",
    "print('symp_diff_count:', symp_diff_count)\n",
    "print('symp_diff_count2:', symp_diff_count2)\n",
    "pprint.pprint(symp_diff_list)\n",
    "print()\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "\n",
      "28\n",
      "['00789', '00934', '01034', '00495', '01028', '00294', '00565', '01087', '00860', '00004', '00709', '01252', '00435', '00061', '00837', '00841', '00301', '00690', '00259', '00298', '01091', '00243', '00770', '01118', '00787', '00516', '00828', '00143']\n",
      "\n",
      "39\n",
      "['00790', '00933', '01035', '01037', '00493', '01029', '00293', '00566', '01088', '00859', '00858', '00857', '00006', '00005', '00708', '00710', '00707', '01253', '00436', '00434', '00063', '00062', '00838', '00840', '00839', '00302', '00691', '00258', '00299', '00297', '01092', '00244', '00771', '01119', '00788', '00515', '00829', '00145', '00144']\n"
     ]
    }
   ],
   "source": [
    "test_serials = [test_serial for d in duplicates for test_serial in d['test_serials']]\n",
    "train_serials = [train_serial for d in duplicates for train_serial in d['train_serials']]\n",
    "\n",
    "print(len(duplicates))\n",
    "print()\n",
    "print(len(test_serials))\n",
    "print(test_serials)\n",
    "print()\n",
    "print(len(train_serials))\n",
    "print(train_serials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $\\leftrightarrow$ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "35\n",
      "\n",
      "min delta: 7\n",
      "mean delta: 329\n",
      "median delta: 136\n",
      "max delta: 1417\n",
      "\n",
      "symp_diff_count: 5\n",
      "symp_diff_count2: 5\n",
      "[({'normal', 'smi'}, {'mci_amnestic', 'mci_ad', 'mci'}),\n",
      " ({'load', 'ad', 'dementia'}, {'mci_amnestic', 'mci_amnestic_ef', 'mci'}),\n",
      " ({'mci_amnestic', 'mci_amnestic_ef', 'mci'},\n",
      "  {'mci_amnestic', 'mci', 'mci_amnestic_rf'}),\n",
      " ({'mci_amnestic', 'mci_amnestic_ef', 'mci'}, {'eoad', 'ad', 'dementia'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('local/dataset/caueeg-dataset/dementia.json', 'r') as json_file:\n",
    "    dementia_dataset = json.load(json_file)\n",
    "\n",
    "# find edf information from annotation\n",
    "for train_data in dementia_dataset['train_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if train_data['serial'] == anno_data['serial']:\n",
    "            train_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            train_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "    \n",
    "for validation_data in dementia_dataset['validation_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if validation_data['serial'] == anno_data['serial']:\n",
    "            validation_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            validation_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "            \n",
    "duplicates = []\n",
    "for validation_data in dementia_dataset['validation_split']:\n",
    "    if validation_data['edfname'] in [d['edfname'] for d in duplicates]:\n",
    "        for dup_data in duplicates:\n",
    "            if dup_data['edfname'] == validation_data['edfname']:\n",
    "                dup_data['validation_serials'].add(validation_data['serial'])\n",
    "                dup_data['validation_records'].add(validation_data['record'])\n",
    "                break\n",
    "    else:\n",
    "        for train_data in dementia_dataset['train_split']:\n",
    "            if validation_data['edfname'] == train_data['edfname']:\n",
    "                dup_data = {'edfname': validation_data['edfname'], \n",
    "                            'validation_serials': set([validation_data['serial']]), \n",
    "                            'validation_records': set([validation_data['record']]),\n",
    "                            'validation_symptoms': set([*validation_data['symptom']]),\n",
    "                            'train_serials': set([train_data['serial']]), \n",
    "                            'train_records': set([train_data['record']]),\n",
    "                            'train_symptoms': set([*train_data['symptom']])\n",
    "                           }\n",
    "                duplicates.append(dup_data)\n",
    "                break\n",
    "\n",
    "for train_data in dementia_dataset['train_split']:\n",
    "    for dup_data in duplicates:\n",
    "        if dup_data['edfname'] == train_data['edfname']:\n",
    "            dup_data['train_serials'].add(train_data['serial'])\n",
    "            dup_data['train_records'].add(train_data['record'])\n",
    "            break\n",
    "print(len(dementia_dataset['validation_split']))\n",
    "print(len(duplicates))\n",
    "print()\n",
    "\n",
    "delta_list = []\n",
    "symp_diff_count = 0\n",
    "symp_diff_count2 = 0\n",
    "symp_diff_list = []\n",
    "minimum_case = None\n",
    "\n",
    "for i, dup_data in enumerate(duplicates):\n",
    "    for validation_record in dup_data['validation_records']:\n",
    "        for train_record in dup_data['train_records']:\n",
    "            delta = abs(validation_record - train_record)\n",
    "            delta_list.append(delta)\n",
    "    \n",
    "    if dup_data['train_symptoms'] != dup_data['validation_symptoms']:\n",
    "        symp_diff_list.append((dup_data['train_symptoms'], dup_data['validation_symptoms']))\n",
    "        symp_diff_count += len(dup_data['validation_serials'])\n",
    "        symp_diff_count2 += len(dup_data['train_serials'])\n",
    "            \n",
    "sum_delta = datetime.timedelta(days=0)\n",
    "for delta in delta_list:\n",
    "    sum_delta += delta\n",
    "\n",
    "print('min delta:', round(min(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('mean delta:', round(sum_delta / len(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('median delta:', round(statistics.median(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('max delta:', round(max(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print()\n",
    "print('symp_diff_count:', symp_diff_count)\n",
    "print('symp_diff_count2:', symp_diff_count2)\n",
    "pprint.pprint(symp_diff_list)\n",
    "print()\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "\n",
      "37\n",
      "['00172', '00807', '01052', '01036', '00999', '00935', '01310', '01309', '00586', '00957', '00539', '00627', '00753', '01101', '00481', '01099', '01241', '01305', '00039', '00338', '00917', '00938', '00028', '00478', '01178', '01058', '01156', '01165', '00460', '00276', '00974', '00729', '00701', '00703', '00494', '00160', '00992']\n",
      "\n",
      "50\n",
      "['00171', '00173', '00808', '00806', '01053', '01035', '01037', '01000', '00933', '01308', '01307', '00587', '00956', '00538', '00628', '00752', '01102', '00480', '00482', '01100', '01242', '01240', '01304', '00040', '00337', '00918', '00937', '00026', '00031', '00029', '00030', '00027', '00477', '01179', '01059', '01155', '01167', '01166', '00459', '00277', '00975', '00973', '00728', '00702', '00493', '00161', '00988', '00991', '00989', '00990']\n"
     ]
    }
   ],
   "source": [
    "validation_serials = [validation_serial for d in duplicates for validation_serial in d['validation_serials']]\n",
    "train_serials = [train_serial for d in duplicates for train_serial in d['train_serials']]\n",
    "\n",
    "print(len(duplicates))\n",
    "print()\n",
    "print(len(validation_serials))\n",
    "print(validation_serials)\n",
    "print()\n",
    "print(len(train_serials))\n",
    "print(train_serials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test $\\leftrightarrow$ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "3\n",
      "\n",
      "min delta: 14\n",
      "mean delta: 391\n",
      "median delta: 314\n",
      "max delta: 846\n",
      "\n",
      "symp_diff_count: 1\n",
      "symp_diff_count2: 1\n",
      "[({'mci_amnestic', 'mci_amnestic_ef', 'mci'}, {'load', 'ad', 'dementia'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('local/dataset/caueeg-dataset/dementia.json', 'r') as json_file:\n",
    "    dementia_dataset = json.load(json_file)\n",
    "\n",
    "# find edf information from annotation\n",
    "for test_data in dementia_dataset['test_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if test_data['serial'] == anno_data['serial']:\n",
    "            test_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            test_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "    \n",
    "for validation_data in dementia_dataset['validation_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if validation_data['serial'] == anno_data['serial']:\n",
    "            validation_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            validation_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "            \n",
    "duplicates = []\n",
    "for validation_data in dementia_dataset['validation_split']:\n",
    "    if validation_data['edfname'] in [d['edfname'] for d in duplicates]:\n",
    "        for dup_data in duplicates:\n",
    "            if dup_data['edfname'] == validation_data['edfname']:\n",
    "                dup_data['validation_serials'].add(validation_data['serial'])\n",
    "                dup_data['validation_records'].add(validation_data['record'])\n",
    "                break\n",
    "    else:\n",
    "        for test_data in dementia_dataset['test_split']:\n",
    "            if validation_data['edfname'] == test_data['edfname']:\n",
    "                dup_data = {'edfname': validation_data['edfname'], \n",
    "                            'validation_serials': set([validation_data['serial']]), \n",
    "                            'validation_records': set([validation_data['record']]),\n",
    "                            'validation_symptoms': set([*validation_data['symptom']]),\n",
    "                            'test_serials': set([test_data['serial']]), \n",
    "                            'test_records': set([test_data['record']]),\n",
    "                            'test_symptoms': set([*test_data['symptom']])\n",
    "                           }\n",
    "                duplicates.append(dup_data)\n",
    "                break\n",
    "\n",
    "for test_data in dementia_dataset['test_split']:\n",
    "    for dup_data in duplicates:\n",
    "        if dup_data['edfname'] == test_data['edfname']:\n",
    "            dup_data['test_serials'].add(test_data['serial'])\n",
    "            dup_data['test_records'].add(test_data['record'])\n",
    "            break\n",
    "print(len(dementia_dataset['validation_split']))\n",
    "print(len(duplicates))\n",
    "print()\n",
    "\n",
    "delta_list = []\n",
    "symp_diff_count = 0\n",
    "symp_diff_count2 = 0\n",
    "symp_diff_list = []\n",
    "minimum_case = None\n",
    "\n",
    "for i, dup_data in enumerate(duplicates):\n",
    "    for validation_record in dup_data['validation_records']:\n",
    "        for test_record in dup_data['test_records']:\n",
    "            delta = abs(validation_record - test_record)\n",
    "            delta_list.append(delta)\n",
    "    \n",
    "    if dup_data['test_symptoms'] != dup_data['validation_symptoms']:\n",
    "        symp_diff_list.append((dup_data['test_symptoms'], dup_data['validation_symptoms']))\n",
    "        symp_diff_count += len(dup_data['validation_serials'])\n",
    "        symp_diff_count2 += len(dup_data['test_serials'])\n",
    "            \n",
    "sum_delta = datetime.timedelta(days=0)\n",
    "for delta in delta_list:\n",
    "    sum_delta += delta\n",
    "\n",
    "print('min delta:', round(min(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('mean delta:', round(sum_delta / len(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('median delta:', round(statistics.median(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('max delta:', round(max(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print()\n",
    "print('symp_diff_count:', symp_diff_count)\n",
    "print('symp_diff_count2:', symp_diff_count2)\n",
    "pprint.pprint(symp_diff_list)\n",
    "print()\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "3\n",
      "['01036', '00935', '00494']\n",
      "\n",
      "3\n",
      "['01034', '00934', '00495']\n"
     ]
    }
   ],
   "source": [
    "validation_serials = [validation_serial for d in duplicates for validation_serial in d['validation_serials']]\n",
    "test_serials = [test_serial for d in duplicates for test_serial in d['test_serials']]\n",
    "\n",
    "print(len(duplicates))\n",
    "print()\n",
    "print(len(validation_serials))\n",
    "print(validation_serials)\n",
    "print()\n",
    "print(len(test_serials))\n",
    "print(test_serials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAUEEG-Abnormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $\\leftrightarrow$ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "27\n",
      "\n",
      "min delta: 7\n",
      "mean delta: 340\n",
      "median delta: 80\n",
      "max delta: 1080\n",
      "\n",
      "symp_diff_count: 6\n",
      "symp_diff_count2: 9\n",
      "[({'mci_amnestic', 'mci_amnestic_ef', 'mci'},\n",
      "  {'mci_amnestic', 'mci', 'mci_amnestic_rf'}),\n",
      " ({'mci_multi_domain', 'mci_amnestic', 'mci_amnestic_ef', 'mci'},\n",
      "  {'normal', 'smi'}),\n",
      " ({'load', 'ad', 'dementia'}, {'mci_amnestic', 'mci'}),\n",
      " ({'load', 'ad', 'dementia'}, {'mci_amnestic', 'mci_amnestic_ef', 'mci'}),\n",
      " ({'load', 'ad', 'dementia'}, {'mci_amnestic', 'mci_amnestic_ef', 'mci'}),\n",
      " ({'mci', 'mci_non_amnestic'}, {'ftd', 'bvftd'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('local/dataset/caueeg-dataset/abnormal.json', 'r') as json_file:\n",
    "    abnormal_dataset = json.load(json_file)\n",
    "\n",
    "# find edf information from annotation\n",
    "for train_data in abnormal_dataset['train_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if train_data['serial'] == anno_data['serial']:\n",
    "            train_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            train_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "    \n",
    "for test_data in abnormal_dataset['test_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if test_data['serial'] == anno_data['serial']:\n",
    "            test_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            test_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "            \n",
    "duplicates = []\n",
    "for test_data in abnormal_dataset['test_split']:\n",
    "    if test_data['edfname'] in [d['edfname'] for d in duplicates]:\n",
    "        for dup_data in duplicates:\n",
    "            if dup_data['edfname'] == test_data['edfname']:\n",
    "                dup_data['test_serials'].add(test_data['serial'])\n",
    "                dup_data['test_records'].add(test_data['record'])\n",
    "                break\n",
    "    else:\n",
    "        for train_data in abnormal_dataset['train_split']:\n",
    "            if test_data['edfname'] == train_data['edfname']:\n",
    "                dup_data = {'edfname': test_data['edfname'], \n",
    "                            'test_serials': set([test_data['serial']]), \n",
    "                            'test_records': set([test_data['record']]),\n",
    "                            'test_symptoms': set([*test_data['symptom']]),\n",
    "                            'train_serials': set([train_data['serial']]), \n",
    "                            'train_records': set([train_data['record']]),\n",
    "                            'train_symptoms': set([*train_data['symptom']])\n",
    "                           }\n",
    "                duplicates.append(dup_data)\n",
    "                break\n",
    "\n",
    "for train_data in abnormal_dataset['train_split']:\n",
    "    for dup_data in duplicates:\n",
    "        if dup_data['edfname'] == train_data['edfname']:\n",
    "            dup_data['train_serials'].add(train_data['serial'])\n",
    "            dup_data['train_records'].add(train_data['record'])\n",
    "            break\n",
    "print(len(abnormal_dataset['test_split']))\n",
    "print(len(duplicates))\n",
    "print()\n",
    "\n",
    "delta_list = []\n",
    "symp_diff_count = 0\n",
    "symp_diff_count2 = 0\n",
    "symp_diff_list = []\n",
    "minimum_case = None\n",
    "\n",
    "for i, dup_data in enumerate(duplicates):\n",
    "    for test_record in dup_data['test_records']:\n",
    "        for train_record in dup_data['train_records']:\n",
    "            delta = abs(test_record - train_record)\n",
    "            delta_list.append(delta)\n",
    "    \n",
    "    if dup_data['train_symptoms'] != dup_data['test_symptoms']:\n",
    "        symp_diff_list.append((dup_data['train_symptoms'], dup_data['test_symptoms']))\n",
    "        symp_diff_count += len(dup_data['test_serials'])\n",
    "        symp_diff_count2 += len(dup_data['train_serials'])\n",
    "            \n",
    "sum_delta = datetime.timedelta(days=0)\n",
    "for delta in delta_list:\n",
    "    sum_delta += delta\n",
    "\n",
    "print('min delta:', round(min(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('mean delta:', round(sum_delta / len(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('median delta:', round(statistics.median(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('max delta:', round(max(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print()\n",
    "print('symp_diff_count:', symp_diff_count)\n",
    "print('symp_diff_count2:', symp_diff_count2)\n",
    "pprint.pprint(symp_diff_list)\n",
    "print()\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "\n",
      "27\n",
      "['01156', '00999', '00481', '00690', '00787', '00028', '00298', '00172', '00841', '00753', '01118', '01178', '00860', '01165', '00494', '00259', '00938', '00586', '00371', '00039', '00243', '00789', '00750', '00565', '01028', '00516', '00924']\n",
      "\n",
      "39\n",
      "['01155', '01000', '00482', '00480', '00691', '00788', '00026', '00031', '00029', '00030', '00027', '00299', '00297', '00171', '00173', '00840', '00839', '00752', '01119', '01179', '00858', '00857', '00859', '01167', '01166', '00493', '00495', '00258', '00937', '00587', '00373', '00040', '00244', '00790', '00749', '00566', '01029', '00515', '00925']\n"
     ]
    }
   ],
   "source": [
    "test_serials = [test_serial for d in duplicates for test_serial in d['test_serials']]\n",
    "train_serials = [train_serial for d in duplicates for train_serial in d['train_serials']]\n",
    "\n",
    "print(len(duplicates))\n",
    "print()\n",
    "print(len(test_serials))\n",
    "print(test_serials)\n",
    "print()\n",
    "print(len(train_serials))\n",
    "print(train_serials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $\\leftrightarrow$ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "27\n",
      "\n",
      "min delta: 9\n",
      "mean delta: 361\n",
      "median delta: 119\n",
      "max delta: 1508\n",
      "\n",
      "symp_diff_count: 2\n",
      "symp_diff_count2: 5\n",
      "[({'load', 'ad', 'dementia'}, {'mci_amnestic', 'mci_amnestic_ef', 'mci'}),\n",
      " ({'mci', 'mci_vascular'}, {'vd', 'dementia', 'sivd'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('local/dataset/caueeg-dataset/abnormal.json', 'r') as json_file:\n",
    "    abnormal_dataset = json.load(json_file)\n",
    "\n",
    "# find edf information from annotation\n",
    "for train_data in abnormal_dataset['train_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if train_data['serial'] == anno_data['serial']:\n",
    "            train_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            train_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "    \n",
    "for validation_data in abnormal_dataset['validation_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if validation_data['serial'] == anno_data['serial']:\n",
    "            validation_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            validation_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "            \n",
    "duplicates = []\n",
    "for validation_data in abnormal_dataset['validation_split']:\n",
    "    if validation_data['edfname'] in [d['edfname'] for d in duplicates]:\n",
    "        for dup_data in duplicates:\n",
    "            if dup_data['edfname'] == validation_data['edfname']:\n",
    "                dup_data['validation_serials'].add(validation_data['serial'])\n",
    "                dup_data['validation_records'].add(validation_data['record'])\n",
    "                break\n",
    "    else:\n",
    "        for train_data in abnormal_dataset['train_split']:\n",
    "            if validation_data['edfname'] == train_data['edfname']:\n",
    "                dup_data = {'edfname': validation_data['edfname'], \n",
    "                            'validation_serials': set([validation_data['serial']]), \n",
    "                            'validation_records': set([validation_data['record']]),\n",
    "                            'validation_symptoms': set([*validation_data['symptom']]),\n",
    "                            'train_serials': set([train_data['serial']]), \n",
    "                            'train_records': set([train_data['record']]),\n",
    "                            'train_symptoms': set([*train_data['symptom']])\n",
    "                           }\n",
    "                duplicates.append(dup_data)\n",
    "                break\n",
    "\n",
    "for train_data in abnormal_dataset['train_split']:\n",
    "    for dup_data in duplicates:\n",
    "        if dup_data['edfname'] == train_data['edfname']:\n",
    "            dup_data['train_serials'].add(train_data['serial'])\n",
    "            dup_data['train_records'].add(train_data['record'])\n",
    "            break\n",
    "print(len(abnormal_dataset['validation_split']))\n",
    "print(len(duplicates))\n",
    "print()\n",
    "\n",
    "delta_list = []\n",
    "symp_diff_count = 0\n",
    "symp_diff_count2 = 0\n",
    "symp_diff_list = []\n",
    "minimum_case = None\n",
    "\n",
    "for i, dup_data in enumerate(duplicates):\n",
    "    for validation_record in dup_data['validation_records']:\n",
    "        for train_record in dup_data['train_records']:\n",
    "            delta = abs(validation_record - train_record)\n",
    "            delta_list.append(delta)\n",
    "    \n",
    "    if dup_data['train_symptoms'] != dup_data['validation_symptoms']:\n",
    "        symp_diff_list.append((dup_data['train_symptoms'], dup_data['validation_symptoms']))\n",
    "        symp_diff_count += len(dup_data['validation_serials'])\n",
    "        symp_diff_count2 += len(dup_data['train_serials'])\n",
    "            \n",
    "sum_delta = datetime.timedelta(days=0)\n",
    "for delta in delta_list:\n",
    "    sum_delta += delta\n",
    "\n",
    "print('min delta:', round(min(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('mean delta:', round(sum_delta / len(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('median delta:', round(statistics.median(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('max delta:', round(max(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print()\n",
    "print('symp_diff_count:', symp_diff_count)\n",
    "print('symp_diff_count2:', symp_diff_count2)\n",
    "pprint.pprint(symp_diff_list)\n",
    "print()\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "\n",
      "31\n",
      "['01034', '00372', '00934', '00935', '00160', '00427', '00807', '01310', '01309', '00294', '01260', '01259', '00729', '01305', '01087', '01091', '00718', '00627', '00917', '00478', '00770', '01058', '00393', '00395', '00338', '00709', '00061', '01052', '00837', '01101', '00957']\n",
      "\n",
      "36\n",
      "['01035', '01036', '01037', '00373', '00933', '00161', '00426', '00428', '00808', '00806', '01308', '01307', '00293', '01261', '00728', '01304', '01088', '01092', '00719', '00628', '00918', '00477', '00771', '01059', '00392', '00394', '00337', '00708', '00710', '00707', '00063', '00062', '01053', '00838', '01102', '00956']\n"
     ]
    }
   ],
   "source": [
    "validation_serials = [validation_serial for d in duplicates for validation_serial in d['validation_serials']]\n",
    "train_serials = [train_serial for d in duplicates for train_serial in d['train_serials']]\n",
    "\n",
    "print(len(duplicates))\n",
    "print()\n",
    "print(len(validation_serials))\n",
    "print(validation_serials)\n",
    "print()\n",
    "print(len(train_serials))\n",
    "print(train_serials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test $\\leftrightarrow$ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "3\n",
      "\n",
      "min delta: 33\n",
      "mean delta: 413\n",
      "median delta: 36\n",
      "max delta: 1170\n",
      "\n",
      "symp_diff_count: 1\n",
      "symp_diff_count2: 1\n",
      "[({'ftd', 'bvftd'}, {'mci_multi_domain', 'mci_amnestic', 'mci'})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('local/dataset/caueeg-dataset/abnormal.json', 'r') as json_file:\n",
    "    abnormal_dataset = json.load(json_file)\n",
    "\n",
    "# find edf information from annotation\n",
    "for test_data in abnormal_dataset['test_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if test_data['serial'] == anno_data['serial']:\n",
    "            test_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            test_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "    \n",
    "for validation_data in abnormal_dataset['validation_split']:\n",
    "    for anno_data in annotation['data']:\n",
    "        if validation_data['serial'] == anno_data['serial']:\n",
    "            validation_data['edfname'] = anno_data['edfname'].split('_')[0]\n",
    "            validation_data['record'] = datetime.datetime.fromisoformat(anno_data['record'])\n",
    "            break\n",
    "            \n",
    "duplicates = []\n",
    "for validation_data in abnormal_dataset['validation_split']:\n",
    "    if validation_data['edfname'] in [d['edfname'] for d in duplicates]:\n",
    "        for dup_data in duplicates:\n",
    "            if dup_data['edfname'] == validation_data['edfname']:\n",
    "                dup_data['validation_serials'].add(validation_data['serial'])\n",
    "                dup_data['validation_records'].add(validation_data['record'])\n",
    "                break\n",
    "    else:\n",
    "        for test_data in abnormal_dataset['test_split']:\n",
    "            if validation_data['edfname'] == test_data['edfname']:\n",
    "                dup_data = {'edfname': validation_data['edfname'], \n",
    "                            'validation_serials': set([validation_data['serial']]), \n",
    "                            'validation_records': set([validation_data['record']]),\n",
    "                            'validation_symptoms': set([*validation_data['symptom']]),\n",
    "                            'test_serials': set([test_data['serial']]), \n",
    "                            'test_records': set([test_data['record']]),\n",
    "                            'test_symptoms': set([*test_data['symptom']])\n",
    "                           }\n",
    "                duplicates.append(dup_data)\n",
    "                break\n",
    "\n",
    "for test_data in abnormal_dataset['test_split']:\n",
    "    for dup_data in duplicates:\n",
    "        if dup_data['edfname'] == test_data['edfname']:\n",
    "            dup_data['test_serials'].add(test_data['serial'])\n",
    "            dup_data['test_records'].add(test_data['record'])\n",
    "            break\n",
    "print(len(abnormal_dataset['validation_split']))\n",
    "print(len(duplicates))\n",
    "print()\n",
    "\n",
    "delta_list = []\n",
    "symp_diff_count = 0\n",
    "symp_diff_count2 = 0\n",
    "symp_diff_list = []\n",
    "minimum_case = None\n",
    "\n",
    "for i, dup_data in enumerate(duplicates):\n",
    "    for validation_record in dup_data['validation_records']:\n",
    "        for test_record in dup_data['test_records']:\n",
    "            delta = abs(validation_record - test_record)\n",
    "            delta_list.append(delta)\n",
    "    \n",
    "    if dup_data['test_symptoms'] != dup_data['validation_symptoms']:\n",
    "        symp_diff_list.append((dup_data['test_symptoms'], dup_data['validation_symptoms']))\n",
    "        symp_diff_count += len(dup_data['validation_serials'])\n",
    "        symp_diff_count2 += len(dup_data['test_serials'])\n",
    "            \n",
    "sum_delta = datetime.timedelta(days=0)\n",
    "for delta in delta_list:\n",
    "    sum_delta += delta\n",
    "\n",
    "print('min delta:', round(min(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('mean delta:', round(sum_delta / len(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('median delta:', round(statistics.median(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print('max delta:', round(max(delta_list) / datetime.timedelta(hours=1) / 24))\n",
    "print()\n",
    "print('symp_diff_count:', symp_diff_count)\n",
    "print('symp_diff_count2:', symp_diff_count2)\n",
    "pprint.pprint(symp_diff_list)\n",
    "print()\n",
    "# pprint.pprint(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "3\n",
      "['00372', '00169', '00498']\n",
      "\n",
      "3\n",
      "['00371', '00168', '00499']\n"
     ]
    }
   ],
   "source": [
    "validation_serials = [validation_serial for d in duplicates for validation_serial in d['validation_serials']]\n",
    "test_serials = [test_serial for d in duplicates for test_serial in d['test_serials']]\n",
    "\n",
    "print(len(duplicates))\n",
    "print()\n",
    "print(len(validation_serials))\n",
    "print(validation_serials)\n",
    "print()\n",
    "print(len(test_serials))\n",
    "print(test_serials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
