{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Finetune Masked-AutoEncoder\n",
    "\n",
    "- Finetune the deep network after pretraining the self-supervised learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/imkbsz/workspace/eeg_analysis\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from collections import OrderedDict\n",
    "from cycler import cycler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "# custom package\n",
    "from run_train import check_device_env\n",
    "from run_train import set_seed\n",
    "from run_train import compose_dataset\n",
    "from run_train import generate_model\n",
    "from train.ssl_train_script import ssl_train_script\n",
    "from train.train_script import train_script\n",
    "from models.utils import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Specify the dataset, model, and train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_model_path = 'local/checkpoint/'\n",
    "pre_model_name = 'lz7ffvxk'\n",
    "device = 'cuda:2'\n",
    "\n",
    "############\n",
    "# Artifact #\n",
    "############\n",
    "art_config = dict()\n",
    "art_config['project'] = 'tuab-mae-artifact'\n",
    "art_config['use_wandb'] = False\n",
    "art_config['pre_model'] = pre_model_name\n",
    "art_config['device'] = device\n",
    "\n",
    "# art_config[\"art_filter_list\"] = [9, 9, 9, 9, 9]\n",
    "art_config[\"art_dropout\"] = 0.1\n",
    "art_config[\"art_use_age\"] = \"no\"  # \"conv\", \"embedding\", \"no\"\n",
    "\n",
    "art_config['total_samples'] = 5.0e+5\n",
    "art_config['search_lr'] = False\n",
    "art_config['base_lr'] = 1e-3\n",
    "art_config['lr_scheduler_type'] = 'cosine_decay_with_warmup_half'\n",
    "\n",
    "art_config[\"warmup_min\"] = 150\n",
    "art_config[\"num_history\"] = 50\n",
    "art_config['save_model'] = False\n",
    "\n",
    "##################\n",
    "# Classification #\n",
    "##################\n",
    "finetune_config = dict()\n",
    "finetune_config['project'] = 'tuab-mae-artifact-finetune'\n",
    "finetune_config['use_wandb'] = True\n",
    "finetune_config['pre_model'] = pre_model_name\n",
    "finetune_config['device'] = device\n",
    "finetune_config[\"task\"] = \"dementia\"\n",
    "finetune_config[\"out_dims\"] = 3\n",
    "\n",
    "# finetune_config[\"mask_ratio\"] = 0.25\n",
    "finetune_config[\"descending\"] = False  #######################################################\n",
    "finetune_config[\"global_pool\"] = True\n",
    "finetune_config[\"fc_stages\"] = 3\n",
    "finetune_config[\"dropout\"] = 0.1\n",
    "# finetune_config[\"use_age\"] = \"fc\"\n",
    "# finetune_config[\"mixup\"] = 0.3 ###\n",
    "# finetune_config[\"crop_length\"] = 8192*4  #############################################################\n",
    "# finetune_config[\"criterion\"] = \"multi-bce\"  # \"cross-entropy\", \"multi-bce\"\n",
    "\n",
    "finetune_config[\"tuning_type\"] = \"finetune\"  # \"finetune\", \"fc_stage\"\n",
    "finetune_config[\"layer_wise_lr\"] = True\n",
    "\n",
    "finetune_config['total_samples'] = 2.0e+7\n",
    "finetune_config['base_lr'] = 1e-3\n",
    "finetune_config['search_lr'] = False\n",
    "finetune_config['lr_scheduler_type'] = 'cosine_decay_with_warmup_half'\n",
    "finetune_config[\"warmup_min\"] = 200\n",
    "finetune_config[\"num_history\"] = 50\n",
    "finetune_config['save_model'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mask_ratio': 0.1, 'crop_length': 5120, 'minibatch': 256, 'descending': False}\n",
      "{'mask_ratio': 0.1, 'crop_length': 5120, 'minibatch': 256, 'descending': True}\n",
      "{'mask_ratio': 0.1, 'crop_length': 10240, 'minibatch': 192, 'descending': False}\n",
      "{'mask_ratio': 0.1, 'crop_length': 10240, 'minibatch': 192, 'descending': True}\n",
      "{'mask_ratio': 0.3, 'crop_length': 5120, 'minibatch': 256, 'descending': False}\n",
      "{'mask_ratio': 0.3, 'crop_length': 5120, 'minibatch': 256, 'descending': True}\n",
      "{'mask_ratio': 0.3, 'crop_length': 10240, 'minibatch': 192, 'descending': False}\n",
      "{'mask_ratio': 0.3, 'crop_length': 10240, 'minibatch': 192, 'descending': True}\n",
      "{'mask_ratio': 0.5, 'crop_length': 5120, 'minibatch': 256, 'descending': False}\n",
      "{'mask_ratio': 0.5, 'crop_length': 5120, 'minibatch': 256, 'descending': True}\n",
      "{'mask_ratio': 0.5, 'crop_length': 10240, 'minibatch': 192, 'descending': False}\n",
      "{'mask_ratio': 0.5, 'crop_length': 10240, 'minibatch': 192, 'descending': True}\n",
      "{'mask_ratio': 0.7, 'crop_length': 5120, 'minibatch': 256, 'descending': False}\n",
      "{'mask_ratio': 0.7, 'crop_length': 5120, 'minibatch': 256, 'descending': True}\n",
      "{'mask_ratio': 0.7, 'crop_length': 10240, 'minibatch': 192, 'descending': False}\n",
      "{'mask_ratio': 0.7, 'crop_length': 10240, 'minibatch': 192, 'descending': True}\n",
      "{'mask_ratio': 0.9, 'crop_length': 5120, 'minibatch': 256, 'descending': False}\n",
      "{'mask_ratio': 0.9, 'crop_length': 5120, 'minibatch': 256, 'descending': True}\n",
      "{'mask_ratio': 0.9, 'crop_length': 10240, 'minibatch': 192, 'descending': False}\n",
      "{'mask_ratio': 0.9, 'crop_length': 10240, 'minibatch': 192, 'descending': True}\n"
     ]
    }
   ],
   "source": [
    "finetune_cycler = cycler(mask_ratio=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "finetune_cycler *= cycler(crop_length=[5120, 5120*2]) + cycler(minibatch=[256, 192])\n",
    "# finetune_cycler *= cycler(mixup=[0.0, 0.3])\n",
    "finetune_cycler *= cycler(descending=[False, True])\n",
    "for cyc in finetune_cycler:\n",
    "    print(cyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Uncertainty Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EKG': 'O',\n",
      " '_target_': 'models.ssl.mae_1d.mae_1d_pre_b_e768_d512',\n",
      " 'activation': 'gelu',\n",
      " 'age_mean': tensor([0.], device='cuda:2'),\n",
      " 'age_std': tensor([0.], device='cuda:2'),\n",
      " 'awgn': 0.003,\n",
      " 'awgn_age': 0.001,\n",
      " 'base_lr': 0.001,\n",
      " 'class_label_to_name': ['Normal', 'Abnormal'],\n",
      " 'class_name_to_label': {'Abnormal': 1, 'Normal': 0},\n",
      " 'criterion': 'cross-entropy',\n",
      " 'crop_length': 5120,\n",
      " 'crop_multiple': 16,\n",
      " 'crop_timing_analysis': False,\n",
      " 'cwd': '',\n",
      " 'dataset_name': 'tuab',\n",
      " 'dataset_path': 'local/dataset/tuab/',\n",
      " 'ddp': True,\n",
      " 'ddp_size': 4,\n",
      " 'device': device(type='cuda', index=0),\n",
      " 'draw_result': True,\n",
      " 'file_format': 'memmap',\n",
      " 'in_channels': 24,\n",
      " 'input_norm': 'datapoint',\n",
      " 'iterations': 195312,\n",
      " 'latency': 2500,\n",
      " 'lr_scheduler_type': 'cosine_decay_with_warmup_half',\n",
      " 'mask_ratio': 0.75,\n",
      " 'mgn': 0.003,\n",
      " 'minibatch': 256,\n",
      " 'minibatch_3090': 256,\n",
      " 'mixed_precision': True,\n",
      " 'mixup': 0.0,\n",
      " 'multi_batch_size': 32,\n",
      " 'norm_pix_loss': True,\n",
      " 'num_history': 500,\n",
      " 'num_params': 125297664,\n",
      " 'num_workers': 16,\n",
      " 'out_dims': 2,\n",
      " 'output_length': 41,\n",
      " 'patch_size': 64,\n",
      " 'photic': 'X',\n",
      " 'preprocess_test': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=0))\n",
      "  (1): EegResample(orig_freq=250, new_freq=125, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([0.], device='cuda:2'), std=tensor([0.], device='cuda:2'), eps=1e-08, std_eps=tensor([1.0000e-08], device='cuda:2'))\n",
      "  (3): EegNormalizePerSignal(eps=1e-08)\n",
      "),\n",
      " 'preprocess_train': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=0))\n",
      "  (1): EegResample(orig_freq=250, new_freq=125, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([0.], device='cuda:2'), std=tensor([0.], device='cuda:2'), eps=1e-08, std_eps=tensor([1.0000e-08], device='cuda:2'))\n",
      "  (3): EegAddGaussianNoiseAge(mean=0.0, std=0.001)\n",
      "  (4): EegNormalizePerSignal(eps=1e-08)\n",
      "  (5): EegMultiplicativeGaussianNoise(mean=0.0, std=0.003)\n",
      "  (6): EegAdditiveGaussianNoise(mean=0.0, std=0.003)\n",
      "),\n",
      " 'project': 'tuab-mae',\n",
      " 'resample': 125,\n",
      " 'run_mode': 'train',\n",
      " 'sampling_rate': 250,\n",
      " 'save_model': True,\n",
      " 'search_lr': False,\n",
      " 'search_multiplier': 1.0,\n",
      " 'seed': 0,\n",
      " 'seq_length': 2560,\n",
      " 'signal_header': ['EEG FP1-REF',\n",
      "                   'EEG FP2-REF',\n",
      "                   'EEG F3-REF',\n",
      "                   'EEG F4-REF',\n",
      "                   'EEG C3-REF',\n",
      "                   'EEG C4-REF',\n",
      "                   'EEG P3-REF',\n",
      "                   'EEG P4-REF',\n",
      "                   'EEG O1-REF',\n",
      "                   'EEG O2-REF',\n",
      "                   'EEG F7-REF',\n",
      "                   'EEG F8-REF',\n",
      "                   'EEG T3-REF',\n",
      "                   'EEG T4-REF',\n",
      "                   'EEG T5-REF',\n",
      "                   'EEG T6-REF',\n",
      "                   'EEG A1-REF',\n",
      "                   'EEG A2-REF',\n",
      "                   'EEG FZ-REF',\n",
      "                   'EEG CZ-REF',\n",
      "                   'EEG PZ-REF',\n",
      "                   'EEG T1-REF',\n",
      "                   'EEG T2-REF',\n",
      "                   'EEG EKG1-REF'],\n",
      " 'signal_length_limit': 10000000,\n",
      " 'task_description': 'Pathological classification of [Normal] and [Abnormal] '\n",
      "                     'EEG',\n",
      " 'task_name': 'Temple University Hospital Abnormal Corpus v2.0.0',\n",
      " 'test_crop_multiple': 8,\n",
      " 'total_samples': 200000000.0,\n",
      " 'transform': Compose(\n",
      "    EegRandomCrop(crop_length=5120, length_limit=10000000, multiple=16, latency=2500, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegToTensor()\n",
      "),\n",
      " 'transform_multicrop': Compose(\n",
      "    EegRandomCrop(crop_length=5120, length_limit=10000000, multiple=8, latency=2500, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegToTensor()\n",
      "),\n",
      " 'use_age': 'conv',\n",
      " 'use_wandb': True,\n",
      " 'val_fraction': 15,\n",
      " 'warmup_min': 3000,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'warmup_steps': 9766,\n",
      " 'watch_model': False,\n",
      " 'weight_decay': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# load pretrained configurations\n",
    "path = os.path.join(pre_model_path, pre_model_name.split(',')[-1], 'checkpoint.pt')\n",
    "try:\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    config = ckpt['config']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(f'- checkpoint cannot be opened: {path}')\n",
    "config[\"cwd\"] = \"\"\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update configuration\n",
    "for k, v in art_config.items():\n",
    "    config[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "if not config.get(\"ddp\", False):\n",
    "    pre_model_state = ckpt[\"ssl_model_state\"]\n",
    "else:\n",
    "    pre_model_state_ddp = deepcopy(ckpt[\"ssl_model_state\"])\n",
    "    pre_model_state = OrderedDict()\n",
    "    for k, v in pre_model_state_ddp.items():\n",
    "        name = k[7:]  # remove 'module.' of DataParallel/DistributedDataParallel\n",
    "        pre_model_state[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EKG': 'O',\n",
      " '_target_': 'models.ssl.mae_1d.mae_1d_pre_b_e768_d512',\n",
      " 'activation': 'gelu',\n",
      " 'age_mean': tensor([0.], device='cuda:2'),\n",
      " 'age_std': tensor([0.], device='cuda:2'),\n",
      " 'art_dropout': 0.1,\n",
      " 'art_use_age': 'no',\n",
      " 'awgn': 0.003,\n",
      " 'awgn_age': 0.001,\n",
      " 'base_lr': 0.001,\n",
      " 'class_label_to_name': ['Normal', 'Abnormal'],\n",
      " 'class_name_to_label': {'Abnormal': 1, 'Normal': 0},\n",
      " 'criterion': 'cross-entropy',\n",
      " 'crop_length': 5120,\n",
      " 'crop_multiple': 16,\n",
      " 'crop_timing_analysis': False,\n",
      " 'cwd': '',\n",
      " 'dataset_name': 'tuab',\n",
      " 'dataset_path': 'local/dataset/tuab/',\n",
      " 'ddp': False,\n",
      " 'ddp_size': 4,\n",
      " 'device': device(type='cuda', index=2),\n",
      " 'draw_result': True,\n",
      " 'file_format': 'memmap',\n",
      " 'in_channels': 24,\n",
      " 'input_norm': 'datapoint',\n",
      " 'iterations': 195312,\n",
      " 'latency': 2500,\n",
      " 'lr_scheduler_type': 'cosine_decay_with_warmup_half',\n",
      " 'mask_ratio': 0.75,\n",
      " 'mgn': 0.003,\n",
      " 'minibatch': 256,\n",
      " 'minibatch_3090': 256,\n",
      " 'mixed_precision': True,\n",
      " 'mixup': 0.0,\n",
      " 'multi_batch_size': 32,\n",
      " 'norm_pix_loss': True,\n",
      " 'num_history': 50,\n",
      " 'num_params': 125297664,\n",
      " 'num_workers': 16,\n",
      " 'out_dims': 2,\n",
      " 'output_length': 41,\n",
      " 'patch_size': 64,\n",
      " 'photic': 'X',\n",
      " 'pre_model': 'lz7ffvxk',\n",
      " 'preprocess_test': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=2))\n",
      "  (1): EegResample(orig_freq=250, new_freq=125, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([0.], device='cuda:2'), std=tensor([0.], device='cuda:2'), eps=1e-08, std_eps=tensor([1.0000e-08], device='cuda:2'))\n",
      "  (3): EegNormalizePerSignal(eps=1e-08)\n",
      "),\n",
      " 'preprocess_train': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=2))\n",
      "  (1): EegResample(orig_freq=250, new_freq=125, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([0.], device='cuda:2'), std=tensor([0.], device='cuda:2'), eps=1e-08, std_eps=tensor([1.0000e-08], device='cuda:2'))\n",
      "  (3): EegAddGaussianNoiseAge(mean=0.0, std=0.001)\n",
      "  (4): EegNormalizePerSignal(eps=1e-08)\n",
      "  (5): EegMultiplicativeGaussianNoise(mean=0.0, std=0.003)\n",
      "  (6): EegAdditiveGaussianNoise(mean=0.0, std=0.003)\n",
      "),\n",
      " 'project': 'tuab-mae-artifact',\n",
      " 'resample': 125,\n",
      " 'run_mode': 'train',\n",
      " 'sampling_rate': 250,\n",
      " 'save_model': False,\n",
      " 'search_lr': False,\n",
      " 'search_multiplier': 1.0,\n",
      " 'seed': 0,\n",
      " 'seq_length': 2560,\n",
      " 'signal_header': ['EEG FP1-REF',\n",
      "                   'EEG FP2-REF',\n",
      "                   'EEG F3-REF',\n",
      "                   'EEG F4-REF',\n",
      "                   'EEG C3-REF',\n",
      "                   'EEG C4-REF',\n",
      "                   'EEG P3-REF',\n",
      "                   'EEG P4-REF',\n",
      "                   'EEG O1-REF',\n",
      "                   'EEG O2-REF',\n",
      "                   'EEG F7-REF',\n",
      "                   'EEG F8-REF',\n",
      "                   'EEG T3-REF',\n",
      "                   'EEG T4-REF',\n",
      "                   'EEG T5-REF',\n",
      "                   'EEG T6-REF',\n",
      "                   'EEG A1-REF',\n",
      "                   'EEG A2-REF',\n",
      "                   'EEG FZ-REF',\n",
      "                   'EEG CZ-REF',\n",
      "                   'EEG PZ-REF',\n",
      "                   'EEG T1-REF',\n",
      "                   'EEG T2-REF',\n",
      "                   'EEG EKG1-REF'],\n",
      " 'signal_length_limit': 10000000,\n",
      " 'task_description': 'Pathological classification of [Normal] and [Abnormal] '\n",
      "                     'EEG',\n",
      " 'task_name': 'Temple University Hospital Abnormal Corpus v2.0.0',\n",
      " 'test_crop_multiple': 8,\n",
      " 'total_samples': 500000.0,\n",
      " 'transform': Compose(\n",
      "    EegRandomCrop(crop_length=5120, length_limit=10000000, multiple=16, latency=2500, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegToTensor()\n",
      "),\n",
      " 'transform_multicrop': Compose(\n",
      "    EegRandomCrop(crop_length=5120, length_limit=10000000, multiple=8, latency=2500, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegToTensor()\n",
      "),\n",
      " 'use_age': 'conv',\n",
      " 'use_wandb': False,\n",
      " 'val_fraction': 15,\n",
      " 'warmup_min': 150,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'warmup_steps': 9766,\n",
      " 'watch_model': False,\n",
      " 'weight_decay': 0.01}\n"
     ]
    }
   ],
   "source": [
    "config[\"ddp\"] = False\n",
    "\n",
    "# check the workstation environment and update some configurations\n",
    "check_device_env(config)\n",
    "\n",
    "# compose dataset\n",
    "train_loader, val_loader, test_loader, multicrop_test_loader = compose_dataset(config)\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the model\n",
    "config[\"_target_\"] = config[\"_target_\"].replace('pre', 'pre_art').replace('.mae_1d.', '.mae_1d_artifact.')\n",
    "model = generate_model(config).to(device)\n",
    "\n",
    "# load the model\n",
    "model_state = model.state_dict()\n",
    "for k, v in model_state.items():\n",
    "    if not k.startswith('art') and not k.endswith(\"pos_embed\"):\n",
    "        model_state[k] = pre_model_state[k]\n",
    "\n",
    "pre_model_state = deepcopy(model.state_dict())\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.requires_grad_(False)\n",
    "model = model.eval()\n",
    "\n",
    "model.art_net.requires_grad_(True)\n",
    "for k, v in model._parameters.items():\n",
    "    if k.startswith(\"art\"):\n",
    "        v.requires_grad_(True)\n",
    "model.art_net = model.art_net.train()\n",
    "\n",
    "config[\"num_params\"] = count_parameters(model)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_token                                                                                         \t|\tFalse\n",
      "enc_pos_embed                                                                                       \t|\tFalse\n",
      "mask_token                                                                                          \t|\tFalse\n",
      "dec_pos_embed                                                                                       \t|\tFalse\n",
      "enc_proj.weight                                                                                     \t|\tFalse\n",
      "enc_proj.bias                                                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_0.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_1.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_2.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_3.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_4.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_5.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_6.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_7.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_8.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm1.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm1.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm2.weight                                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.norm2.bias                                                               \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "enc_blocks.encoder_layer_9.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm1.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm1.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm2.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.norm2.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_10.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm1.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm1.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm2.weight                                                            \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.norm2.bias                                                              \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "enc_blocks.encoder_layer_11.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "enc_norm.weight                                                                                     \t|\tFalse\n",
      "enc_norm.bias                                                                                       \t|\tFalse\n",
      "dec_proj.weight                                                                                     \t|\tFalse\n",
      "dec_proj.bias                                                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_0.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_1.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_2.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_3.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_4.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_5.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_6.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_7.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_8.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm1.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm1.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.in_proj_weight                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.in_proj_bias                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.out_proj.weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.self_attention.out_proj.bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm2.weight                                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.norm2.bias                                                               \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_1.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_1.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_2.weight                                                      \t|\tFalse\n",
      "dec_blocks.encoder_layer_9.mlp.linear_2.bias                                                        \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm1.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm1.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm2.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.norm2.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_10.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm1.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm1.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.in_proj_weight                                           \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.in_proj_bias                                             \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.out_proj.weight                                          \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.self_attention.out_proj.bias                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm2.weight                                                            \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.norm2.bias                                                              \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_1.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_1.bias                                                       \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_2.weight                                                     \t|\tFalse\n",
      "dec_blocks.encoder_layer_11.mlp.linear_2.bias                                                       \t|\tFalse\n",
      "dec_norm.weight                                                                                     \t|\tFalse\n",
      "dec_norm.bias                                                                                       \t|\tFalse\n",
      "decoder_pred.weight                                                                                 \t|\tFalse\n",
      "decoder_pred.bias                                                                                   \t|\tFalse\n",
      "art_net.0.weight                                                                                    \t|\tTrue\n",
      "art_net.0.bias                                                                                      \t|\tTrue\n",
      "art_net.1.weight                                                                                    \t|\tTrue\n",
      "art_net.1.bias                                                                                      \t|\tTrue\n",
      "art_net.3.weight                                                                                    \t|\tTrue\n",
      "art_net.3.bias                                                                                      \t|\tTrue\n",
      "art_net.4.weight                                                                                    \t|\tTrue\n",
      "art_net.4.bias                                                                                      \t|\tTrue\n",
      "art_net.6.weight                                                                                    \t|\tTrue\n",
      "art_net.6.bias                                                                                      \t|\tTrue\n",
      "art_net.7.weight                                                                                    \t|\tTrue\n",
      "art_net.7.bias                                                                                      \t|\tTrue\n",
      "art_net.11.weight                                                                                   \t|\tTrue\n",
      "art_net.13.weight                                                                                   \t|\tTrue\n",
      "art_net.13.bias                                                                                     \t|\tTrue\n",
      "art_net.15.weight                                                                                   \t|\tTrue\n",
      "art_net.15.bias                                                                                     \t|\tTrue\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:100}\\t|\\t{param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************    Configurations for Train    ******************************\n",
      "\n",
      "{'EKG': 'O',\n",
      " '_target_': 'models.ssl.mae_1d_artifact.mae_1d_pre_art_b_e768_d512',\n",
      " 'activation': 'gelu',\n",
      " 'age_mean': tensor([0.], device='cuda:2'),\n",
      " 'age_std': tensor([0.], device='cuda:2'),\n",
      " 'art_dropout': 0.1,\n",
      " 'art_use_age': 'no',\n",
      " 'awgn': 0.003,\n",
      " 'awgn_age': 0.001,\n",
      " 'base_lr': 0.001,\n",
      " 'class_label_to_name': ['Normal', 'Abnormal'],\n",
      " 'class_name_to_label': {'Abnormal': 1, 'Normal': 0},\n",
      " 'criterion': 'cross-entropy',\n",
      " 'crop_length': 5120,\n",
      " 'crop_multiple': 16,\n",
      " 'crop_timing_analysis': False,\n",
      " 'cwd': '',\n",
      " 'dataset_name': 'tuab',\n",
      " 'dataset_path': 'local/dataset/tuab/',\n",
      " 'ddp': False,\n",
      " 'ddp_size': 4,\n",
      " 'device': device(type='cuda', index=2),\n",
      " 'draw_result': True,\n",
      " 'file_format': 'memmap',\n",
      " 'in_channels': 24,\n",
      " 'input_norm': 'datapoint',\n",
      " 'iterations': 195312,\n",
      " 'latency': 2500,\n",
      " 'lr_scheduler_type': 'cosine_decay_with_warmup_half',\n",
      " 'mask_ratio': 0.75,\n",
      " 'mgn': 0.003,\n",
      " 'minibatch': 256,\n",
      " 'minibatch_3090': 256,\n",
      " 'mixed_precision': True,\n",
      " 'mixup': 0.0,\n",
      " 'multi_batch_size': 32,\n",
      " 'norm_pix_loss': True,\n",
      " 'num_history': 50,\n",
      " 'num_params': 90273,\n",
      " 'num_workers': 16,\n",
      " 'out_dims': 2,\n",
      " 'output_length': 41,\n",
      " 'patch_size': 64,\n",
      " 'photic': 'X',\n",
      " 'pre_model': 'lz7ffvxk',\n",
      " 'preprocess_test': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=2))\n",
      "  (1): EegResample(orig_freq=250, new_freq=125, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([0.], device='cuda:2'), std=tensor([0.], device='cuda:2'), eps=1e-08, std_eps=tensor([1.0000e-08], device='cuda:2'))\n",
      "  (3): EegNormalizePerSignal(eps=1e-08)\n",
      "),\n",
      " 'preprocess_train': Sequential(\n",
      "  (0): EegToDevice(device=device(type='cuda', index=2))\n",
      "  (1): EegResample(orig_freq=250, new_freq=125, resampling_method='sinc_interp_hann')\n",
      "  (2): EegNormalizeAge(mean=tensor([0.], device='cuda:2'), std=tensor([0.], device='cuda:2'), eps=1e-08, std_eps=tensor([1.0000e-08], device='cuda:2'))\n",
      "  (3): EegAddGaussianNoiseAge(mean=0.0, std=0.001)\n",
      "  (4): EegNormalizePerSignal(eps=1e-08)\n",
      "  (5): EegMultiplicativeGaussianNoise(mean=0.0, std=0.003)\n",
      "  (6): EegAdditiveGaussianNoise(mean=0.0, std=0.003)\n",
      "),\n",
      " 'project': 'tuab-mae-artifact',\n",
      " 'resample': 125,\n",
      " 'run_mode': 'train',\n",
      " 'sampling_rate': 250,\n",
      " 'save_model': False,\n",
      " 'search_lr': False,\n",
      " 'search_multiplier': 1.0,\n",
      " 'seed': 0,\n",
      " 'seq_length': 2560,\n",
      " 'signal_header': ['EEG FP1-REF',\n",
      "                   'EEG FP2-REF',\n",
      "                   'EEG F3-REF',\n",
      "                   'EEG F4-REF',\n",
      "                   'EEG C3-REF',\n",
      "                   'EEG C4-REF',\n",
      "                   'EEG P3-REF',\n",
      "                   'EEG P4-REF',\n",
      "                   'EEG O1-REF',\n",
      "                   'EEG O2-REF',\n",
      "                   'EEG F7-REF',\n",
      "                   'EEG F8-REF',\n",
      "                   'EEG T3-REF',\n",
      "                   'EEG T4-REF',\n",
      "                   'EEG T5-REF',\n",
      "                   'EEG T6-REF',\n",
      "                   'EEG A1-REF',\n",
      "                   'EEG A2-REF',\n",
      "                   'EEG FZ-REF',\n",
      "                   'EEG CZ-REF',\n",
      "                   'EEG PZ-REF',\n",
      "                   'EEG T1-REF',\n",
      "                   'EEG T2-REF',\n",
      "                   'EEG EKG1-REF'],\n",
      " 'signal_length_limit': 10000000,\n",
      " 'task_description': 'Pathological classification of [Normal] and [Abnormal] EEG',\n",
      " 'task_name': 'Temple University Hospital Abnormal Corpus v2.0.0',\n",
      " 'test_crop_multiple': 8,\n",
      " 'total_samples': 500000.0,\n",
      " 'transform': Compose(\n",
      "    EegRandomCrop(crop_length=5120, length_limit=10000000, multiple=16, latency=2500, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegToTensor()\n",
      "),\n",
      " 'transform_multicrop': Compose(\n",
      "    EegRandomCrop(crop_length=5120, length_limit=10000000, multiple=8, latency=2500, segment_simulation=False, return_timing=False, reject_events=False)\n",
      "    EegToTensor()\n",
      "),\n",
      " 'use_age': 'conv',\n",
      " 'use_wandb': False,\n",
      " 'val_fraction': 15,\n",
      " 'warmup_min': 150,\n",
      " 'warmup_ratio': 0.05,\n",
      " 'warmup_steps': 9766,\n",
      " 'watch_model': False,\n",
      " 'weight_decay': 0.01}\n",
      "\n",
      "********************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imkbsz/anaconda3/envs/eeg/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning:\n",
      "\n",
      "Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       9 /      488 iter - Loss: 1.6210\n",
      "      18 /      488 iter - Loss: 1.4366\n",
      "      27 /      488 iter - Loss: 1.2056\n",
      "      36 /      488 iter - Loss: 0.9599\n",
      "      45 /      488 iter - Loss: 0.7797\n",
      "      54 /      488 iter - Loss: 0.6090\n",
      "      63 /      488 iter - Loss: 0.4844\n",
      "      72 /      488 iter - Loss: 0.3684\n",
      "      81 /      488 iter - Loss: 0.2965\n",
      "      90 /      488 iter - Loss: 0.2445\n",
      "      99 /      488 iter - Loss: 0.2071\n",
      "     108 /      488 iter - Loss: 0.1786\n",
      "     117 /      488 iter - Loss: 0.1583\n",
      "     126 /      488 iter - Loss: 0.1497\n",
      "     135 /      488 iter - Loss: 0.1424\n",
      "     144 /      488 iter - Loss: 0.1356\n",
      "     153 /      488 iter - Loss: 0.1208\n",
      "     162 /      488 iter - Loss: 0.1155\n",
      "     171 /      488 iter - Loss: 0.1129\n",
      "     180 /      488 iter - Loss: 0.1067\n",
      "     189 /      488 iter - Loss: 0.0970\n",
      "     198 /      488 iter - Loss: 0.1012\n",
      "     207 /      488 iter - Loss: 0.0896\n",
      "     216 /      488 iter - Loss: 0.0894\n",
      "     225 /      488 iter - Loss: 0.0800\n",
      "     234 /      488 iter - Loss: 0.0845\n",
      "     243 /      488 iter - Loss: 0.0764\n"
     ]
    }
   ],
   "source": [
    "# collect some garbage\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# fix the seed for reproducibility (a negative seed value means not fixing)\n",
    "set_seed(config, rank=None)\n",
    "\n",
    "# train\n",
    "ssl_train_script(\n",
    "    config,\n",
    "    model,\n",
    "    train_loader,\n",
    "    config[\"preprocess_train\"],\n",
    ")\n",
    "\n",
    "pre_model_state = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_header = [channel.split('-')[0] for i, channel in enumerate(config[\"signal_header\"])]\n",
    "fps = config.get('resample', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plt.style.context(['ieee', 'science', 'default'])\n",
    "def draw_eeg_graph(x, pred, mask, art_out, rec_loss, num=None):\n",
    "    plt.rcParams.update({'font.family': 'Ubuntu'})\n",
    "    N, C, L = x.shape\n",
    "    _, l = mask.shape\n",
    "    patch_size = L // l\n",
    "\n",
    "    art_out = art_out.reshape(N, -1)\n",
    "    rec_loss = rec_loss.reshape(N, -1)\n",
    "\n",
    "    for n in range(N):\n",
    "        if num is not None and num <= n:\n",
    "            break\n",
    "        \n",
    "        fig = plt.figure(num=1, clear=True, figsize=(25.0, 17.0))\n",
    "        fig.subplots_adjust(hspace=0)\n",
    "        \n",
    "        for c in range(C):\n",
    "            ax = fig.add_subplot(C, 1, c + 1)\n",
    "            ax.plot(x[n, c].cpu().numpy(), lw=1, c='tab:red', label='origin')\n",
    "            ax.plot(pred[n, c].cpu().numpy(), lw=1, c='tab:blue', label='pred')\n",
    "\n",
    "            art_mid = art_out[n].median()\n",
    "            rec_mid = art_out[n].median()\n",
    "            for r in range(l):\n",
    "                if r > 0:\n",
    "                    ax.axvline(r*patch_size, color='tab:purple', alpha=0.4)\n",
    "                if mask[n, r]:\n",
    "                    ax.axvspan(r*patch_size, (r + 1)*patch_size, facecolor='tab:purple', alpha=0.2)\n",
    "\n",
    "                if c == 0:\n",
    "                    if art_mid <= art_out[n, r]:\n",
    "                        ax.annotate(f\"{art_out[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, 1), ha='center', va='bottom', color='tab:purple')\n",
    "                    else:\n",
    "                        ax.annotate(f\"{art_out[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, 1), ha='center', va='bottom')\n",
    "                    if rec_mid <= rec_loss[n, r]:\n",
    "                        ax.annotate(f\"{rec_loss[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, -1), ha='center', va='bottom', color='tab:purple')\n",
    "                    else:\n",
    "                        ax.annotate(f\"{rec_loss[n, r]:3.2f}\", \n",
    "                                    xy=((r + 0.5)*patch_size, -1), ha='center', va='bottom')\n",
    "\n",
    "            ax.set_xlim(0, L)\n",
    "            ax.set_ylabel(signal_header[c])\n",
    "            ax.set_xticks(np.arange(round(config[\"seq_length\"] / fps) + 1) * fps)\n",
    "            ax.set_xticklabels([])\n",
    "            # ax.tick_params(axis='x', width=0.1, length=0.1)\n",
    "            ax.set_yticks([0])\n",
    "            ax.set_yticklabels([])\n",
    "        \n",
    "        ax.set_xticks(np.arange(round(config[\"seq_length\"] / fps) + 1) * fps)\n",
    "        ax.set_xticklabels(np.arange(round(config[\"seq_length\"] / fps) + 1))\n",
    "        \n",
    "        ax.set_xlabel('Time (s)')\n",
    "        # fig.savefig(os.path.join(output_folder, 'signal_example.pdf'), transparent=True)\n",
    "        plt.show()\n",
    "        fig.clear()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for target_dataset in tqdm([\"val\"], desc=\"Dataset\", leave=False):\n",
    "        if target_dataset == 'train':\n",
    "            loader = train_loader\n",
    "        elif target_dataset == 'val':\n",
    "            loader = val_loader\n",
    "        elif target_dataset == 'test':\n",
    "            loader = test_loader\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "                \n",
    "        for sample_batched in tqdm(loader, total=len(loader), desc='Batch', leave=False):\n",
    "            print(target_dataset)\n",
    "            config[\"preprocess_test\"](sample_batched)\n",
    "            x = sample_batched[\"signal\"]\n",
    "            age = sample_batched[\"age\"]\n",
    "\n",
    "            pred, mask = model.mask_and_reconstruct(x, age, config[\"mask_ratio\"])\n",
    "            rec_loss = model.compute_reconstruction_loss2(x, pred)\n",
    "            art_out = model.forward_artifact(x, age)\n",
    "            \n",
    "            pred_eeg = model.unpatchify(pred)\n",
    "            draw_eeg_graph(x, pred_eeg, mask, art_out, rec_loss, 2)\n",
    "\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cyc in finetune_cycler:\n",
    "    # update configuration\n",
    "    for k, v in finetune_config.items():\n",
    "        config[k] = v\n",
    "    for k, v in cyc.items():\n",
    "        config[k] = v\n",
    "    config[\"ddp\"] = False\n",
    "\n",
    "    # check the workstation environment and update some configurations\n",
    "    check_device_env(config)\n",
    "    \n",
    "    # compose dataset\n",
    "    train_loader, val_loader, test_loader, multicrop_test_loader = compose_dataset(config)\n",
    "    pprint.pprint(config)\n",
    "\n",
    "    # generate the model\n",
    "    config[\"_target_\"] = config[\"_target_\"].replace('.ssl', '').replace('_pre', '')\n",
    "    model = generate_model(config).to(device)\n",
    "    \n",
    "    # load the model\n",
    "    model_state = model.state_dict()\n",
    "    for k, v in model_state.items():\n",
    "        if not k.startswith(\"fc\") and not k.endswith(\"pos_embed\"):\n",
    "            model_state[k] = deepcopy(pre_model_state[k])\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "\n",
    "    model.finetune_mode(config[\"tuning_type\"])\n",
    "    config[\"num_params\"] = count_parameters(model)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name:100}\\t|\\t{param.requires_grad}\")\n",
    "\n",
    "    # collect some garbage\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # fix the seed for reproducibility (a negative seed value means not fixing)\n",
    "    set_seed(config, rank=None)\n",
    "    \n",
    "    # train\n",
    "    train_script(\n",
    "        config,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        multicrop_test_loader,\n",
    "        config[\"preprocess_train\"],\n",
    "        config[\"preprocess_test\"],\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"- END -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
