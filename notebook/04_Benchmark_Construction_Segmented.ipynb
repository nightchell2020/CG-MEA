{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Benchmark Construction\n",
    "\n",
    "This notebook organizes the standard benchmark of our `CAUEEG` dataset using the previously generated signal, annotation, and event files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minjae\\Desktop\\EEG_Project\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# custom package\n",
    "from datasets.caueeg_dataset import *\n",
    "from datasets.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data file path\n",
    "data_path = r'local/dataset/02_Curated_Data_220720_seg_10s/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "anno_path = os.path.join(data_path, 'annotation.json')\n",
    "with open(anno_path, 'r') as json_file:\n",
    "    annotation = json.load(json_file)\n",
    "\n",
    "pprint.pprint({k: (v if k != 'data' else v[:5]) for (k, v) in annotation.items()}, width=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_splitted_metadata(splitted_metadata, class_label_to_name, ratios, seed=None, verbose=False):\n",
    "    # random seed\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    else:\n",
    "        random.seed()\n",
    "\n",
    "    metadata_train = []\n",
    "    metadata_val = []\n",
    "    metadata_test = []\n",
    "\n",
    "    for split in splitted_metadata:\n",
    "        random.shuffle(split)\n",
    "\n",
    "        n1 = round(len(split) * ratios[0])\n",
    "        n2 = n1 + round(len(split) * ratios[1])\n",
    "\n",
    "        metadata_train.extend(split[:n1])\n",
    "        metadata_val.extend(split[n1:n2])\n",
    "        metadata_test.extend(split[n2:])\n",
    "\n",
    "    random.shuffle(metadata_train)\n",
    "    random.shuffle(metadata_val)\n",
    "    random.shuffle(metadata_test)\n",
    "\n",
    "    if verbose:\n",
    "        train_class_dist = [np.sum([1 for m in metadata_train if m['class_label'] == i])\n",
    "                            for i in range(len(class_label_to_name))]\n",
    "\n",
    "        val_class_dist = [np.sum([1 for m in metadata_val if m['class_label'] == i])\n",
    "                          for i in range(len(class_label_to_name))]\n",
    "\n",
    "        test_class_dist = [np.sum([1 for m in metadata_test if m['class_label'] == i])\n",
    "                           for i in range(len(class_label_to_name))]\n",
    "\n",
    "        print(f'<{\"Train\":^15}> data label distribution\\t:', train_class_dist, '=', np.sum(train_class_dist))\n",
    "        print(f'<{\"Validation\":^15}> data label distribution\\t:', val_class_dist, '=', np.sum(val_class_dist))\n",
    "        print(f'<{\"Test\":^15}> data label distribution\\t:', test_class_dist, '=', np.sum(test_class_dist))\n",
    "\n",
    "    # restore random seed (stochastic)\n",
    "    random.seed()\n",
    "\n",
    "    return metadata_train, metadata_val, metadata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_splitted_metadata_with_initial_train(splitted_metadata, class_label_to_name, ratios, initial_train, seed=None, verbose=False):\n",
    "    # random seed\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    else:\n",
    "        random.seed()\n",
    "\n",
    "    metadata_train = []\n",
    "    metadata_val = []\n",
    "    metadata_test = []\n",
    "\n",
    "    for i, split in enumerate(splitted_metadata):\n",
    "        metadata_train.extend([s for s in split if s['serial'] in [m['serial'] for m in initial_train[i]]])\n",
    "        split_rest = [s for s in split if s['serial'] not in [m['serial'] for m in metadata_train]]\n",
    "        random.shuffle(split_rest)\n",
    "        \n",
    "        n1 = round(len(split) * ratios[0]) - len(initial_train[i])\n",
    "        n2 = n1 + round(len(split) * ratios[1])\n",
    "        \n",
    "        metadata_train.extend(split_rest[:n1])\n",
    "        metadata_val.extend(split_rest[n1:n2])\n",
    "        metadata_test.extend(split_rest[n2:])\n",
    "\n",
    "    random.shuffle(metadata_train)\n",
    "    random.shuffle(metadata_val)\n",
    "    random.shuffle(metadata_test)\n",
    "\n",
    "    if verbose:\n",
    "        train_class_dist = [np.sum([1 for m in metadata_train if m['class_label'] == i])\n",
    "                            for i in range(len(class_label_to_name))]\n",
    "\n",
    "        val_class_dist = [np.sum([1 for m in metadata_val if m['class_label'] == i])\n",
    "                          for i in range(len(class_label_to_name))]\n",
    "\n",
    "        test_class_dist = [np.sum([1 for m in metadata_test if m['class_label'] == i])\n",
    "                           for i in range(len(class_label_to_name))]\n",
    "\n",
    "        print(f'<{\"Train\":^15}> data label distribution\\t:', train_class_dist, '=', np.sum(train_class_dist))\n",
    "        print(f'<{\"Validation\":^15}> data label distribution\\t:', val_class_dist, '=', np.sum(val_class_dist))\n",
    "        print(f'<{\"Test\":^15}> data label distribution\\t:', test_class_dist, '=', np.sum(test_class_dist))\n",
    "\n",
    "    # restore random seed (stochastic)\n",
    "    random.seed()\n",
    "\n",
    "    return metadata_train, metadata_val, metadata_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Main Task 2: Classification of Three Symptoms (Normal, MCI, Dementia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define the target diagnoses and split them by their symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'name': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'name': 'MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular dementia\n",
    "    {'name': 'Dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': []},\n",
    "]\n",
    "\n",
    "class_label_to_name = [d_f['name'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_name:', class_label_to_name)\n",
    "\n",
    "class_name_to_label = {d_f['name']: i for i, d_f in enumerate(diagnosis_filter)}\n",
    "print('class_name_to_label:', class_name_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the filtered dataset\n",
    "splitted_metadata = [[] for _ in diagnosis_filter]\n",
    "\n",
    "for m in annotation['data']:\n",
    "    symptom = m['symptom']\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        inc = set(f['include']) & set(symptom) == set(f['include'])\n",
    "        # inc = len(set(f['include']) & set(label)) > 0\n",
    "        exc = len(set(f['exclude']) & set(symptom)) == 0\n",
    "        if inc and exc:\n",
    "            m['class_name'] = f['name']\n",
    "            m['class_label'] = c\n",
    "            splitted_metadata[c].append(m)\n",
    "            break\n",
    "\n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        raise ValueError(f'(Warning) Split group {i} has no data.')\n",
    "    print(f'- There are {len(split):} data belonging to {split[0][\"class_name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Shuffle the divided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratios = np.array([8, 1, 1])\n",
    "ratios = ratios / ratios.sum()\n",
    "print('Train, validation, test sets ratios:', ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metadata_train, metadata_val, metadata_test = shuffle_splitted_metadata(splitted_metadata, \n",
    "                                                                        class_label_to_name, \n",
    "                                                                        ratios, \n",
    "                                                                        seed=None, \n",
    "                                                                        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save the dataset as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_dict = dict()\n",
    "\n",
    "task_dict['task_name'] = 'CAUEEG-task2 benchmark'\n",
    "task_dict['task_description'] = 'Classification of [Normal], [MCI], and [Dementia] symptoms.'\n",
    "task_dict['class_label_to_name'] = class_label_to_name\n",
    "task_dict['class_name_to_label'] = class_name_to_label\n",
    "\n",
    "task_dict['train_split'] = metadata_train\n",
    "task_dict['validation_split'] = metadata_val\n",
    "task_dict['test_split'] = metadata_test\n",
    "\n",
    "print('{')\n",
    "for k, v in task_dict.items():\n",
    "    print(f'\\t{k}:')\n",
    "    if isinstance(v, list) and len(v) > 3:\n",
    "        print(f'\\t\\t{v[0]}')\n",
    "        print(f'\\t\\t{v[1]}')\n",
    "        print(f'\\t\\t{v[2]}')\n",
    "        print(f'\\t\\t.')\n",
    "        print(f'\\t\\t.')\n",
    "        print(f'\\t\\t.')\n",
    "        print(f'\\t\\t{v[-1]}')\n",
    "    else:\n",
    "        print(f'\\t\\t{v}')\n",
    "    print()\n",
    "print('}')\n",
    "\n",
    "with open(os.path.join(data_path, 'task2.json'), 'w') as json_file:\n",
    "    json.dump(task_dict, json_file, indent=4)\n",
    "    print('task2.json file is saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Classification of Normal and Abnormal Symptoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define the target diagnoses and split them by their symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'name': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Abnormal\n",
    "    {'name': 'Abnormal',\n",
    "     'include': [], \n",
    "     'exclude': ['normal']},\n",
    "]\n",
    "\n",
    "class_label_to_name = [d_f['name'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_name:', class_label_to_name)\n",
    "\n",
    "class_name_to_label = {d_f['name']: i for i, d_f in enumerate(diagnosis_filter)}\n",
    "print('class_name_to_label:', class_name_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the filtered dataset\n",
    "splitted_metadata = [[] for _ in diagnosis_filter]\n",
    "\n",
    "for m in annotation['data']:\n",
    "    symptom = m['symptom']\n",
    "    \n",
    "    # ignore data with the unknown label \n",
    "    if len(symptom) == 0:\n",
    "        continue\n",
    "    \n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        inc = set(f['include']) & set(symptom) == set(f['include'])\n",
    "        # inc = len(set(f['include']) & set(label)) > 0\n",
    "        exc = len(set(f['exclude']) & set(symptom)) == 0\n",
    "        if inc and exc:\n",
    "            m['class_name'] = f['name']\n",
    "            m['class_label'] = c\n",
    "            splitted_metadata[c].append(m)\n",
    "            break\n",
    "\n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        raise ValueError(f'(Warning) Split group {i} has no data.')\n",
    "    print(f'- There are {len(split):} data belonging to {split[0][\"class_name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Shuffle the divided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ratios = np.array([8, 1, 1])\n",
    "ratios = ratios / ratios.sum()\n",
    "print('Train, validation, test sets ratios:', ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Consider `Task 2` training split to be also `Task 1` training split preferentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_train, metadata_val, metadata_test = shuffle_splitted_metadata(splitted_metadata, \n",
    "#                                                                         class_label_to_name, \n",
    "#                                                                         ratios, \n",
    "#                                                                         seed=None, \n",
    "#                                                                         verbose=True)\n",
    "\n",
    "with open(os.path.join(data_path, 'task2.json')) as json_file:\n",
    "    task2_dict = json.load(json_file)\n",
    "    \n",
    "task2_normals = [m for m in task2_dict['train_split'] if m['class_label'] == 0]\n",
    "task2_abnormals = [m for m in task2_dict['train_split'] if m['class_label'] > 0]\n",
    "    \n",
    "print('Task2  -  Normal:', len(task2_normals), ' / Abnormal:', len(task2_abnormals))\n",
    "print()\n",
    "initial_train = [task2_normals, task2_abnormals]\n",
    "\n",
    "metadata_train, metadata_val, metadata_test = shuffle_splitted_metadata_with_initial_train(splitted_metadata, \n",
    "                                                                                           class_label_to_name, \n",
    "                                                                                           ratios, \n",
    "                                                                                           initial_train,\n",
    "                                                                                           seed=None, \n",
    "                                                                                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save the dataset as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task_dict = dict()\n",
    "\n",
    "task_dict['task_name'] = 'CAUEEG-task1 benchmark'\n",
    "task_dict['task_description'] = 'Classification of [Normal] and [Abnormal] symptoms'\n",
    "task_dict['class_label_to_name'] = class_label_to_name\n",
    "task_dict['class_name_to_label'] = class_name_to_label\n",
    "\n",
    "task_dict['train_split'] = metadata_train\n",
    "task_dict['validation_split'] = metadata_val\n",
    "task_dict['test_split'] = metadata_test\n",
    "\n",
    "print('{')\n",
    "for k, v in task_dict.items():\n",
    "    print(f'\\t{k}:')\n",
    "    if isinstance(v, list) and len(v) > 3:\n",
    "        print(f'\\t\\t{v[0]}')\n",
    "        print(f'\\t\\t{v[1]}')\n",
    "        print(f'\\t\\t{v[2]}')\n",
    "        print(f'\\t\\t.')\n",
    "        print(f'\\t\\t.')\n",
    "        print(f'\\t\\t.')\n",
    "        print(f'\\t\\t{v[-1]}')\n",
    "    else:\n",
    "        print(f'\\t\\t{v}')\n",
    "    print()\n",
    "print('}')\n",
    "\n",
    "with open(os.path.join(data_path, 'task1.json'), 'w') as json_file:\n",
    "    json.dump(task_dict, json_file, indent=4)\n",
    "    print('task1.json file is saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'task1.json')) as json_file:\n",
    "    task1_dict = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(data_path, 'task2.json')) as json_file:\n",
    "    task2_dict = json.load(json_file)\n",
    "\n",
    "# sanity check 1\n",
    "task1_train_serials = [m1['serial'] for m1 in task1_dict['train_split']]\n",
    "task2_train_serials = [m2['serial'] for m2 in task2_dict['train_split']]\n",
    "print(len(task1_train_serials), len(task2_train_serials))\n",
    "\n",
    "for serial2 in task2_train_serials:\n",
    "    if serial2 not in task1_train_serials:\n",
    "        print('NO' * 5)\n",
    "        \n",
    "# sanity check 2\n",
    "for split in ['train_split', 'validation_split', 'test_split']:\n",
    "    temp_dict = {'set': {}, 'counter': {}}\n",
    "    for m1 in task1_dict[split]:\n",
    "        cl = m1['class_label']\n",
    "        temp_dict['counter'][cl] = temp_dict['counter'].get(cl, 0) + 1\n",
    "        temp_dict['set'][cl] = temp_dict['set'].get(cl, set(m1['symptom']))\n",
    "        temp_dict['set'][cl].update(m1['symptom'])\n",
    "        \n",
    "    print(split, ':', )\n",
    "    pprint.pprint(temp_dict)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
