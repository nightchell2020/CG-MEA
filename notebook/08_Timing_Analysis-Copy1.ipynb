{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aef1d0b-f084-44c6-a332-127a3d4af1fb",
   "metadata": {},
   "source": [
    "# Occlusion Sensitivity\n",
    "\n",
    "This notebook conducts an experiment for the occlusion sensitivity of our networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b731a-e5d9-429d-9313-4d6ad242a70f",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52098d-3075-4185-8408-a5bc3a52b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdea24-1a20-4989-9b69-69c05da32f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import hydra\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cycler import cycler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredDirectionArrows\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# custom package\n",
    "from datasets.caueeg_script import load_caueeg_config\n",
    "from datasets.caueeg_script import load_caueeg_task_datasets\n",
    "from datasets.caueeg_script import make_dataloader\n",
    "from datasets.caueeg_script import compose_preprocess\n",
    "from datasets.pipeline import EegDropChannels\n",
    "from datasets.pipeline import EegToTensor\n",
    "from datasets.pipeline import eeg_collate_fn\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f80b1-51d1-4b23-ba5a-97750a8ab3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa174d-f3f2-4de6-91b6-4e64ab0cd86a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddbd12-da9a-4373-9775-020ab6a6fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_list = [\n",
    "#     '1vc80n1f',  # 1D-VGG-19 \n",
    "#     'l8524nml',  # 1D-ResNet-18   // 2s1700lg, l8524nml\n",
    "#     'gvqyvmrj',  # 1D-ResNet-50 \n",
    "#     'v301o425',  # 1D-ResNeXt-50 \n",
    "#     'lo88puq7',  # 2D-VGG-19\n",
    "#     'xci5svkl',  # 2D-ResNet-18 \n",
    "#     'syrx7bmk',  # 2D-ResNet-50 \n",
    "#     '1sl7ipca',  # 2D-ResNeXt-50 \n",
    "#     'gjkysllw',  # 2D-ViT-B-16 \n",
    "# ]\n",
    "\n",
    "model_list = [\n",
    "    # 'nemy8ikm',  # 1D-VGG-19\n",
    "    # '4439k9pg',  # 1D-ResNet-18\n",
    "    # 'q1hhkmik',  # 1D-ResNet-50\n",
    "    # 'tp7qn5hd',  # 1D-ResNeXt-50 \n",
    "    # 'ruqd8r7g',  # 2D-VGG-19\n",
    "    'dn10a6bv',  # 2D-ResNet-18\n",
    "    'atbhqdgg',  # 2D-ResNet-50\n",
    "    # '0svudowu',  # 2D-ResNeXt-50\n",
    "    # '1cdws3t5',  # 2D-ViT-B-16\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119a08f-f9cc-4db4-a8f6-c7030bd08813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dfec22-3837-46a7-8b4d-4588300068bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = 512\n",
    "time_interval = 10  # 10\n",
    "\n",
    "target_datasets = [\n",
    "    'train',\n",
    "    # 'val',\n",
    "    # 'test',\n",
    "]\n",
    "\n",
    "save_fig = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99490d-03bd-4bee-8bed-5253ab045b54",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa336b7-2298-4da9-8072-af22f2a81958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compose_transforms_modified(config, verbose=False):\n",
    "    transform = []\n",
    "    \n",
    "    channel_reduction_list = config.get(\"channel_reduction_list\", [])\n",
    "    if config.get(\"EKG\", None) not in [\"O\", \"X\", None]:\n",
    "        raise ValueError(f\"config['EKG'] should be one of ['O', 'X', None].\")\n",
    "    elif config.get(\"EKG\", None) == \"X\":\n",
    "        channel_reduction_list.append(config[\"signal_header\"].index(\"EKG\"))\n",
    "\n",
    "    if config.get(\"photic\", None) not in [\"O\", \"X\", None]:\n",
    "        raise ValueError(f\"config['photic'] should be one of ['O', 'X', None].\")\n",
    "    elif config.get(\"photic\", None) == \"X\":\n",
    "        channel_reduction_list.append(config[\"signal_header\"].index(\"Photic\"))    \n",
    "    channel_reduction_set = set(channel_reduction_list)\n",
    "    transform += [EegDropChannels(sorted([*channel_reduction_set]))]\n",
    "    transform += [EegToTensor()]\n",
    "    transform = transforms.Compose(transform)\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a92e2-c6e7-495a-9e52-9ccacf67ac7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset_for_train2(config, verbose=False):\n",
    "    dataset_path = config[\"dataset_path\"]\n",
    "    if \"cwd\" in config:\n",
    "        dataset_path = os.path.join(config[\"cwd\"], dataset_path)\n",
    "\n",
    "    config_dataset = load_caueeg_config(dataset_path)\n",
    "    config.update(**config_dataset)\n",
    "\n",
    "    if \"run_mode\" not in config.keys():\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        print('WARNING: run_mode is not specified.\\n \\t==> run_mode is set to \"train\" automatically.')\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        config[\"run_mode\"] = \"train\"\n",
    "\n",
    "    transform = compose_transforms_modified(config, verbose=verbose)\n",
    "    config[\"transform\"] = transform\n",
    "    load_event = config[\"load_event\"] or config.get(\"reject_events\", False)\n",
    "\n",
    "    _ = load_caueeg_task_datasets(dataset_path=dataset_path, task=config[\"task\"], \n",
    "                                  load_event=load_event, file_format=config[\"file_format\"], \n",
    "                                  transform=transform, verbose=verbose)\n",
    "    config_task, train_dataset, val_dataset, test_dataset = _\n",
    "    config.update(**config_task)\n",
    "\n",
    "    _ = make_dataloader(config, train_dataset, \n",
    "                        val_dataset, test_dataset, test_dataset, verbose=False)\n",
    "    train_loader, val_loader, test_loader, multicrop_test_loader = _\n",
    "\n",
    "    preprocess_train, preprocess_test = compose_preprocess(config, train_loader, verbose=verbose)\n",
    "    config[\"preprocess_train\"] = preprocess_train\n",
    "    config[\"preprocess_test\"] = preprocess_test\n",
    "    config[\"in_channels\"] = preprocess_train(next(iter(train_loader)))[\"signal\"].shape[1]\n",
    "    config[\"out_dims\"] = len(config[\"class_label_to_name\"])\n",
    "\n",
    "    if verbose:\n",
    "        for i_batch, sample_batched in enumerate(train_loader):\n",
    "            # preprocessing includes to-device operation\n",
    "            preprocess_train(sample_batched)\n",
    "\n",
    "            print(\n",
    "                i_batch,\n",
    "                sample_batched[\"signal\"].shape,\n",
    "                sample_batched[\"age\"].shape,\n",
    "                sample_batched[\"class_label\"].shape,\n",
    "            )\n",
    "\n",
    "            if i_batch > 3:\n",
    "                break\n",
    "        print(\"\\n\" + \"-\" * 100 + \"\\n\")\n",
    "\n",
    "    return (\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        multicrop_test_loader,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba96a8e-94b1-4ce3-9877-6f767bd6a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_score(model, sample_batched, config):\n",
    "    # compute output embedding\n",
    "    x = sample_batched['signal']\n",
    "    age = sample_batched['age']\n",
    "    output = model.compute_feature_embedding(x, age)\n",
    "\n",
    "    # map depending on the loss function\n",
    "    # if config['criterion'] == 'cross-entropy':\n",
    "    #     score = F.softmax(output, dim=1)\n",
    "    # elif config['criterion'] == 'multi-bce':\n",
    "    #     score = torch.sigmoid(output)\n",
    "    # elif config['criterion'] == 'svm':\n",
    "    #     score = output\n",
    "    # else:\n",
    "    #     raise ValueError(f\"estimate_score(): cannot parse config['criterion']={config['criterion']}.\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e085b-fadf-40c4-a37b-1913c4f8a9c6",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca0f95-ed61-474c-9075-ae624342d270",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name in tqdm(model_list, desc='Model', leave=False):\n",
    "    # load from disk\n",
    "    try:\n",
    "        path = os.path.join(r'./local/checkpoint', model_name, 'checkpoint.pt')\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    model_state = ckpt['model_state']\n",
    "    config = ckpt['config']\n",
    "\n",
    "    # initiate the model\n",
    "    if '_target_' in config:\n",
    "        model = hydra.utils.instantiate(config).to(device)\n",
    "    elif type(config['generator']) is str:\n",
    "        config['generator'] = getattr(models, config['generator'].split('.')[-1])\n",
    "        if 'block' in config:\n",
    "            config['block'] = getattr(models, config['block'].split('.')[-1])\n",
    "        model = config['generator'](**config).to(device)\n",
    "    else:\n",
    "        if 'block' in config:\n",
    "            if config['block'] == models.resnet_1d.BottleneckBlock1D:\n",
    "                config['block'] = 'bottleneck'\n",
    "            elif config['block'] == models.resnet_2d.Bottleneck2D:\n",
    "                config['block'] = 'bottleneck'\n",
    "            elif config['block'] == models.resnet_1d.BasicBlock1D:\n",
    "                config['block'] = 'basic'\n",
    "            elif config['block'] == models.resnet_2d.BasicBlock2D:\n",
    "                config['block'] = 'basic'\n",
    "    \n",
    "        model = config['generator'](**config).to(device)\n",
    "    \n",
    "    if config.get('ddp', False):\n",
    "        model_state_ddp = deepcopy(model_state)\n",
    "        model_state = OrderedDict()\n",
    "        for k, v in model_state_ddp.items():\n",
    "            name = k[7:]  # remove 'module.' of DataParallel/DistributedDataParallel\n",
    "            model_state[name] = v\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    task = config['task']\n",
    "    config.pop('cwd', 0)\n",
    "    config['ddp'] = False\n",
    "    config['minibatch'] = 1\n",
    "    config['crop_multiple'] = 1\n",
    "    config['test_crop_multiple'] = 1\n",
    "    config['crop_timing_analysis'] = False\n",
    "    config['eval'] = True\n",
    "    config['device'] = device\n",
    "    \n",
    "    if '220419' in config['dataset_path']:\n",
    "        config['dataset_path'] = './local/dataset/caueeg-dataset/'\n",
    "    config['run_mode'] = 'eval'\n",
    "    \n",
    "    _ = build_dataset_for_train2(config, verbose=False)\n",
    "    train_loader = _[0]\n",
    "    val_loader = _[1]\n",
    "    test_loader = _[2]\n",
    "\n",
    "    for target_dataset in tqdm(target_datasets, desc=\"Dataset\", leave=False):\n",
    "        if target_dataset == 'train':\n",
    "            loader = train_loader\n",
    "        elif target_dataset == 'val':\n",
    "            loader = val_loader\n",
    "        elif target_dataset == 'test':\n",
    "            loader = test_loader\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "                \n",
    "        for sample in tqdm(loader, total=len(loader), desc='Batch', leave=False):\n",
    "            timing_results = []\n",
    "            sample_origin = deepcopy(sample)\n",
    "            st = config.get(\"latency\", 0)\n",
    "            ed = sample_origin['signal'].shape[2] - config['seq_length'] + 1\n",
    "            sample_batched = []\n",
    "            \n",
    "            for t in range(st, ed, time_interval):\n",
    "                sample = deepcopy(sample_origin)\n",
    "\n",
    "                # custom cropping\n",
    "                sample['signal'] = sample['signal'][:, :, t:t + config['seq_length']]\n",
    "                config['preprocess_test'](sample)\n",
    "                sample['timing'] = t\n",
    "\n",
    "                # gather minibatch\n",
    "                sample_batched.append(sample)\n",
    "\n",
    "                # gather data until it becomes minibatch size\n",
    "                if len(sample_batched) == minibatch:\n",
    "                    sample_batched = eeg_collate_fn(sample_batched)\n",
    "                    N, _, *R = sample_batched['signal'].shape\n",
    "                    sample_batched['signal'] = sample_batched['signal'].reshape(N, *R)\n",
    "                    sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "                    sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "                    s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        result = {\n",
    "                            'st': sample_batched['timing'][i],\n",
    "                            'ed': sample_batched['timing'][i] + config['seq_length'],\n",
    "                            'logit': s[i].cpu(),\n",
    "                        }\n",
    "                        timing_results.append(result)\n",
    "                    sample_batched = []\n",
    "\n",
    "            # the rest data\n",
    "            if len(sample_batched) > 0:\n",
    "                sample_batched = eeg_collate_fn(sample_batched)\n",
    "                N, _, *R = sample_batched['signal'].shape\n",
    "                sample_batched['signal'] = sample_batched['signal'].reshape(N, *R)\n",
    "                sample_batched['age'] = sample_batched['age'].reshape(N)\n",
    "                sample_batched['class_label'] = sample_batched['class_label'].reshape(N)\n",
    "                s = estimate_score(model, sample_batched, config)\n",
    "\n",
    "                for i in range(N):\n",
    "                    result = {\n",
    "                        'st': sample_batched['timing'][i],\n",
    "                        'ed': sample_batched['timing'][i] + config['seq_length'],\n",
    "                        'logit': s[i].cpu(),\n",
    "                    }\n",
    "                    timing_results.append(result)\n",
    "                sample_batched = []\n",
    "            \n",
    "            result_dict = {\n",
    "                'name': model_name,\n",
    "                'model': config['model'],\n",
    "                'seq_length': config['seq_length'],\n",
    "                'serial': sample['serial'][0],\n",
    "                'class_label': sample['class_label'][0],\n",
    "                'timing_results': timing_results,\n",
    "            }\n",
    "            \n",
    "            path = f'local/output/timing_analysis/{model_name}/{target_dataset}/'\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            torch.save(result_dict, os.path.join(path, sample[\"serial\"][0] + '.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd41b9-bbb7-462f-a3ba-3b6a7eee4da8",
   "metadata": {},
   "source": [
    "## Post analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f2670-a723-4b02-b6a3-7dccf38306ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_results = []\n",
    "for model_name in tqdm(model_list, desc='Model', leave=False):\n",
    "    results = {'model': model_name}\n",
    "    for dataset in tqdm(target_datasets, desc=\"Dataset\", leave=False):\n",
    "        results[dataset] = []\n",
    "        for fname in tqdm(glob.glob(f'local/output/timing_analysis/{model_name}/{dataset}/*'), desc='Inner Loop', leave=False):\n",
    "            result_dict = torch.load(fname)\n",
    "            timing_results = result_dict['timing_results']\n",
    "            C = timing_results[0]['logit'].shape[0]\n",
    "            eed = timing_results[-1]['ed']\n",
    "            \n",
    "            result = {'serial': result_dict['serial'], \n",
    "                      'name': result_dict['name'],\n",
    "                      'model': result_dict['model'],\n",
    "                      'class_label': result_dict['class_label'],\n",
    "                      'seq_length': result_dict['seq_length'],\n",
    "                      'timing': torch.arange(eed), \n",
    "                      'logit': torch.zeros((eed, C)), \n",
    "                      'fraction': torch.zeros((eed))}\n",
    "            \n",
    "            for i, r in enumerate(timing_results):\n",
    "                result['logit'][r['st']:r['ed']] += r['logit']\n",
    "                result['fraction'][r['st']:r['ed']] += 1\n",
    "            results[dataset].append(result) \n",
    "    total_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35026b76-99af-41b4-984b-105f25ff28cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_by_serial = {}\n",
    "\n",
    "for results in total_results:\n",
    "    for dataset, result in results.items():\n",
    "        if dataset == 'model':\n",
    "            continue\n",
    "        for r in result:\n",
    "            if r['serial'] not in results_by_serial.keys():\n",
    "                results_by_serial[r['serial']] = []\n",
    "                \n",
    "            fraction = r['fraction']\n",
    "            fraction[fraction < 0.5] = 1\n",
    "            results_by_serial[r['serial']].append({\n",
    "                'name': r['name'],\n",
    "                'model': r['model'],\n",
    "                'seq_length': r['seq_length'],\n",
    "                'class_label': r['class_label'].item(),\n",
    "                'timing': r['timing'].numpy(),\n",
    "                'logit': (r['logit'] / fraction.unsqueeze(1)).numpy(),\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcdcde-1615-40da-bb0c-2d977ea8a9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensemble_result_by_serial = {}\n",
    "\n",
    "# for serial, results in results_by_serial.items():\n",
    "#     ensemble_score = torch.zeros_like(torch.Tensor(results[0]['score']))\n",
    "#     for r in results:\n",
    "#         ensemble_score += r['score']\n",
    "#     ensemble_score /= len(results)\n",
    "#     ensemble_result_by_serial[serial] = ensemble_score\n",
    "\n",
    "# path = f'local/output/timing_analysis/'\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "# torch.save(ensemble_result_by_serial, os.path.join(path, 'ensemble_result_by_serial.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf0058-f73a-4762-bba2-99f5da978eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "plt.style.use('classic') \n",
    "plt.style.use('default') \n",
    "plt.style.use('bmh') # default, ggplot, fivethirtyeight, bmh, dark_background, classic\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.rcParams.update({'font.family': 'Arial'})\n",
    "\n",
    "base_path = f'./local/output/imgs/timing_analysis'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "colors = ['tab:red', 'tab:orange', 'tab:green']\n",
    "\n",
    "for serial, results in tqdm(results_by_serial.items(), desc='Serial', leave=False):\n",
    "    N = max(len(results), 2)\n",
    "    fig, axs = plt.subplots(N, 1, sharex=True, figsize=(15.0, 3.0 * N), constrained_layout=True)\n",
    "    # fig.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "    # fig.subplots_adjust(hspace=0)    `\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        score = torch.Tensor(result['logit']).softmax(axis=1).numpy()\n",
    "        for c in range(result['logit'].shape[1]):\n",
    "            axs[i].plot(result['timing'], score[:, c], colors[c], \n",
    "                        lw=2.0 if c == result['class_label'] else 1.0, \n",
    "                        ls='-' if c == result['class_label'] else '--')\n",
    "        axs[i].set_xlim(0, result['timing'][-1])\n",
    "        axs[i].set_ylim(0, 1.0)\n",
    "        axs[i].set_ylabel('Prob')\n",
    "        axs[i].set_title(f\"Model: {result['model']}, Crop length: {result['seq_length']}\")\n",
    "        \n",
    "    axs[-1].set_xlabel('Timing')\n",
    "    fig.suptitle(serial, fontsize=25, fontweight='semibold')\n",
    "    fig.savefig(os.path.join(base_path, f'{serial}.jpg'), transparent=True)\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbf328-22c9-4fc5-a8db-935b628f261e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
