{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 1D CNN\n",
    "\n",
    "1D Convolution을 기본 구성 요소로 하는 EEG classifier를 학습해보는 노트북.\n",
    "\n",
    "- Three-way SoftMax classifier of normal, non-vascular MCI, and non-vascular dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from itertools import cycle\n",
    "\n",
    "# custom package\n",
    "from utils.eeg_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook name\n",
    "def get_notebook_name():\n",
    "    import ipynbname\n",
    "    return ipynbname.name()\n",
    "\n",
    "nb_fname = get_notebook_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n",
    "#  'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n",
    "#  'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n",
    "#  'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', \n",
    "#  'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic' # for Hangul in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "data_path = r'dataset/02_Curated_Data/'\n",
    "meta_path = os.path.join(data_path, 'metadata_debug.json')\n",
    "\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        # inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random.seed(0)\n",
    "\n",
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())\n",
    "\n",
    "# random seed\n",
    "random.seed()\n",
    "\n",
    "# print([m['serial']  for m in metadata_train[:15]])\n",
    "# print([m['serial']  for m in metadata_val[:15]])\n",
    "# print([m['serial']  for m in metadata_test[:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap the splitted data using PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for m in metadata_train:\n",
    "    ages.append(m['age'])\n",
    "\n",
    "ages = np.array(ages)\n",
    "age_mean = np.mean(ages)\n",
    "age_std = np.std(ages)\n",
    "\n",
    "print('Age mean and standard deviation:')\n",
    "print(age_mean, age_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_length = 200* 10 # 10 seconds\n",
    "\n",
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=crop_length),\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGAddGaussianNoise(mean=0.0, std=5e-2),\n",
    "                               EEGToTensor()])\n",
    "\n",
    "train_dataset = EEGDataset(data_path, metadata_train, composed)\n",
    "val_dataset = EEGDataset(data_path, metadata_val, composed)\n",
    "test_dataset = EEGDataset(data_path, metadata_test, composed)\n",
    "\n",
    "print(train_dataset[0]['signal'].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0]['signal'].shape)\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0]['signal'].shape)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current PyTorch device:', device)\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(i_batch, \n",
    "          sample_batched['signal'].shape, \n",
    "          sample_batched['age'].shape, \n",
    "          sample_batched['class_label'].shape, \n",
    "          len(sample_batched['metadata']))\n",
    "    \n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation, test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        num_workers=num_workers, \n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=eeg_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=32, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory,\n",
    "                         collate_fn=eeg_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Define Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def visualize_network_tensorboard(model, name):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter('runs/' + nb_fname + '_' + name)\n",
    "\n",
    "    for batch_i, sample_batched in enumerate(train_loader):\n",
    "        # pull up the batch data\n",
    "        x = sample_batched['signal'].to(device)\n",
    "        age = sample_batched['age'].to(device)\n",
    "        target = sample_batched['class_label'].to(device)\n",
    "\n",
    "        # apply model on whole batch directly on device\n",
    "        writer.add_graph(model, (x, age))\n",
    "        output = model(x, age, print_shape=True)\n",
    "        break\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer on top of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1024):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCNN(nn.Module):\n",
    "    def __init__(self, n_input=20, n_output=3, stride=2, n_channel=512, \n",
    "                 dropout=0.1, n_encoders=2, crop_length=200*60, use_age=True):\n",
    "        super().__init__()\n",
    "        self.use_age = use_age\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=25, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "    \n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=9, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(n_channel, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(n_channel, nhead=8, \n",
    "                                                    dim_feedforward=n_channel*2, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_encoders)\n",
    "        \n",
    "        self.final_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        if self.use_age:        \n",
    "            self.fc1 = nn.Linear(n_channel + 1, n_channel)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(n_channel, n_channel)                \n",
    "                    \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.bnfc1 = nn.BatchNorm1d(n_channel)\n",
    "        self.fc2 = nn.Linear(n_channel, n_output)\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "                \n",
    "    def forward(self, x, age, print_shape=False):\n",
    "        # conv-bn-relu-pool \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        \n",
    "        x = x.permute(2, 0, 1) # minibatch, dimension, length --> length, minibatch, dimension\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 2, 0) # length, minibatch, dimension --> minibatch, dimension, length\n",
    "        \n",
    "        if print_shape:\n",
    "            print('Shape right before squeezing:', x.shape)\n",
    "            if x.shape[2] > 1024:\n",
    "                print('- Problematic! Sequence length exceeds 1,024.')\n",
    "\n",
    "        x = self.final_pool(x).squeeze()\n",
    "        if self.use_age:\n",
    "            x = torch.cat((x, age.reshape(-1, 1)), dim=1)\n",
    "\n",
    "        # fc-bn-dropout-relu-fc\n",
    "        x = self.fc1(x)\n",
    "        x = self.bnfc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_TransformerCNN():\n",
    "    return TransformerCNN(n_input=train_dataset[0]['signal'].shape[0], n_output=3, \n",
    "                          n_channel=512, dropout=0.3, crop_length=crop_length, use_age=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = generate_TransformerCNN()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# tensorboard visualization\n",
    "visualize_network_tensorboard(model, 'Transformer-CNN')\n",
    "\n",
    "# number of parameters\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Some useful functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val_accuracy(model, repeat=1):\n",
    "    model.eval()\n",
    "    \n",
    "    correct, total = (0, 0)\n",
    "    C = len(class_label_to_type)\n",
    "    val_confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in range(repeat):\n",
    "            for sample_batched in val_loader:\n",
    "                # pull up the data\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                target = sample_batched['class_label'].to(device)\n",
    "\n",
    "                # apply model on whole batch directly on device\n",
    "                output = model(x, age)\n",
    "                pred = F.log_softmax(output, dim=1)\n",
    "\n",
    "                # val accuracy\n",
    "                pred = pred.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(target).sum().item()\n",
    "                total += pred.shape[0]\n",
    "\n",
    "                # confusion matrix\n",
    "                val_confusion += calculate_confusion_matrix(pred, target)\n",
    "            \n",
    "    val_accuracy = 100.0 * correct / total\n",
    "    return (val_accuracy, val_confusion)\n",
    "\n",
    "    \n",
    "def check_test_accuracy(model, repeat=1):\n",
    "    model.eval()\n",
    "    \n",
    "    # for test accuracy\n",
    "    correct, total = (0, 0) \n",
    "    \n",
    "    # for confusion matrix\n",
    "    C = len(class_label_to_type)\n",
    "    test_confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    # for test debug\n",
    "    test_debug = {data['metadata']['serial']: \n",
    "                  {'GT': data['class_label'].item(), \n",
    "                   'Acc': 0, \n",
    "                   'Pred': [0] * C} for data in test_dataset}\n",
    "    \n",
    "    # for ROC curve\n",
    "    score = None\n",
    "    target = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in range(repeat):\n",
    "            for sample_batched in test_loader:\n",
    "                # pull up the data\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                y = sample_batched['class_label'].to(device)\n",
    "\n",
    "                # apply model on whole batch directly on device\n",
    "                output = model(x, age)\n",
    "                s = F.softmax(output, dim=1)\n",
    "                pred = F.log_softmax(output, dim=1)\n",
    "\n",
    "                # test accuracy\n",
    "                pred = pred.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(y).sum().item()\n",
    "                total += pred.shape[0]\n",
    "\n",
    "                if score is None:\n",
    "                    score = s.detach().cpu().numpy()\n",
    "                    target = y.detach().cpu().numpy()\n",
    "                else:\n",
    "                    score = np.concatenate((score, s.detach().cpu().numpy()), axis=0)\n",
    "                    target = np.concatenate((target, y.detach().cpu().numpy()), axis=0)\n",
    "\n",
    "                # confusion matrix\n",
    "                test_confusion += calculate_confusion_matrix(pred, y)\n",
    "\n",
    "                # test debug\n",
    "                for n in range(pred.shape[0]):\n",
    "                    serial = sample_batched['metadata'][n]['serial']\n",
    "                    test_debug[serial]['edfname'] = sample_batched['metadata'][n]['edfname']\n",
    "                    test_debug[serial]['Pred'][pred[n].item()] += 1\n",
    "                    acc = test_debug[serial]['Pred'][y[n].item()] / np.sum(test_debug[serial]['Pred']) * 100\n",
    "                    test_debug[serial]['Acc'] = f'{acc:>6.02f}%'\n",
    "        \n",
    "    test_accuracy = 100.0 * correct / total\n",
    "    return (test_accuracy, test_confusion, test_debug, score, target)\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(pred, target):\n",
    "    N = target.shape[0]\n",
    "    C = len(class_label_to_type)\n",
    "    confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        r = target[i]\n",
    "        c = pred[i]\n",
    "        confusion[r, c] += 1\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_plot(losses, lr_decay_step):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.0, 3.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    N = len(losses)\n",
    "    x = np.arange(1, N + 1)\n",
    "    ax.plot(x, losses)\n",
    "    \n",
    "    if type(lr_decay_step) is list:\n",
    "        ax.vlines(lr_decay_step, 0, 1, transform=ax.get_xaxis_transform(), \n",
    "                  colors='m', alpha=0.5, linestyle='solid')\n",
    "    else:\n",
    "        x2 = np.arange(lr_decay_step, N, lr_decay_step)\n",
    "        ax.vlines(x2, 0, 1, transform=ax.get_xaxis_transform(), \n",
    "                  colors='m', alpha=0.5, linestyle='solid')\n",
    "    # ax.vlines([1, N], 0, 1, transform=ax.get_xaxis_transform(), \n",
    "    #           colors='k', alpha=0.7, linestyle='solid')\n",
    "    \n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_title('Loss Plot')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Training Loss')\n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "\n",
    "def draw_accuracy_history(train_acc_history, val_acc_history, history_interval, lr_decay_step):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.0, 3.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    N = len(train_acc_history) * history_interval\n",
    "    x = np.arange(history_interval, N + 1, history_interval)\n",
    "    ax.plot(x, train_acc_history, 'r-', label='Train accuracy')\n",
    "    ax.plot(x, val_acc_history, 'b-', label='Validation accuracy')\n",
    "    \n",
    "    if type(lr_decay_step) is list:\n",
    "        ax.vlines(lr_decay_step, 0, 1, transform=ax.get_xaxis_transform(), \n",
    "                  colors='m', alpha=0.5, linestyle='solid')\n",
    "    else:\n",
    "        x2 = np.arange(lr_decay_step, N + 1, lr_decay_step)\n",
    "        ax.vlines(x2, 0, 1, transform=ax.get_xaxis_transform(), \n",
    "                  colors='m', alpha=0.5, linestyle='solid')\n",
    "    # ax.vlines([history_interval, N], 0, 1, transform=ax.get_xaxis_transform(), \n",
    "    #           colors='k', alpha=0.7, linestyle='solid')\n",
    "    \n",
    "    ax.set_xlim(left=0)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title('Accuracy Plot during Training')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(confusion):\n",
    "    C = len(class_label_to_type)\n",
    "    \n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    plt.rcParams['image.cmap'] = 'jet' # 'nipy_spectral'\n",
    "\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(4.0, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    im = ax.imshow(confusion, alpha=0.8)\n",
    "\n",
    "    ax.set_xticks(np.arange(C))\n",
    "    ax.set_yticks(np.arange(C))\n",
    "    ax.set_xticklabels(class_label_to_type)\n",
    "    ax.set_yticklabels(class_label_to_type)\n",
    "    \n",
    "    for r in range(C):\n",
    "        for c in range(C):\n",
    "            text = ax.text(c, r, confusion[r, c],\n",
    "                           ha=\"center\", va=\"center\", color='k')\n",
    "    \n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "def draw_roc_curve(score, target):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    \n",
    "    # Binarize the output\n",
    "    n_classes = len(class_label_to_type)\n",
    "    target = label_binarize(target, classes=np.arange(n_classes))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(target[:, i], score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(target.ravel(), score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # draw class-agnostic ROC curve\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.5, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    lw = 1.5\n",
    "    colors = cycle(['limegreen', 'mediumpurple', 'darkorange', \n",
    "                    'dodgerblue', 'lightcoral', 'goldenrod', \n",
    "                    'indigo', 'darkgreen', 'navy', 'brown'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                label='{0} (area = {1:0.2f})'\n",
    "                ''.format(class_label_to_type[i], roc_auc[i]))    \n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Class-Wise ROC Curves')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Plot class-aware ROC curves\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle='-', linewidth=lw)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle='-', linewidth=lw)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Class-Agnostic ROC Curves')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_search(model, min_log_lr, max_log_lr, trials, iters):\n",
    "    learning_rate_record = []\n",
    "    for t in tqdm(range(trials)):\n",
    "        log_lr = np.random.uniform(min_log_lr, max_log_lr)\n",
    "        lr = 10 ** log_lr\n",
    "        \n",
    "        model.reset_weights()\n",
    "        model.train()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "        correct, total = (0, 0)\n",
    "        \n",
    "        i = 1\n",
    "        while True:\n",
    "            for sample_batched in train_loader:\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                target = sample_batched['class_label'].to(device)\n",
    "\n",
    "                output = model(x, age)\n",
    "                pred = F.log_softmax(output, dim=1)\n",
    "                loss = F.nll_loss(pred, target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = pred.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(target).sum().item()\n",
    "                total += pred.shape[0]\n",
    "                \n",
    "                i += 1\n",
    "                if i >= iters:\n",
    "                    break\n",
    "            if i >= iters:\n",
    "                break\n",
    "        \n",
    "        train_accuracy = 100.0 * correct / total\n",
    "        \n",
    "        # Train accuracy for the final epoch is stored\n",
    "        learning_rate_record.append((log_lr, train_accuracy))\n",
    "    \n",
    "    return learning_rate_record\n",
    "\n",
    "\n",
    "def draw_learning_rate_record(learning_rate_record):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "\n",
    "    fig = plt.figure(num=1, clear=True, constrained_layout=True) # figsize=(6.0, 6.0)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.set_title('Learning Rate Search')\n",
    "    ax.set_xlabel('Learning rate in log-scale')\n",
    "    ax.set_ylabel('Train accuracy')\n",
    "    \n",
    "\n",
    "    ax.scatter(*max(learning_rate_record, key=lambda x: x[1]), \n",
    "               s=150, c='w', marker='o', edgecolors='limegreen')\n",
    "    \n",
    "    for log_lr, val_accuracy in learning_rate_record:\n",
    "        ax.scatter(log_lr, val_accuracy, c='r',\n",
    "                   alpha=0.5, edgecolors='none')\n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pool = []\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = 'Transformer-CNN'\n",
    "model_dict['generator'] = generate_TransformerCNN\n",
    "model_dict['lr_start'] = None\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "pprint.pp(model_pool, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dict in model_pool:\n",
    "    if model_dict['lr_start'] is None:\n",
    "        print(f'{model_dict[\"name\"]} LR searching..')\n",
    "        model = model_dict['generator']().to(device)\n",
    "        model.train()\n",
    "        \n",
    "        record = learning_rate_search(model, min_log_lr=-4.5, max_log_lr=-1.4, \n",
    "                                      trials=300, iters=30)\n",
    "        \n",
    "        draw_learning_rate_record(record)\n",
    "        best_log_lr = record[np.argmax(np.array([v for lr, v in record]))][0]\n",
    "        model_dict['lr_start'] = 10 ** best_log_lr\n",
    "        \n",
    "        print(f'best lr {model_dict[\"lr_start\"]:.5e} / log_lr {best_log_lr}')\n",
    "    else:\n",
    "        print(f'{model_dict[\"name\"]}: {model_dict[\"lr_start\"]:.5e}')\n",
    "        \n",
    "    print('-' * 100)\n",
    "        \n",
    "pprint.pp(model_pool, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_check = True\n",
    "save_model = True\n",
    "draw_result = True\n",
    "\n",
    "# log path\n",
    "log_path = f'history_temp/{nb_fname}/'\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# train iterations\n",
    "n_repeats = 1\n",
    "n_iters = 70000\n",
    "# n_iters = 12500 * ((200 * 60) // crop_length)\n",
    "history_interval = n_iters // 400\n",
    "lr_decay_step = round(n_iters * 0.8)\n",
    "\n",
    "print(f'# iterations: {n_iters}, # steps for LR decay: {lr_decay_step}')\n",
    "\n",
    "# progress bar\n",
    "pbar = tqdm(total=len(model_pool) * n_iters * n_repeats)\n",
    "\n",
    "# train process on model_pool\n",
    "for model_dict in model_pool:\n",
    "    print(f'{\"*\"*40} {model_dict[\"name\"]} train starts {\"*\"*40}')\n",
    "    best_r_test_acc = 0\n",
    "    \n",
    "    for r in range(n_repeats):\n",
    "        if file_check:\n",
    "            endwith = '' if n_repeats == 1 else f'_r{r:02d}'\n",
    "            path = os.path.join(log_path, f'{model_dict[\"name\"]}_log{endwith}')\n",
    "            if os.path.isfile(path):\n",
    "                log_dict = torch.load(path)\n",
    "                # loss and accuracy plots\n",
    "                if draw_result:\n",
    "                    draw_loss_plot(log_dict[\"losses\"], log_dict[\"lr_decay_step\"])\n",
    "                    draw_accuracy_history(log_dict[\"train_acc_history\"], log_dict[\"val_acc_history\"], \n",
    "                                          log_dict[\"history_interval\"], log_dict[\"lr_decay_step\"])\n",
    "                script = f'- {r:02d} train accuracy {log_dict[\"train_acc_history\"][-1]:.2f}%, '\\\n",
    "                f'best / last test accuracies {log_dict[\"best_test_accuracy\"]:.2f}% / {log_dict[\"last_test_accuracy\"]:.2f}% - file exists'\n",
    "                print()\n",
    "                print(script)\n",
    "                print()\n",
    "                pbar.update(n_iters)\n",
    "                continue\n",
    "\n",
    "        # load the model dict\n",
    "        model = model_dict['generator']().to(device)\n",
    "        \n",
    "        model.train()\n",
    "        lr_start = model_dict['lr_start']\n",
    "        \n",
    "        # configure for training\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr_start, weight_decay=1e-2)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)\n",
    "\n",
    "        # log during training\n",
    "        losses = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        best_val_acc = 0\n",
    "        correct, total = (0, 0)\n",
    "\n",
    "        i = 0    \n",
    "        while True:\n",
    "            for sample_batched in train_loader:\n",
    "                model.train()\n",
    "\n",
    "                # load the data\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                target = sample_batched['class_label'].to(device)\n",
    "\n",
    "                # forward pass\n",
    "                output = model(x, age)\n",
    "                pred = F.log_softmax(output, dim=1)\n",
    "                loss = F.nll_loss(pred, target)\n",
    "\n",
    "                # backward and update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "                # train accuracy\n",
    "                pred = pred.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(target).sum().item()\n",
    "                total += pred.shape[0]\n",
    "\n",
    "                # log\n",
    "                losses.append(loss.item())\n",
    "                pbar.update(1)\n",
    "                i += 1\n",
    "\n",
    "                # history\n",
    "                if i % history_interval == 0:\n",
    "                    train_acc = 100.0 * correct / total\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    correct, total = (0, 0)\n",
    "\n",
    "                    val_acc, _ = check_val_accuracy(model, repeat=5)\n",
    "                    val_acc_history.append(val_acc)\n",
    "\n",
    "                    if best_val_acc < val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "                if i >= n_iters:\n",
    "                    break\n",
    "            if i >= n_iters:\n",
    "                break\n",
    "        \n",
    "        # loss and accuracy plots\n",
    "        if draw_result:\n",
    "            draw_loss_plot(losses, lr_decay_step)\n",
    "            draw_accuracy_history(train_acc_history, val_acc_history, history_interval, lr_decay_step)\n",
    "            \n",
    "        # calculate the test accuracies for best and last models\n",
    "        last_model_state = deepcopy(model.state_dict())\n",
    "        last_test_acc, last_test_confusion, last_test_debug, _, _ = check_test_accuracy(model, repeat=30)\n",
    "        \n",
    "        model.load_state_dict(best_model_state)\n",
    "        best_test_acc, best_test_confusion, best_test_debug, _, _ = check_test_accuracy(model, repeat=30)\n",
    "\n",
    "        # save the model if it is best among repeatedly trained models\n",
    "        if save_model and best_r_test_acc < max(last_test_acc, best_test_acc):\n",
    "            best_r_test_acc = max(last_test_acc, best_test_acc)\n",
    "            model_state = last_model_state if best_test_acc < last_test_acc else best_model_state\n",
    "            path = os.path.join(log_path, f'{model_dict[\"name\"]}')\n",
    "            torch.save(model_state, path)\n",
    "\n",
    "        # leave the log\n",
    "        endwith = '' if n_repeats == 1 else f'_r{r:02d}'\n",
    "        path = os.path.join(log_path, f'{model_dict[\"name\"]}_log{endwith}')\n",
    "        log_dict = {}\n",
    "        log_dict['model'] = model_dict['name']\n",
    "        log_dict['num_params'] = count_parameters(model)\n",
    "        log_dict['weight_decay'] = optimizer.defaults['weight_decay']\n",
    "        log_dict['starting_lr'] = lr_start\n",
    "        log_dict['final_lr'] = optimizer.param_groups[-1][\"lr\"]\n",
    "        log_dict['history_interval'] = history_interval\n",
    "        log_dict['lr_decay_step'] = lr_decay_step\n",
    "        log_dict['losses'] = losses\n",
    "        log_dict['train_acc_history'] = train_acc_history\n",
    "        log_dict['val_acc_history'] = val_acc_history\n",
    "        log_dict['best_test_accuracy'] = best_test_acc\n",
    "        log_dict['best_test_confusion'] = best_test_confusion\n",
    "        log_dict['best_test_debug'] = best_test_debug\n",
    "        log_dict['last_test_accuracy'] = last_test_acc\n",
    "        log_dict['last_test_confusion'] = last_test_confusion\n",
    "        log_dict['last_test_debug'] = last_test_debug\n",
    "        torch.save(log_dict, path)\n",
    "\n",
    "        script = f'- {r:02d} train accuracy {train_acc:.2f}%, '\\\n",
    "        f'best / last test accuracies {best_test_acc:.2f}% / {last_test_acc:.2f}%'\n",
    "        print()\n",
    "        print(script)\n",
    "        print()\n",
    "    \n",
    "    if draw_result and save_model:\n",
    "        model = model_dict['generator']().to(device)\n",
    "        path = os.path.join(log_path, f'{model_dict[\"name\"]}')\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        temp_result = check_test_accuracy(model, repeat=30)\n",
    "        test_acc, test_confusion, test_debug, score, target = temp_result\n",
    "        draw_roc_curve(score, target)\n",
    "        draw_confusion(test_confusion)\n",
    "        print('\\n' * 2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log path\n",
    "log_path = f'history_temp/{nb_fname}/'\n",
    "\n",
    "pprint.pprint(model_pool)\n",
    "print('\\n' * 3)\n",
    "\n",
    "# train process on model_pool\n",
    "for model_dict in model_pool:\n",
    "    for r in range(n_repeats):\n",
    "        endwith = '' if n_repeats == 1 else f'_r{r:02d}'\n",
    "        path = os.path.join(log_path, f'{model_dict[\"name\"]}_log{endwith}')\n",
    "        if os.path.isfile(path):\n",
    "            log_dict = torch.load(path)\n",
    "            script = f'{log_dict[\"num_params\"]}\\t'\n",
    "            script += f'{log_dict[\"weight_decay\"]:.2e}\\t'\n",
    "            script += f'{log_dict[\"starting_lr\"]:.2e}\\t'\n",
    "            script += f'{len(log_dict[\"losses\"])}\\t'\n",
    "            script += f'{log_dict[\"lr_decay_step\"]}\\t'\n",
    "            script += f'{log_dict[\"train_acc_history\"][-1]:.2f}%\\t'\n",
    "            script += f'{log_dict[\"best_test_accuracy\"]:.2f}% / {log_dict[\"last_test_accuracy\"]:.2f}%'\n",
    "            print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
