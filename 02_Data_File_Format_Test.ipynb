{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Data File Format Test\n",
    "\n",
    "Given the metadata generated in `01_Data_Curation1` notebook, this notebook tests some file formats to read and write the EEG dataset and decides the best one.  \n",
    "The following file formats are tested: `NumPy pickle`, `Feather`, `Parquet`, `Jay`, and `HDF5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load Packages and Configure Notebook Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import glob\n",
    "from openpyxl import load_workbook, Workbook, styles\n",
    "import json\n",
    "\n",
    "import pyedflib\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pprint\n",
    "import warnings\n",
    "import ctypes\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import datatable as dt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# custom package\n",
    "from datasets.cau_eeg_dataset import *\n",
    "from datasets.utils import *\n",
    "from datasets.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.11.0\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load and Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "original_path = r'local/dataset/01_Original_Data_220307'\n",
    "curated_path = r'local/dataset/02_Curated_Data_temp'\n",
    "\n",
    "os.makedirs(curated_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 1390\n",
      "\n",
      "Loaded metadata (first three displayed):\n",
      "[\n",
      "    {\n",
      "        \"edfname\": \"00001809_261018\",\n",
      "        \"dx1\": \"mci_rf\",\n",
      "        \"birth\": 400602,\n",
      "        \"anomaly\": false\n",
      "    },\n",
      "    {\n",
      "        \"edfname\": \"00029426_020817\",\n",
      "        \"dx1\": \"smi\",\n",
      "        \"birth\": 601204,\n",
      "        \"anomaly\": false\n",
      "    },\n",
      "    {\n",
      "        \"edfname\": \"00047327_090718\",\n",
      "        \"dx1\": \"vascular mci\",\n",
      "        \"birth\": 241019,\n",
      "        \"anomaly\": false\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "meta_file = os.path.join(original_path, r'new_DB_list.xlsx')\n",
    "ws = load_workbook(meta_file, data_only=True)['metadata']\n",
    "\n",
    "metadata = []\n",
    "\n",
    "num = 2\n",
    "while True:\n",
    "    m = dict()\n",
    "    m['edfname'] = ws.cell(row=num, column=1).value\n",
    "    m['dx1'] = ws.cell(row=num, column=2).value\n",
    "    m['birth'] = ws.cell(row=num, column=3).value\n",
    "    m['anomaly'] = True if ws.cell(row=num, column=4).value is not None else False\n",
    "    num += 1\n",
    "    \n",
    "    # check whether the row is empty (which is EOF condition)\n",
    "    if m['edfname'] is None:\n",
    "        break\n",
    "    elif m['anomaly']:\n",
    "        continue\n",
    "        \n",
    "    # move the pivot row\n",
    "    metadata.append(m)\n",
    "    \n",
    "print('Size:', len(metadata))\n",
    "print()\n",
    "print('Loaded metadata (first three displayed):')\n",
    "print(json.dumps(metadata[:3], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2820,) 4843.0\n",
      "\n",
      "{'edfname': '00001809_261018',\n",
      " 'dx1': 'mci_rf',\n",
      " 'birth': 400602,\n",
      " 'anomaly': False}\n",
      "\n",
      "{'technician': '',\n",
      " 'recording_additional': '',\n",
      " 'patientname': '',\n",
      " 'patient_additional': '',\n",
      " 'patientcode': '',\n",
      " 'equipment': '',\n",
      " 'admincode': '',\n",
      " 'gender': '',\n",
      " 'startdate': datetime.datetime(2018, 10, 26, 15, 46, 26),\n",
      " 'birthdate': '',\n",
      " 'annotations': []}\n",
      "\n",
      "[{'label': 'Fp1-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'F3-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'C3-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'P3-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'O1-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'Fp2-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'F4-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'C4-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'P4-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'O2-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'F7-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'T3-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'T5-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'F8-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'T4-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'T6-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'FZ-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'CZ-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'PZ-AVG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'EKG',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'},\n",
      " {'label': 'Photic',\n",
      "  'dimension': 'uV',\n",
      "  'sample_rate': 200.0,\n",
      "  'physical_max': 32767.0,\n",
      "  'physical_min': -32768.0,\n",
      "  'digital_max': 32767,\n",
      "  'digital_min': -32768,\n",
      "  'prefilter': '',\n",
      "  'transducer': 'E'}]\n"
     ]
    }
   ],
   "source": [
    "m = metadata[0]\n",
    "edf_file = os.path.join(original_path, m['edfname'] + '.edf')\n",
    "signals, signal_headers, edf_header  = pyedflib.highlevel.read_edf(edf_file)\n",
    "\n",
    "refer_headers = signal_headers\n",
    "\n",
    "print(np.unique(signals.reshape(-1)).shape, signals.max() - signals.min()) # check the signal is discrete\n",
    "print()\n",
    "pprint.pp(m)\n",
    "print()\n",
    "pprint.pp(edf_header)\n",
    "print()\n",
    "pprint.pp(signal_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Arrange and Save the Data\n",
    "\n",
    "1. Remove EDF files with irregular signal header\n",
    "2. Extract EDF signal and save the data after reformatting\n",
    "3. Save all metadata as JSON and XLSX where JSON is for further use and XLSX for examination of human\n",
    "    - `metadata_debug`: Full inclusion metadata for debugging\n",
    "    - `metadata_public`: Metadata without personal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf47974c9814f91ae39cd8b94d29836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The age information is unknown: 00602793_300518\n",
      "- The age information is unknown: 00850537_061014\n",
      "WARNING - calculate_age() generated an unordinary age: 39\n",
      "WARNING - calculate_age() generated an unordinary age: 39\n",
      "Done.\n",
      "\n",
      "Among 1390, 1388 data were saved.\n"
     ]
    }
   ],
   "source": [
    "start_latency_length = 10 * 200  # 10 seconds\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "text = f'Delete ALL files in {curated_path}?'\n",
    "if ctypes.windll.user32.MessageBoxExW(0, text, 'Question', 4) == 6: # Yes\n",
    "    for f in glob.glob(os.path.join(curated_path, '*/*')):\n",
    "        os.remove(f)\n",
    "    for f in glob.glob(os.path.join(curated_path, '*.*')):\n",
    "        os.remove(f)\n",
    "        \n",
    "metadata_debug = []\n",
    "metadata_public = []\n",
    "\n",
    "for m in tqdm(metadata):\n",
    "    # EDF file check\n",
    "    edf_file = os.path.join(original_path, m['edfname'] + '.edf')\n",
    "    signals, signal_headers, edf_header = pyedflib.highlevel.read_edf(edf_file)\n",
    "        \n",
    "    if refer_headers != signal_headers:\n",
    "        print('- Signal header differs from the majority:', m['edfname'])\n",
    "        continue\n",
    "        \n",
    "    # calculate age\n",
    "    age = calculate_age(birth_to_datetime(m['birth']), \n",
    "                        edf_header['startdate'])\n",
    "    \n",
    "    if age is None:\n",
    "        print('- The age information is unknown:', m['edfname'])\n",
    "        continue\n",
    "    \n",
    "    # EDF recoding events\n",
    "    event_file = os.path.join(original_path, m['edfname'] + '.xlsx')\n",
    "    wb = load_workbook(event_file, data_only=True)\n",
    "    ws = wb[wb.sheetnames[0]]\n",
    "    \n",
    "    num = 2\n",
    "    event = [] \n",
    "    \n",
    "    while True:\n",
    "        t = ws.cell(row=num, column=3).value\n",
    "        e = ws.cell(row=num, column=4).value\n",
    "        \n",
    "        if t is None:\n",
    "            break\n",
    "        \n",
    "        t = edf_header['startdate'].strftime('%Y%m%d') + t\n",
    "        t = datetime.datetime.strptime(t, '%Y%m%d %H:%M:%S.%f')\n",
    "        \n",
    "        if num == 2: \n",
    "            startTime = t\n",
    "            \n",
    "        t = int(np.floor((t - startTime).total_seconds() * 200))\n",
    "        event.append((t, e))\n",
    "        num += 1\n",
    "    \n",
    "    # metadata_debug\n",
    "    m2 = {}\n",
    "    m2['serial'] = f'{len(metadata_debug) + 1:05}'\n",
    "    m2['edfname'] = m['edfname']\n",
    "    m2['birth'] = birth_to_datetime(m['birth'])\n",
    "    m2['record'] = edf_header['startdate']\n",
    "    m2['age'] = age\n",
    "    m2['dx1'] = m['dx1']\n",
    "    m2['label'] = MultiLabel.load_from_string(m['dx1'])\n",
    "    m2['event'] = event\n",
    "    metadata_debug.append(m2)\n",
    "    \n",
    "    # metadata_public\n",
    "    m3 = {}\n",
    "    m3['serial'] = m2['serial']\n",
    "    m3['age'] = age\n",
    "    m3['label'] = m2['label']\n",
    "    metadata_public.append(m3)\n",
    "    \n",
    "    # EDF signal\n",
    "    signals = trim_tailing_zeros(signals)        # trim garbage zeros\n",
    "    signals = signals[:, start_latency_length:]  # throw away some signal right after starting recording\n",
    "    signals = signals.astype('int32')\n",
    "    df = pd.DataFrame(data=signals.T, columns=[s_h['label'] for s_h in signal_headers], dtype=np.int32)\n",
    "    \n",
    "    np.save(os.path.join(curated_path, 'signal', m2['serial']), signals)\n",
    "    df.to_feather(os.path.join(curated_path, 'signal', m2['serial'] + '.feather'))\n",
    "    df.to_parquet(os.path.join(curated_path, 'signal', m2['serial']+ '.parquet'))\n",
    "    dt.Frame(df).to_jay(os.path.join(curated_path, 'signal', m2['serial'] + '.jay'))\n",
    "    df.to_hdf(os.path.join(curated_path, 'signal', 'signal.h5') , m2['serial'])\n",
    "\n",
    "    # event\n",
    "    df = pd.DataFrame(data=event, columns=['timing', 'event'])\n",
    "    with open(os.path.join(curated_path, 'event', m2['serial']) + '.json', 'w') as json_file:\n",
    "        json.dump(event, json_file, indent=4, default=serialize_json)    \n",
    "    df.to_feather(os.path.join(curated_path, 'event', m2['serial']) + '.feather')\n",
    "    \n",
    "print('Done.')\n",
    "print()\n",
    "print(f'Among {len(metadata)}, {len(metadata_public)} data were saved.')\n",
    "\n",
    "warnings.filterwarnings(action='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata_public as JSON\n",
    "path = os.path.join(curated_path, 'metadata_public.json')\n",
    "with open(path, 'w') as json_file:\n",
    "    json.dump(metadata_public, json_file, indent=4, default=serialize_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 78,\n",
      " 'label': ['mci', 'mci_amnestic', 'mci_amnestic_rf'],\n",
      " 'serial': '00001'}\n"
     ]
    }
   ],
   "source": [
    "# load metadata_public\n",
    "meta_path = os.path.join(curated_path, 'metadata_public.json')\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label_to_type: ['Normal', 'Non-vascular MCI', 'Non-vascular dementia']\n"
     ]
    }
   ],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        # inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 458 data belonging to Normal\n",
      "- There are 350 data belonging to Non-vascular MCI\n",
      "- There are 233 data belonging to Non-vascular dementia\n"
     ]
    }
   ],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size\t\t: 832\n",
      "Validation data size\t: 104\n",
      "Test data size\t\t: 105\n",
      "\n",
      " --- Recheck --- \n",
      "\n",
      "Train data label distribution\t: [366 280 186] 832\n",
      "Val data label distribution\t: [46 35 23] 104\n",
      "Test data label distribution\t: [46 35 24] 105\n"
     ]
    }
   ],
   "source": [
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Check Signal Loading Time by Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_np(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.npy')\n",
    "        signal = np.load(fname).astype('float32')\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_feather(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.feather')\n",
    "        signal = pd.read_feather(fname).to_numpy().T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_parquet(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.parquet')\n",
    "        signal = pd.read_parquet(fname).to_numpy().T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_jay(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.jay')\n",
    "        signal = dt.fread(fname).to_numpy().T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_h5(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', 'signal.h5')\n",
    "        signal = pd.read_hdf(fname, m['serial']).to_numpy().T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Current PyTorch device:', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    \n",
    "composed = transforms.Compose([\n",
    "    EegRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "    EegDropPhoticChannel(),\n",
    "    EegNormalizePerSignal(),\n",
    "    EegToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NumPy Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 2s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_np(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-3.3522, -3.2946, -3.3152,  ..., -0.1242, -0.1325, -0.1119],\n",
      "        [-4.3566, -4.3079, -4.3404,  ..., -0.1373, -0.1129, -0.1048],\n",
      "        [-1.0906, -1.0153, -1.0906,  ...,  0.0758,  0.0758,  0.0758],\n",
      "        ...,\n",
      "        [-0.9271, -0.9396, -0.9647,  ...,  0.0127,  0.0127,  0.0127],\n",
      "        [-0.0912, -0.0912, -0.1094,  ...,  0.2360,  0.1996,  0.1815],\n",
      "        [ 0.3789,  0.2249,  0.1223,  ...,  0.1325, -0.0626, -0.1858]]),\n",
      " tensor([[ 0.4156,  0.4156,  0.4853,  ..., -0.5610, -0.2820, -0.2123],\n",
      "        [ 0.4940,  0.4940,  0.5827,  ..., -0.2156,  0.0505,  0.2279],\n",
      "        [-0.9845, -0.8280, -0.6714,  ..., -1.1411, -0.9845, -0.6714],\n",
      "        ...,\n",
      "        [ 0.1537,  0.2702,  0.5030,  ...,  1.2016,  1.3180,  1.4345],\n",
      "        [-1.1479, -1.2525, -1.2525,  ...,  0.2113,  0.4204,  0.5249],\n",
      "        [-0.1101, -0.1766, -0.0686,  ...,  1.3192,  1.0616,  1.1197]])]\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 39s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_feather(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.6269, -0.6983, -0.8173,  ...,  1.0148,  0.9910,  0.9672],\n",
      "        [-0.2354, -0.2626, -0.3442,  ..., -0.5074, -0.5618, -0.5618],\n",
      "        [-0.2798, -0.2453, -0.1762,  ...,  0.8943,  0.9288,  0.8598],\n",
      "        ...,\n",
      "        [ 0.0498,  0.0498, -0.0515,  ...,  1.5016,  1.5016,  1.4678],\n",
      "        [ 0.1102,  0.1486,  0.1486,  ...,  1.3389,  1.3389,  1.3773],\n",
      "        [ 1.0724,  1.1464,  1.1570,  ...,  0.3751,  0.3434,  0.3223]]),\n",
      " tensor([[ 0.5909,  0.4657,  0.4657,  ...,  2.6199,  2.6951,  2.7452],\n",
      "        [ 0.5798,  0.3845,  0.5147,  ...,  0.8402,  0.9053,  1.1005],\n",
      "        [ 0.7264,  0.8804,  0.5724,  ..., -0.8139, -0.3518,  0.1103],\n",
      "        ...,\n",
      "        [ 0.1455,  0.1455,  0.0241,  ..., -1.0680, -0.9466, -0.9466],\n",
      "        [-0.8366, -0.8366, -0.8366,  ..., -0.9195, -0.6707, -0.5878],\n",
      "        [ 1.4154,  1.0992,  0.7115,  ..., -1.0431, -0.9512, -0.7982]])]\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 47s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_parquet(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.5593, -0.5781, -0.7854,  ...,  1.8718,  1.7587,  1.9283],\n",
      "        [-0.8838, -0.5368, -1.0573,  ..., -0.7971, -1.1441, -1.1441],\n",
      "        [-0.4284, -1.0146,  2.2681,  ...,  2.7371,  2.6198,  2.5026],\n",
      "        ...,\n",
      "        [ 2.0997,  1.8857,  2.2067,  ...,  1.1366,  1.0296,  1.0296],\n",
      "        [ 0.6551,  0.4951,  0.6551,  ...,  0.3351,  0.3351,  0.6551],\n",
      "        [-0.7340, -0.7025, -0.7025,  ..., -0.2418, -0.2732, -0.1685]]),\n",
      " tensor([[ 0.6115,  0.6545,  0.7405,  ...,  0.9124,  1.0413,  1.2132],\n",
      "        [ 0.5572,  0.6246,  0.7596,  ...,  1.9065,  2.0414,  2.1763],\n",
      "        [ 0.9045,  0.7507,  0.7507,  ...,  1.8275,  1.9813,  1.9813],\n",
      "        ...,\n",
      "        [ 0.6922,  0.6922,  0.5610,  ..., -0.2265, -0.2265, -0.4890],\n",
      "        [ 0.3906,  0.4735,  0.3076,  ...,  0.3906,  0.3906,  0.3906],\n",
      "        [ 0.9183,  0.9948,  0.8200,  ...,  1.0604,  1.1259,  1.1587]])]\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Jay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 14s\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_jay(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-3.6729, -3.6630, -3.6530,  ..., -0.0396, -0.0446, -0.0146],\n",
      "        [-3.1607, -3.3226, -3.3449,  ...,  0.0215,  0.0104,  0.0160],\n",
      "        [ 4.5313,  4.7697,  5.8935,  ...,  0.0362, -0.2022, -0.2703],\n",
      "        ...,\n",
      "        [ 3.7753,  3.9661,  4.0933,  ..., -0.1369, -0.1369, -0.1687],\n",
      "        [ 4.0867,  4.1835,  4.3447,  ..., -0.2027, -0.2995, -0.3317],\n",
      "        [ 0.3593,  0.4749,  0.3908,  ...,  2.4919,  2.8175,  2.7440]]),\n",
      " tensor([[ 0.3633,  0.3633,  0.4955,  ...,  0.0990, -0.2975, -0.0772],\n",
      "        [-0.3637, -0.2852,  0.0287,  ..., -0.4422, -0.1283,  0.0287],\n",
      "        [ 0.6135,  0.4735,  0.3335,  ..., -2.1864, -1.9064, -1.3464],\n",
      "        ...,\n",
      "        [ 0.4991,  0.4991,  0.4991,  ..., -1.9070, -1.6063, -1.6063],\n",
      "        [ 1.2605,  1.1668,  0.7920,  ...,  0.6983,  0.9794,  1.2605],\n",
      "        [ 1.7590,  1.6566,  1.6873,  ..., -0.6785, -0.7604, -0.7707]])]\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 43s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "train_dataset = EegDataset_h5(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)\n",
    "    \n",
    "warnings.filterwarnings(action='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-2.4124, -2.4764, -2.5084,  ...,  0.4368,  0.3087,  0.1807],\n",
      "        [-1.3673, -1.3673, -1.3673,  ..., -0.6582, -0.8608, -0.8608],\n",
      "        [-0.1513,  0.0243,  0.1121,  ..., -0.8538, -0.6782, -0.7660],\n",
      "        ...,\n",
      "        [ 0.5907,  0.1172, -0.5141,  ...,  1.5378,  1.3799,  0.7486],\n",
      "        [ 0.7186,  0.9050,  0.7186,  ...,  0.7186,  0.9050,  0.5321],\n",
      "        [ 0.0874, -0.1146, -0.2528,  ...,  0.7463,  0.7782,  0.6932]]),\n",
      " tensor([[-1.7081, -1.6583, -1.6583,  ...,  0.4017,  0.3685,  0.3519],\n",
      "        [-0.0171,  0.1241, -0.6760,  ..., -0.0642, -0.1113, -0.2054],\n",
      "        [ 0.1934,  0.4106, -0.0962,  ..., -0.2411, -0.0962, -0.0962],\n",
      "        ...,\n",
      "        [-0.2234, -0.1130, -0.2234,  ...,  0.5497,  0.2184, -0.1130],\n",
      "        [ 0.1776,  0.1904,  0.1776,  ...,  0.0237,  0.0109, -0.0148],\n",
      "        [ 4.9668,  5.3080,  4.8256,  ...,  0.0544,  0.1095,  0.1187]])]\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bengb\\anaconda3\\envs\\eeg_analysis\\lib\\site-packages\\tables\\array.py:241: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (oid, self.atom, self.shape, self._v_chunkshape) = self._open_array()\n",
      "C:\\Users\\bengb\\anaconda3\\envs\\eeg_analysis\\lib\\site-packages\\tables\\array.py:241: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (oid, self.atom, self.shape, self._v_chunkshape) = self._open_array()\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Check Event Loading Time by Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_event_json(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'event', m['serial'] + '.json')\n",
    "        with open(fname, 'r') as json_file:\n",
    "            event = json.load(json_file)        \n",
    "        sample = {'signal': np.zeros((20, 100)),\n",
    "                  'event': event,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_event_feather(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'event', m['serial'] + '.feather')\n",
    "        event = pd.read_feather(fname)\n",
    "        sample = {'signal': np.zeros((20, 100)),\n",
    "                  'event': event,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([\n",
    "    EegToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.1 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_event_json(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for k in range(10):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        sample_batched['signal'].to(device)\n",
    "        sample_batched['age'].to(device)\n",
    "        sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "[[0, 'Start Recording'],\n",
      " [0, 'New Montage - Montage 002'],\n",
      " [2674, 'Eyes Open'],\n",
      " [4396, 'Eyes Closed'],\n",
      " [15362, 'artifact'],\n",
      " [16912, 'Eyes Open'],\n",
      " [18844, 'Eyes Closed'],\n",
      " [28556, 'Eyes Open'],\n",
      " [30698, 'Eyes Closed'],\n",
      " [39182, 'Eyes Open'],\n",
      " [40778, 'Eyes Closed'],\n",
      " [49052, 'Photic On - 3.0 Hz'],\n",
      " [51068, 'Photic Off'],\n",
      " [53126, 'Photic On - 6.0 Hz'],\n",
      " [55141, 'Photic Off'],\n",
      " [57200, 'Photic On - 9.0 Hz'],\n",
      " [59216, 'Photic Off'],\n",
      " [61232, 'Photic On - 12.0 Hz'],\n",
      " [63248, 'Photic Off'],\n",
      " [63667, 'Eyes Open'],\n",
      " [65054, 'Eyes Closed'],\n",
      " [65305, 'Photic On - 15.0 Hz'],\n",
      " [67322, 'Photic Off'],\n",
      " [69380, 'Photic On - 18.0 Hz'],\n",
      " [71396, 'Photic Off'],\n",
      " [73412, 'Photic On - 21.0 Hz'],\n",
      " [75428, 'Photic Off'],\n",
      " [77486, 'Photic On - 24.0 Hz'],\n",
      " [77764, 'Move'],\n",
      " [79502, 'Photic Off'],\n",
      " [79838, 'Eyes Open'],\n",
      " [81098, 'Eyes Closed'],\n",
      " [81564, 'Photic On - 27.0 Hz'],\n",
      " [83588, 'Photic Off'],\n",
      " [85604, 'Photic On - 30.0 Hz'],\n",
      " [87620, 'Photic Off'],\n",
      " [100600, 'Move'],\n",
      " [113326, 'Eyes Open'],\n",
      " [116224, 'Eyes Closed'],\n",
      " [129748, 'Eyes Open'],\n",
      " [133396, 'Eyes Closed'],\n",
      " [151360, 'Move'],\n",
      " [153918, 'Eyes Closed'],\n",
      " [164598, 'Move'],\n",
      " [180200, 'Paused']]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 6s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_event_feather(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for k in range(10):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        sample_batched['signal'].to(device)\n",
    "        sample_batched['age'].to(device)\n",
    "        sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "    timing                      event\n",
      "0        0            Start Recording\n",
      "1        0  New Montage - Montage 002\n",
      "2     2674                  Eyes Open\n",
      "3     4396                Eyes Closed\n",
      "4    15362                   artifact\n",
      "5    16912                  Eyes Open\n",
      "6    18844                Eyes Closed\n",
      "7    28556                  Eyes Open\n",
      "8    30698                Eyes Closed\n",
      "9    39182                  Eyes Open\n",
      "10   40778                Eyes Closed\n",
      "11   49052         Photic On - 3.0 Hz\n",
      "12   51068                 Photic Off\n",
      "13   53126         Photic On - 6.0 Hz\n",
      "14   55141                 Photic Off\n",
      "15   57200         Photic On - 9.0 Hz\n",
      "16   59216                 Photic Off\n",
      "17   61232        Photic On - 12.0 Hz\n",
      "18   63248                 Photic Off\n",
      "19   63667                  Eyes Open\n",
      "20   65054                Eyes Closed\n",
      "21   65305        Photic On - 15.0 Hz\n",
      "22   67322                 Photic Off\n",
      "23   69380        Photic On - 18.0 Hz\n",
      "24   71396                 Photic Off\n",
      "25   73412        Photic On - 21.0 Hz\n",
      "26   75428                 Photic Off\n",
      "27   77486        Photic On - 24.0 Hz\n",
      "28   77764                       Move\n",
      "29   79502                 Photic Off\n",
      "30   79838                  Eyes Open\n",
      "31   81098                Eyes Closed\n",
      "32   81564        Photic On - 27.0 Hz\n",
      "33   83588                 Photic Off\n",
      "34   85604        Photic On - 30.0 Hz\n",
      "35   87620                 Photic Off\n",
      "36  100600                       Move\n",
      "37  113326                  Eyes Open\n",
      "38  116224                Eyes Closed\n",
      "39  129748                  Eyes Open\n",
      "40  133396                Eyes Closed\n",
      "41  151360                       Move\n",
      "42  153918                Eyes Closed\n",
      "43  164598                       Move\n",
      "44  180200                     Paused\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "279px",
    "width": "378px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "356px",
    "left": "1090px",
    "top": "213px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
