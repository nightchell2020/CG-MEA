{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Data File Format Test\n",
    "\n",
    "Given the metadata generated in `01_Data_Curation1` notebook, this notebook tests some file formats to read and write the EEG dataset and decides the best one.  \n",
    "The following file formats are tested: `NumPy pickle`, `Feather`, `Parquet`, `Jay`, and `HDF5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load Packages and Configure Notebook Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import glob\n",
    "from openpyxl import load_workbook, Workbook, styles\n",
    "import json\n",
    "\n",
    "import pyedflib\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import pprint\n",
    "import warnings\n",
    "import ctypes\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import datatable as dt\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# custom package\n",
    "from datasets.cau_eeg_dataset import *\n",
    "from datasets.utils import *\n",
    "from datasets.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Load and Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "original_path = r'local/dataset/01_Original_Data_220307'\n",
    "curated_path = r'local/dataset/02_Curated_Data_temp'\n",
    "\n",
    "os.makedirs(curated_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file = os.path.join(original_path, r'new_DB_list.xlsx')\n",
    "ws = load_workbook(meta_file, data_only=True)['metadata']\n",
    "\n",
    "metadata = []\n",
    "\n",
    "num = 2\n",
    "while True:\n",
    "    m = dict()\n",
    "    m['edfname'] = ws.cell(row=num, column=1).value\n",
    "    m['dx1'] = ws.cell(row=num, column=2).value\n",
    "    m['birth'] = ws.cell(row=num, column=3).value\n",
    "    m['anomaly'] = True if ws.cell(row=num, column=4).value is not None else False\n",
    "    num += 1\n",
    "    \n",
    "    # check whether the row is empty (which is EOF condition)\n",
    "    if m['edfname'] is None:\n",
    "        break\n",
    "    elif m['anomaly']:\n",
    "        continue\n",
    "        \n",
    "    # move the pivot row\n",
    "    metadata.append(m)\n",
    "    \n",
    "print('Size:', len(metadata))\n",
    "print()\n",
    "print('Loaded metadata (first three displayed):')\n",
    "print(json.dumps(metadata[:3], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = metadata[0]\n",
    "edf_file = os.path.join(original_path, m['edfname'] + '.edf')\n",
    "signals, signal_headers, edf_header  = pyedflib.highlevel.read_edf(edf_file)\n",
    "\n",
    "refer_headers = signal_headers\n",
    "\n",
    "print(np.unique(signals.reshape(-1)).shape, signals.max() - signals.min()) # check the signal is discrete\n",
    "print()\n",
    "pprint.pp(m)\n",
    "print()\n",
    "pprint.pp(edf_header)\n",
    "print()\n",
    "pprint.pp(signal_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Arrange and Save the Data\n",
    "\n",
    "1. Remove EDF files with irregular signal header\n",
    "2. Extract EDF signal and save the data after reformatting\n",
    "3. Save all metadata as JSON and XLSX where JSON is for further use and XLSX for examination of human\n",
    "    - `metadata_debug`: Full inclusion metadata for debugging\n",
    "    - `metadata_public`: Metadata without personal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latency_length = 10 * 200  # 10 seconds\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "text = f'Delete ALL files in {curated_path}?'\n",
    "if ctypes.windll.user32.MessageBoxExW(0, text, 'Question', 4) == 6: # Yes\n",
    "    for f in glob.glob(os.path.join(curated_path, '*/*')):\n",
    "        os.remove(f)\n",
    "    for f in glob.glob(os.path.join(curated_path, '*.*')):\n",
    "        os.remove(f)\n",
    "\n",
    "os.makedirs(os.path.join(curated_path, 'signal'), exist_ok=True)\n",
    "os.makedirs(os.path.join(curated_path, 'event'), exist_ok=True)\n",
    "\n",
    "metadata_debug = []\n",
    "metadata_public = []\n",
    "fh5 = h5py.File(os.path.join(curated_path, 'signal', 'signal.h5'), 'w', \n",
    "                rdcc_nbytes =(1024**2)*15, rdcc_nslots=1e6)\n",
    "\n",
    "for m in tqdm(metadata):\n",
    "    # EDF file check\n",
    "    edf_file = os.path.join(original_path, m['edfname'] + '.edf')\n",
    "    signals, signal_headers, edf_header = pyedflib.highlevel.read_edf(edf_file)\n",
    "        \n",
    "    if refer_headers != signal_headers:\n",
    "        print('- Signal header differs from the majority:', m['edfname'])\n",
    "        continue\n",
    "        \n",
    "    # calculate age\n",
    "    age = calculate_age(birth_to_datetime(m['birth']), \n",
    "                        edf_header['startdate'])\n",
    "    \n",
    "    if age is None:\n",
    "        print('- The age information is unknown:', m['edfname'])\n",
    "        continue\n",
    "    \n",
    "    # EDF recoding events\n",
    "    event_file = os.path.join(original_path, m['edfname'] + '.xlsx')\n",
    "    wb = load_workbook(event_file, data_only=True)\n",
    "    ws = wb[wb.sheetnames[0]]\n",
    "    \n",
    "    num = 2\n",
    "    event = [] \n",
    "    \n",
    "    while True:\n",
    "        t = ws.cell(row=num, column=3).value\n",
    "        e = ws.cell(row=num, column=4).value\n",
    "        \n",
    "        if t is None:\n",
    "            break\n",
    "        \n",
    "        t = edf_header['startdate'].strftime('%Y%m%d') + t\n",
    "        t = datetime.datetime.strptime(t, '%Y%m%d %H:%M:%S.%f')\n",
    "        \n",
    "        if num == 2: \n",
    "            startTime = t\n",
    "            \n",
    "        t = int(np.floor((t - startTime).total_seconds() * 200))\n",
    "        event.append((t, e))\n",
    "        num += 1\n",
    "    \n",
    "    # metadata_debug\n",
    "    m2 = {}\n",
    "    m2['serial'] = f'{len(metadata_debug) + 1:05}'\n",
    "    m2['edfname'] = m['edfname']\n",
    "    m2['birth'] = birth_to_datetime(m['birth'])\n",
    "    m2['record'] = edf_header['startdate']\n",
    "    m2['age'] = age\n",
    "    m2['dx1'] = m['dx1']\n",
    "    m2['label'] = MultiLabel.load_from_string(m['dx1'])\n",
    "    m2['event'] = event\n",
    "    metadata_debug.append(m2)\n",
    "    \n",
    "    # metadata_public\n",
    "    m3 = {}\n",
    "    m3['serial'] = m2['serial']\n",
    "    m3['age'] = age\n",
    "    m3['label'] = m2['label']\n",
    "    metadata_public.append(m3)\n",
    "    \n",
    "    # EDF signal\n",
    "    signals = trim_tailing_zeros(signals)        # trim garbage zeros\n",
    "    signals = signals[:, start_latency_length:]  # throw away some signal right after starting recording\n",
    "    signals = signals.astype('int32')\n",
    "    df = pd.DataFrame(data=signals.T, columns=[s_h['label'] for s_h in signal_headers], dtype=np.int32)\n",
    "    \n",
    "    # numpy pickle\n",
    "    np.save(os.path.join(curated_path, 'signal', m2['serial']), signals)\n",
    "    \n",
    "    # numpy memmap\n",
    "    fp = np.memmap(os.path.join(curated_path, 'signal', m2['serial'] + '.dat'), \n",
    "                   dtype='int32', mode='w+', shape=signals.shape)\n",
    "    fp[:] = signals[:]\n",
    "    fp.flush()\n",
    "    \n",
    "    # feather\n",
    "    df.to_feather(os.path.join(curated_path, 'signal', m2['serial'] + '.feather'))\n",
    "    \n",
    "    # parquet\n",
    "    df.to_parquet(os.path.join(curated_path, 'signal', m2['serial']+ '.parquet'))\n",
    "    \n",
    "    # jay\n",
    "    dt.Frame(df).to_jay(os.path.join(curated_path, 'signal', m2['serial'] + '.jay'))\n",
    "    \n",
    "    # hdf5\n",
    "    # df.to_hdf(os.path.join(curated_path, 'signal', 'signal.h5') , m2['serial'])\n",
    "    fh5.create_dataset(m2['serial'], shape=signals.shape, dtype='int32', chunks=True, data=signals)\n",
    "    # rdcc_nbytes =(1024**2)*15, rdcc_nslots=1e6\n",
    "    \n",
    "    # event\n",
    "    df = pd.DataFrame(data=event, columns=['timing', 'event'])\n",
    "    with open(os.path.join(curated_path, 'event', m2['serial']) + '.json', 'w') as json_file:\n",
    "        json.dump(event, json_file, indent=4, default=serialize_json)    \n",
    "    df.to_feather(os.path.join(curated_path, 'event', m2['serial']) + '.feather')\n",
    "    \n",
    "print('Done.')\n",
    "print()\n",
    "print(f'Among {len(metadata)}, {len(metadata_public)} data were saved.')\n",
    "\n",
    "warnings.filterwarnings(action='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata_public as JSON\n",
    "path = os.path.join(curated_path, 'metadata_public.json')\n",
    "with open(path, 'w') as json_file:\n",
    "    json.dump(metadata_public, json_file, indent=4, default=serialize_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata_public\n",
    "meta_path = os.path.join(curated_path, 'metadata_public.json')\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        # inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Check Signal Loading Time by Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_np(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.npy')\n",
    "        signal = np.load(fname)\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_np_memmap(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.dat')\n",
    "        signal = np.memmap(fname, dtype='int32', mode='r').reshape(21, -1)\n",
    "        # signal = np.load(fname).astype('float32')\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_feather(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.feather')\n",
    "        # signal = pd.read_feather(fname).to_numpy().T\n",
    "        # signal = pd.read_feather(fname).values.T\n",
    "        signal = feather.read_feather(fname).values.T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_parquet(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.parquet')\n",
    "        # signal = pd.read_parquet(fname).to_numpy().T\n",
    "        signal = pd.read_parquet(fname).values.T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_jay(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'signal', m['serial'] + '.jay')\n",
    "        signal = dt.fread(fname).to_numpy().T\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_h5(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.f_handle = h5py.File(os.path.join(self.root_dir, 'signal', 'signal.h5'), 'r')\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        #fname = os.path.join(self.root_dir, 'signal', 'signal.h5')\n",
    "        # signal = pd.read_hdf(fname, m['serial']).to_numpy().T\n",
    "        signal = self.f_handle[m['serial']][:]\n",
    "        sample = {'signal': signal,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'event': [],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print('Current PyTorch device:', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    \n",
    "composed = transforms.Compose([\n",
    "    EegRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "    EegDropPhoticChannel(),\n",
    "    EegToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NumPy Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_np(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NumPy Memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_np_memmap(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_feather(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_parquet(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Jay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_jay(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_h5(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['signal'][0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Check Event Loading Time by Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_event_json(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'event', m['serial'] + '.json')\n",
    "        with open(fname, 'r') as json_file:\n",
    "            event = json.load(json_file)        \n",
    "        sample = {'signal': np.zeros((20, 100)),\n",
    "                  'event': event,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EegDataset_event_feather(Dataset):\n",
    "    \"\"\"EEG Dataset Class for PyTorch.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root path to the EDF data files.\n",
    "        metadata (list of dict): List of dictionary with metadata.\n",
    "        transform (callable, optional): Optional transform to be applied on each data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, metadata, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        m = self.metadata[idx]\n",
    "        fname = os.path.join(self.root_dir, 'event', m['serial'] + '.feather')\n",
    "        event = pd.read_feather(fname)\n",
    "        sample = {'signal': np.zeros((20, 100)),\n",
    "                  'event': event,\n",
    "                  'age': m['age'],\n",
    "                  'class_label': m['class_label'],\n",
    "                  'metadata': m}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([\n",
    "    EegToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_event_json(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for k in range(10):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        sample_batched['signal'].to(device)\n",
    "        sample_batched['age'].to(device)\n",
    "        sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = EegDataset_event_feather(curated_path, metadata_train, composed)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=False, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for k in range(10):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        sample_batched['signal'].to(device)\n",
    "        sample_batched['age'].to(device)\n",
    "        sample_batched['class_label'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_dataset[0]['signal'])\n",
    "pprint.pprint(train_dataset[0]['event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "279px",
    "width": "378px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "356px",
    "left": "1090px",
    "top": "213px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
