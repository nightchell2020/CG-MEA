{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Networks\n",
    "\n",
    "- Three-way SoftMax classifier of normal, non-vascular MCI, and non-vascular dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Load Packages and Get Ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from itertools import cycle\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "\n",
    "# custom package\n",
    "from utils.eeg_dataset import *\n",
    "from models import *\n",
    "from utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook name\n",
    "def get_notebook_name():\n",
    "    import ipynbname\n",
    "    return ipynbname.name()\n",
    "nb_fname = get_notebook_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n",
    "#  'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n",
    "#  'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n",
    "#  'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', \n",
    "#  'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic' # for Hangul in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Set up the Dataset and the PyTorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_data = {}\n",
    "cfg_data['dataset'] = 'CAUHS'\n",
    "cfg_data['vascular'] = 'X'\n",
    "cfg_data['segment'] = 'no' # 'train', 'all'\n",
    "cfg_data['seed'] = 0\n",
    "cfg_data['crop_length'] = 200 * 10 # 10 seconds\n",
    "cfg_data['input_norm'] = 'dataset' # 'datatset', 'datapoint', 'no'\n",
    "cfg_data['EKG'] = 'O'\n",
    "cfg_data['photic'] = 'X'\n",
    "cfg_data['awgn'] = 5e-2\n",
    "cfg_data['minibatch'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "data_path = r'dataset/02_Curated_Data/'\n",
    "meta_path = os.path.join(data_path, 'metadata_debug.json')\n",
    "\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "# pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the Data According to the Target Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider only non-vascular symptoms\n",
    "if cfg_data['vascular'] == 'X': \n",
    "    diagnosis_filter = [\n",
    "        # Normal\n",
    "        {'type': 'Normal',\n",
    "         'include': ['normal'], \n",
    "         'exclude': []},\n",
    "        # Non-vascular MCI\n",
    "        {'type': 'Non-vascular MCI',\n",
    "         'include': ['mci'], \n",
    "         'exclude': ['mci_vascular']},\n",
    "        # Non-vascular dementia\n",
    "        {'type': 'Non-vascular dementia',\n",
    "         'include': ['dementia'], \n",
    "         'exclude': ['vd']},\n",
    "    ]\n",
    "# consider all cases\n",
    "elif cfg_data['vascular'] == 'O':\n",
    "    diagnosis_filter = [\n",
    "        # Normal\n",
    "        {'type': 'Normal',\n",
    "         'include': ['normal'], \n",
    "         'exclude': []},\n",
    "        # Non-vascular MCI\n",
    "        {'type': 'Non-vascular MCI',\n",
    "         'include': ['mci'], \n",
    "         'exclude': []},\n",
    "        # Non-vascular dementia\n",
    "        {'type': 'Non-vascular dementia',\n",
    "         'include': ['dementia'], \n",
    "         'exclude': []},\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(f\"cfg_data['vascular'] have to be set to one of ['O', 'X']\")\n",
    "\n",
    "    \n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        # inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "\n",
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random.seed(cfg_data['seed'])\n",
    "\n",
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())\n",
    "\n",
    "# random seed\n",
    "random.seed()\n",
    "\n",
    "# print([m['serial']  for m in metadata_train[:15]])\n",
    "# print([m['serial']  for m in metadata_val[:15]])\n",
    "# print([m['serial']  for m in metadata_test[:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose the dataset transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for m in metadata_train:\n",
    "    ages.append(m['age'])\n",
    "\n",
    "ages = np.array(ages)\n",
    "age_mean = np.mean(ages)\n",
    "age_std = np.std(ages)\n",
    "\n",
    "print('Age mean and standard deviation:')\n",
    "print(age_mean, age_std)\n",
    "\n",
    "cfg_data['age_mean'] = age_mean\n",
    "cfg_data['age_std'] = age_std\n",
    "\n",
    "composed_train = [EEGRandomCrop(crop_length=cfg_data['crop_length']), \n",
    "                  EEGNormalizeAge(mean=cfg_data['age_mean'], std=cfg_data['age_std'])]\n",
    "composed_test = [EEGRandomCrop(crop_length=cfg_data['crop_length']), \n",
    "                 EEGNormalizeAge(mean=cfg_data['age_mean'], std=cfg_data['age_std'])]\n",
    "longer_composed_test = [EEGRandomCrop(crop_length=cfg_data['crop_length'] * 10), \n",
    "                        EEGNormalizeAge(mean=cfg_data['age_mean'], std=cfg_data['age_std'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg_data['input_norm'] == 'dataset':\n",
    "    # composed = transforms.Compose([EEGRandomCrop(crop_length=cfg_data['crop_length'])])\n",
    "    # train_dataset = EEGDataset(data_path, metadata_train, composed)\n",
    "\n",
    "    # signal_means = []\n",
    "    # signal_stds = []\n",
    "\n",
    "    # for i in range(10):\n",
    "    #     for d in train_dataset:\n",
    "    #         signal_means.append(d['signal'].mean(axis=1, keepdims=True))\n",
    "    #         signal_stds.append(d['signal'].std(axis=1, keepdims=True))\n",
    "\n",
    "    # signal_mean = np.mean(np.array(signal_means), axis=0)\n",
    "    # signal_std = np.mean(np.array(signal_stds), axis=0)\n",
    "\n",
    "    # print('Mean and standard deviation for signal:')\n",
    "    # print(signal_means, '\\n\\n', signal_stds)\n",
    "\n",
    "    # SPEED-UP\n",
    "    signal_mean = np.array([[ 0.1127599 ], [ 0.06298441], [-0.02522413], [ 0.00508518], \n",
    "                             [ 0.12026667], [-0.19987741], [-0.00516898], [ 0.00239212], \n",
    "                             [-0.02861219], [-0.02973673], [-0.02515898], [-0.00060568], \n",
    "                             [ 0.04921601], [-0.00562142], [-0.04888308], [-0.0438447 ], \n",
    "                             [ 0.07532331], [-0.01890181], [-0.044876  ], [-0.00365138], [-0.01564376]])\n",
    "    signal_std = np.array([[46.09896  ], [20.50783  ], [11.196733 ], [11.236944 ], [15.070532 ], \n",
    "                            [47.664406 ], [19.32747  ], [10.106162 ], [11.314243 ], [15.065008 ],\n",
    "                            [20.478817 ], [13.86243  ], [13.2378435], [21.554531 ], [16.875841 ],\n",
    "                            [13.989367 ], [19.789454 ], [10.839711 ], [11.179158 ], [94.12114  ], [65.64865  ]])\n",
    "    \n",
    "    cfg_data['signal_mean'] = signal_mean\n",
    "    cfg_data['signal_std'] = signal_std\n",
    "    \n",
    "    composed_train += [EEGNormalizeMeanStd(mean=cfg_data['signal_mean'], \n",
    "                                           std=cfg_data['signal_std'])]\n",
    "    composed_test += [EEGNormalizeMeanStd(mean=cfg_data['signal_mean'], \n",
    "                                          std=cfg_data['signal_std'])]\n",
    "    longer_composed_test += [EEGNormalizeMeanStd(mean=cfg_data['signal_mean'], \n",
    "                                                 std=cfg_data['signal_std'])]\n",
    "elif cfg_data['input_norm'] == 'datapoint':\n",
    "    composed_train += [EEGNormalizePerSignal()]\n",
    "    composed_test += [EEGNormalizePerSignal()]\n",
    "    longer_composed_test += [EEGNormalizePerSignal()]\n",
    "elif cfg_data['input_norm'] == 'no':\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(f\"cfg_data['input_norm'] have to be set to one of ['dataset', 'datapoint', 'no']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg_data['EKG'] == 'O':\n",
    "    pass\n",
    "elif cfg_data['EKG'] == 'X':\n",
    "    composed_train += [EEGDropEKGChannel()]\n",
    "    composed_test += [EEGDropEKGChannel()]\n",
    "    longer_composed_test += [EEGDropEKGChannel()]\n",
    "else:\n",
    "    raise ValueError(f\"cfg_data['EKG'] have to be set to one of ['O', 'X']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg_data['photic'] == 'O':\n",
    "    pass\n",
    "elif cfg_data['photic'] == 'X':\n",
    "    composed_train += [EEGDropPhoticChannel()]\n",
    "    composed_test += [EEGDropPhoticChannel()]\n",
    "    longer_composed_test += [EEGDropPhoticChannel()]\n",
    "else:\n",
    "    raise ValueError(f\"cfg_data['photic'] have to be set to one of ['O', 'X']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg_data['awgn'] is None:\n",
    "    pass\n",
    "elif cfg_data['awgn'] > 0.0:\n",
    "    composed_train += [EEGAddGaussianNoise(mean=0.0, std=cfg_data['awgn'])]\n",
    "else:\n",
    "    raise ValueError(f\"cfg_data['awgn'] have to be None or a positive floating point number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_train += [EEGToTensor()]\n",
    "composed_test += [EEGToTensor()]\n",
    "longer_composed_test += [EEGToTensor()]\n",
    "\n",
    "composed_train = transforms.Compose(composed_train)\n",
    "composed_test = transforms.Compose(composed_test)\n",
    "longer_composed_test = transforms.Compose(longer_composed_test)\n",
    "\n",
    "print('composed_train:', composed_train)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "print('composed_test:', composed_test)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "print('longer_composed_test:', longer_composed_test)\n",
    "print('\\n' + '-' * 100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap the splitted data using PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EEGDataset(data_path, metadata_train, composed_train)\n",
    "val_dataset = EEGDataset(data_path, metadata_val, composed_test)\n",
    "test_dataset = EEGDataset(data_path, metadata_test, composed_test)\n",
    "longer_test_dataset = EEGDataset(data_path, metadata_test, longer_composed_test)\n",
    "\n",
    "print(train_dataset[0]['signal'].shape)\n",
    "print(train_dataset[0])\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "print(val_dataset[0]['signal'].shape)\n",
    "print(val_dataset[0])\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "print(test_dataset[0]['signal'].shape)\n",
    "print(test_dataset[0])\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "print(longer_test_dataset[0]['signal'].shape)\n",
    "print(longer_test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation, test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=cfg_data['minibatch'], \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=cfg_data['minibatch'], \n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        num_workers=num_workers, \n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=eeg_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=cfg_data['minibatch'], \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory,\n",
    "                         collate_fn=eeg_collate_fn)\n",
    "\n",
    "longer_test_loader = DataLoader(longer_test_dataset, \n",
    "                                batch_size=cfg_data['minibatch'] // 2, # memory capacity\n",
    "                                shuffle=False, \n",
    "                                drop_last=False,\n",
    "                                num_workers=num_workers, \n",
    "                                pin_memory=pin_memory,\n",
    "                                collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(i_batch, \n",
    "          sample_batched['signal'].shape, \n",
    "          sample_batched['age'].shape, \n",
    "          sample_batched['class_label'].shape, \n",
    "          len(sample_batched['metadata']))\n",
    "    \n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Define Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def calculate_final_shape(model):\n",
    "    x = torch.zeros_like(sample_batched['signal']).to(device)\n",
    "    model(x, age=sample_batched['age'].to(device))\n",
    "    return model.get_final_shape()\n",
    "\n",
    "\n",
    "def visualize_network_tensorboard(model, name):\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter('runs/' + nb_fname + '_' + name)\n",
    "\n",
    "    for batch_i, sample_batched in enumerate(train_loader):\n",
    "        # pull up the batch data\n",
    "        x = sample_batched['signal'].to(device)\n",
    "        age = sample_batched['age'].to(device)\n",
    "        target = sample_batched['class_label'].to(device)\n",
    "\n",
    "        # apply model on whole batch directly on device\n",
    "        writer.add_graph(model, (x, age))\n",
    "        output = model(x, age, print_shape=True)\n",
    "        break\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_common_model = {'in_channels': train_dataset[0]['signal'].shape[0], \n",
    "                    'out_dims': len(class_label_to_type)}\n",
    "model_pool = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D Tiny CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-Tiny-CNN'\n",
    "cfg_model['generator'] = TinyCNN1D\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M7 model (fc-age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = 'M7'\n",
    "cfg_model['generator'] = M7\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 256\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M7 model (conv-age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = 'M7'\n",
    "cfg_model['generator'] = M7\n",
    "cfg_model['use_age'] = 'conv'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 256\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M7 model (no-age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = 'M7'\n",
    "cfg_model['generator'] = M7\n",
    "cfg_model['use_age'] = None\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 256\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D ResNet model (fc-age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNet-29'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BottleneckBlock1D\n",
    "cfg_model['conv_layers'] = [2, 2, 2, 2]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D ResNet model (conv-age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNet-29'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BottleneckBlock1D\n",
    "cfg_model['conv_layers'] = [2, 2, 2, 2]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'conv'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D ResNet model (no-age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNet-29'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BottleneckBlock1D\n",
    "cfg_model['conv_layers'] = [2, 2, 2, 2]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = None\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper 1D ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNet-53'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BottleneckBlock1D\n",
    "cfg_model['conv_layers'] = [3, 4, 6, 3]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallower 1D ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNet-21'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BasicBlock1D\n",
    "cfg_model['conv_layers'] = [2, 2, 2, 2]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiny 1D ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNet-13'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BasicBlock1D\n",
    "cfg_model['conv_layers'] = [1, 1, 1, 1]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Dilated 1D ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-Multi-Dilated-ResNet-53'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = MultiBottleneckBlock1D\n",
    "cfg_model['conv_layers'] = [3, 4, 6, 3]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 32\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D ResNeXt-53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-ResNeXt-53'\n",
    "cfg_model['generator'] = ResNet1D\n",
    "cfg_model['block'] = BottleneckBlock1D\n",
    "cfg_model['conv_layers'] = [3, 4, 6, 3]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['groups'] = 32\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "    \n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D ResNet-20 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '2D-ResNet-20' # resnet-18 + two more fc layer\n",
    "cfg_model['generator'] = ResNet2D\n",
    "cfg_model['block'] = BasicBlock2D\n",
    "cfg_model['conv_layers'] = [2, 2, 2, 2]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['n_fft'] = 100\n",
    "cfg_model['complex_mode'] = 'as_real' # 'power', 'remove'\n",
    "cfg_model['hop_length'] = cfg_model['n_fft'] // 2\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D ResNet-52 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '2D-ResNet-52' # resnet-18 + two more fc layer\n",
    "cfg_model['generator'] = ResNet2D\n",
    "cfg_model['block'] = Bottleneck2D\n",
    "cfg_model['conv_layers'] = [3, 4, 6, 3]\n",
    "cfg_model['fc_stages'] = 3\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 64\n",
    "cfg_model['n_fft'] = 100\n",
    "cfg_model['complex_mode'] = 'as_real' # 'power', 'remove'\n",
    "cfg_model['hop_length'] = cfg_model['n_fft'] // 2\n",
    "cfg_model['LR'] = None\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "cfg_model['num_params'] = count_parameters(model)\n",
    "cfg_model['final_shape'] = calculate_final_shape(model)\n",
    "\n",
    "print(f'- The Number of parameters of the model: {cfg_model[\"num_params\"]:,}')\n",
    "print('- Tensor shape right before FC stage:', cfg_model[\"final_shape\"])\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg_model in model_pool:\n",
    "    pprint.pp(cfg_model, width=150)\n",
    "    print('\\n' + '-' * 100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Some useful functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multistep(model, loader, optimizer, scheduler, config, steps):\n",
    "    model.train()\n",
    "        \n",
    "    i = 0\n",
    "    cumu_loss = 0\n",
    "    correct, total = (0, 0)\n",
    "    \n",
    "    while True:\n",
    "        for sample_batched in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # load the mini-batched data\n",
    "            x = sample_batched['signal'].to(device)\n",
    "            age = sample_batched['age'].to(device)\n",
    "            y = sample_batched['class_label'].to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            output = model(x, age)\n",
    "            \n",
    "            # loss function\n",
    "            if config['criterion'] == 'cross-entropy':\n",
    "                s = F.log_softmax(output, dim=1)\n",
    "                loss = F.nll_loss(s, y)\n",
    "            elif config['criterion'] == 'multi-bce':\n",
    "                y_oh = F.one_hot(y, num_classes=len(class_label_to_type))\n",
    "                s = torch.sigmoid(output)\n",
    "                loss = F.binary_cross_entropy_with_logits(output, y_oh.float())\n",
    "\n",
    "            # backward and update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # train accuracy\n",
    "            pred = s.argmax(dim=-1)\n",
    "            correct += pred.squeeze().eq(y).sum().item()\n",
    "            total += pred.shape[0]\n",
    "            cumu_loss += loss.item()\n",
    "            \n",
    "            i += 1\n",
    "            if steps <= i: break\n",
    "        if steps <= i: break\n",
    "            \n",
    "    train_acc = 100.0 * correct / total\n",
    "    avg_loss = cumu_loss / steps\n",
    "    \n",
    "    return (avg_loss, train_acc)\n",
    "\n",
    "\n",
    "def train_mixup_multistep(model, loader, optimizer, scheduler, config, steps):\n",
    "    model.train()\n",
    "        \n",
    "    i = 0\n",
    "    cumu_loss = 0\n",
    "    correct, total = (0, 0)\n",
    "    \n",
    "    while True:\n",
    "        for sample_batched in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # load and mixup the mini-batched data\n",
    "            x1 = sample_batched['signal'].to(device)\n",
    "            age1 = sample_batched['age'].to(device)\n",
    "            y1 = sample_batched['class_label'].to(device)\n",
    "\n",
    "            index = torch.randperm(x1.shape[0]).cuda()                \n",
    "            x2 = x1[index]\n",
    "            age2 = age1[index]\n",
    "            y2 = y1[index]\n",
    "            \n",
    "            mixup_alpha = config['mixup']\n",
    "            lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "            x = lam * x1 + (1.0 - lam) * x2\n",
    "            age = lam * age1 + (1.0 - lam) * age2\n",
    "\n",
    "            # forward pass\n",
    "            output = model(x, age)\n",
    "            \n",
    "            # loss function\n",
    "            if config['criterion'] == 'cross-entropy':\n",
    "                s = F.log_softmax(output, dim=1)\n",
    "                loss1 = F.nll_loss(s, y1)\n",
    "                loss2 = F.nll_loss(s, y2)\n",
    "                loss = lam * loss1 + (1 - lam) * loss2\n",
    "            elif config['criterion'] == 'multi-bce':\n",
    "                y1_oh = F.one_hot(y1, num_classes=len(class_label_to_type))\n",
    "                y2_oh = F.one_hot(y2, num_classes=len(class_label_to_type))\n",
    "                y_oh = lam * y1_oh + (1.0 - lam) * y2_oh\n",
    "                s = torch.sigmoid(output)\n",
    "                loss = F.binary_cross_entropy_with_logits(output, y_oh)\n",
    "\n",
    "            # backward and update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # train accuracy\n",
    "            pred = s.argmax(dim=-1)\n",
    "            correct1 = pred.squeeze().eq(y1).sum().item()\n",
    "            correct2 = pred.squeeze().eq(y2).sum().item()\n",
    "            correct += lam * correct1 + (1.0 - lam) * correct2\n",
    "            total += pred.shape[0]\n",
    "            cumu_loss += loss.item()\n",
    "            \n",
    "            i += 1\n",
    "            if steps <= i: break\n",
    "        if steps <= i: break\n",
    "            \n",
    "    train_acc = 100.0 * correct / total\n",
    "    avg_loss = cumu_loss / steps\n",
    "    \n",
    "    return (avg_loss, train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader, config, repeat=1):\n",
    "    model.eval()\n",
    "    \n",
    "    # for accuracy\n",
    "    correct, total = (0, 0) \n",
    "    \n",
    "    # for confusion matrix\n",
    "    C = len(class_label_to_type)\n",
    "    confusion_matrix = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    # for debug table\n",
    "    debug_table = {data['metadata']['serial']: {'GT': data['class_label'].item(), \n",
    "                                                'Acc': 0, \n",
    "                                                'Pred': [0] * C} for data in loader.dataset}\n",
    "    \n",
    "    # for ROC curve\n",
    "    score = None\n",
    "    target = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in range(repeat):\n",
    "            for sample_batched in loader:\n",
    "                # pull up the data\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                y = sample_batched['class_label'].to(device)\n",
    "\n",
    "                # apply model on whole batch directly on device\n",
    "                output = model(x, age)\n",
    "                \n",
    "                if config['criterion'] == 'cross-entropy':\n",
    "                    s = F.softmax(output, dim=1)\n",
    "                elif config['criterion'] == 'multi-bce':\n",
    "                    s = torch.sigmoid(output)\n",
    "                \n",
    "                # calculate accuracy\n",
    "                pred = s.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(y).sum().item()\n",
    "                total += pred.shape[0]\n",
    "\n",
    "                if score is None:\n",
    "                    score = s.detach().cpu().numpy()\n",
    "                    target = y.detach().cpu().numpy()\n",
    "                else:\n",
    "                    score = np.concatenate((score, s.detach().cpu().numpy()), axis=0)\n",
    "                    target = np.concatenate((target, y.detach().cpu().numpy()), axis=0)\n",
    "\n",
    "                # confusion matrix\n",
    "                confusion_matrix += calculate_confusion_matrix(pred, y)\n",
    "\n",
    "                # debug table\n",
    "                for n in range(pred.shape[0]):\n",
    "                    serial = sample_batched['metadata'][n]['serial']\n",
    "                    debug_table[serial]['edfname'] = sample_batched['metadata'][n]['edfname']\n",
    "                    debug_table[serial]['Pred'][pred[n].item()] += 1\n",
    "                    acc = debug_table[serial]['Pred'][y[n].item()] / np.sum(debug_table[serial]['Pred']) * 100\n",
    "                    debug_table[serial]['Acc'] = f'{acc:>6.02f}%'\n",
    "\n",
    "    # debug table update\n",
    "    debug_table_serial = []\n",
    "    debug_table_edf = []\n",
    "    debug_table_pred = []\n",
    "    debug_table_gt = []\n",
    "    \n",
    "    for key, val in debug_table.items():\n",
    "        debug_table_serial.append(key)\n",
    "        debug_table_edf.append(val['edfname'])\n",
    "        debug_table_pred.append(val['Pred'])\n",
    "        debug_table_gt.append(val['GT'])\n",
    "        \n",
    "    debug_table = (debug_table_serial, debug_table_edf, debug_table_pred, debug_table_gt)\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return (accuracy, confusion_matrix, debug_table, score, target)\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(pred, target):\n",
    "    N = target.shape[0]\n",
    "    C = len(class_label_to_type)\n",
    "    confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        r = target[i]\n",
    "        c = pred[i]\n",
    "        confusion[r, c] += 1\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(confusion, use_wandb=False):\n",
    "    C = len(class_label_to_type)\n",
    "    \n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    plt.rcParams['image.cmap'] = 'jet' # 'nipy_spectral'\n",
    "\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(4.0, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    im = ax.imshow(confusion, alpha=0.8)\n",
    "\n",
    "    ax.set_xticks(np.arange(C))\n",
    "    ax.set_yticks(np.arange(C))\n",
    "    ax.set_xticklabels(class_label_to_type)\n",
    "    ax.set_yticklabels(class_label_to_type)\n",
    "    \n",
    "    for r in range(C):\n",
    "        for c in range(C):\n",
    "            text = ax.text(c, r, confusion[r, c],\n",
    "                           ha=\"center\", va=\"center\", color='k')\n",
    "    \n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # draw\n",
    "    if use_wandb:\n",
    "        wandb.log({'Confusion Matrix (Image)': wandb.Image(plt)})\n",
    "    else: \n",
    "        plt.show()\n",
    "    \n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "def draw_roc_curve(score, target, use_wandb=False):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    \n",
    "    # Binarize the output\n",
    "    n_classes = len(class_label_to_type)\n",
    "    target = label_binarize(target, classes=np.arange(n_classes))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(target[:, i], score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(target.ravel(), score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # draw class-agnostic ROC curve\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.5, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    lw = 1.5\n",
    "    colors = cycle(['limegreen', 'mediumpurple', 'darkorange', \n",
    "                    'dodgerblue', 'lightcoral', 'goldenrod', \n",
    "                    'indigo', 'darkgreen', 'navy', 'brown'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                label='{0} (area = {1:0.2f})'\n",
    "                ''.format(class_label_to_type[i], roc_auc[i]))    \n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Class-Wise ROC Curves')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Plot class-aware ROC curves\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle='-', linewidth=lw)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle='-', linewidth=lw)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Class-Agnostic ROC Curves')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    \n",
    "    # draw\n",
    "    if use_wandb:\n",
    "        wandb.log({'ROC Curve (Image)': wandb.Image(plt)})\n",
    "    else: \n",
    "        plt.show()\n",
    "        \n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "def draw_debug_table(debug_table, use_wandb=False):\n",
    "    (debug_table_serial, debug_table_edf, debug_table_pred, debug_table_gt) = debug_table\n",
    "    \n",
    "    fig = plt.figure(num=1, clear=True, figsize=(20.0, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    total_error, total_count = (0, 0)\n",
    "\n",
    "    for edf in np.unique(debug_table_edf):\n",
    "        indices = [i for i, x in enumerate(debug_table_edf) if x == edf]\n",
    "\n",
    "        err, cnt = (0, 0)\n",
    "        for i in indices:\n",
    "            cnt += sum(debug_table_pred[i])\n",
    "            err += sum(debug_table_pred[i]) - debug_table_pred[i][debug_table_gt[i]]\n",
    "\n",
    "        total_error += err\n",
    "        total_count += cnt\n",
    "\n",
    "        ax.bar(edf, err / cnt, color=['g', 'b', 'r'][debug_table_gt[i]])\n",
    "\n",
    "    ax.set_title(f'Debug Table (Acc. {1.0 - total_error / total_count: .2f}%)', fontsize=18)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", fontsize=9, visible=True)\n",
    "    \n",
    "    if use_wandb:\n",
    "        table = [[serial, edf, pred, gt] for serial, edf, pred, gt in zip(*debug_table)]\n",
    "        table = wandb.Table(data=table, columns=['Serial', 'EDF', 'Prediction', 'Ground-truth'])\n",
    "        wandb.log({'Debug Table': table})\n",
    "        \n",
    "        wandb.log({'Debug Table (Image)': wandb.Image(plt)})\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_search(model, min_log_lr, max_log_lr, trials, config, steps):\n",
    "    learning_rate_record = []\n",
    "    for t in tqdm(range(trials)):\n",
    "        log_lr = np.random.uniform(min_log_lr, max_log_lr)\n",
    "        lr = 10 ** log_lr\n",
    "        \n",
    "        model.reset_weights()\n",
    "        model.train()\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=config[\"weight_decay\"])\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config['lr_decay_step'], gamma=config['lr_decay_gamma'])        \n",
    "        \n",
    "        _, train_accuracy = config['tr_ms'](model, train_loader, optimizer, scheduler, config, steps)\n",
    "        \n",
    "        # Train accuracy for the final epoch is stored\n",
    "        learning_rate_record.append((log_lr, train_accuracy))\n",
    "    \n",
    "    return learning_rate_record\n",
    "\n",
    "\n",
    "def draw_learning_rate_record(learning_rate_record, use_wandb=False):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "\n",
    "    fig = plt.figure(num=1, clear=True, constrained_layout=True, figsize=(5.0, 5.0))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.set_title('Learning Rate Search')\n",
    "    ax.set_xlabel('Learning rate in log-scale')\n",
    "    ax.set_ylabel('Train accuracy')\n",
    "\n",
    "    ax.scatter(*max(learning_rate_record, key=lambda x: x[1]), \n",
    "               s=150, c='w', marker='o', edgecolors='limegreen')\n",
    "    \n",
    "    for log_lr, val_accuracy in learning_rate_record:\n",
    "        ax.scatter(log_lr, val_accuracy, c='r',\n",
    "                   alpha=0.5, edgecolors='none')\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({'Learning Rate Search (Image)': wandb.Image(plt)})\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configurations\n",
    "cfg_train = {}\n",
    "cfg_train['iterations'] = 100000\n",
    "cfg_train['history_interval'] = cfg_train['iterations'] // 500\n",
    "cfg_train['lr_decay_step'] = round(cfg_train['iterations'] * 0.8)\n",
    "cfg_train['lr_decay_gamma'] = 0.1\n",
    "cfg_train['weight_decay'] = 1e-2\n",
    "cfg_train['mixup'] = 0.0 # 0 for no usage\n",
    "cfg_train['criterion'] = 'cross-entropy' # 'cross-entropy', 'multi-bce'\n",
    "cfg_train['tr_ms'] = train_multistep if cfg_train.get('mixup', 0) < 1e-3 else train_mixup_multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cfg_model in model_pool:\n",
    "    if cfg_model[\"LR\"] is None:\n",
    "        print(f'{cfg_model[\"model\"]} LR searching..')\n",
    "        model = cfg_model['generator'](**cfg_model).to(device)\n",
    "        model.train()\n",
    "        \n",
    "        record = learning_rate_search(model, min_log_lr=-4.5, max_log_lr=-1.4, \n",
    "                                      trials=5, config=cfg_train, steps=300)\n",
    "        best_log_lr = record[np.argmax(np.array([v for lr, v in record]))][0]\n",
    "        \n",
    "        cfg_model['LR'] = 10 ** best_log_lr\n",
    "        cfg_model['lr_search'] = record\n",
    "        \n",
    "        print(f'best lr {cfg_model[\"LR\"]:.5e} / log_lr {best_log_lr}')\n",
    "    else:\n",
    "        print(f'{cfg_model[\"model\"]}: {cfg_model[\"LR\"]:.5e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "save_temporary = False\n",
    "draw_result = True\n",
    "\n",
    "# progress bar\n",
    "pbar = tqdm(total=len(model_pool) * cfg_train['iterations'])\n",
    "\n",
    "# train process on model_pool\n",
    "for cfg_model in model_pool:\n",
    "    print(f'{\"*\"*40} {cfg_model[\"model\"]} train starts {\"*\"*40}')\n",
    "    \n",
    "    # wandb initialization\n",
    "    config = {}\n",
    "    config.update(cfg_data)\n",
    "    config.update(cfg_train)\n",
    "    config.update(cfg_model)\n",
    "    \n",
    "    # generate model and its trainer\n",
    "    model = config['generator'](**config).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=config['LR'], \n",
    "                            weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                          step_size=config['lr_decay_step'], \n",
    "                                          gamma=config['lr_decay_gamma'])\n",
    "    \n",
    "    wandb_run = wandb.init(project=\"eeg-analysis\", \n",
    "                           entity=\"ipis-mjkim\", \n",
    "                           reinit=True,\n",
    "                           save_code=True, \n",
    "                           notes=nb_fname,\n",
    "                           config=config)\n",
    "    wandb.run.name = wandb.run.id\n",
    "    \n",
    "    save_path = f'history_temp/{wandb.run.name}/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    with wandb_run:\n",
    "        wandb.watch(model, log='all', \n",
    "                    log_freq=config['history_interval'], \n",
    "                    log_graph=True)\n",
    "        \n",
    "        # train and validation routine\n",
    "        best_val_acc = 0\n",
    "        for i in range(0, config[\"iterations\"], config[\"history_interval\"]):\n",
    "            # train 'history_interval' steps\n",
    "            loss, train_acc = cfg_train['tr_ms'](model, train_loader, optimizer, scheduler, \n",
    "                                                 config, config[\"history_interval\"])\n",
    "            \n",
    "            # validation\n",
    "            val_acc, _, _, _, _ = check_accuracy(model, val_loader, config, repeat=10)\n",
    "            \n",
    "            if best_val_acc < val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = deepcopy(model.state_dict())                \n",
    "                if save_model and save_temporary:\n",
    "                    path = os.path.join(save_path, f'{config[\"model\"]}')\n",
    "                    torch.save(best_model_state, path)                    \n",
    "                \n",
    "            # log\n",
    "            wandb.log({'Loss': loss, \n",
    "                       'Train Accuracy': train_acc, \n",
    "                       'Validation Accuracy': val_acc}, step=i)\n",
    "            pbar.update(config['history_interval'])\n",
    "\n",
    "        # calculate the test accuracies for best and last models\n",
    "        last_model_state = deepcopy(model.state_dict())\n",
    "        last_test_result = check_accuracy(model, test_loader, config, repeat=30)\n",
    "        last_test_acc = last_test_result[0]\n",
    "        \n",
    "        model.load_state_dict(best_model_state)\n",
    "        best_test_result = check_accuracy(model, test_loader, config, repeat=30)\n",
    "        best_test_acc = best_test_result[0]\n",
    " \n",
    "        if last_test_acc < best_test_acc:\n",
    "            model_state = best_model_state\n",
    "            test_result = best_test_result\n",
    "        else:\n",
    "            model_state = last_model_state\n",
    "            test_result = last_test_result\n",
    "            \n",
    "        model.load_state_dict(model_state)\n",
    "        test_acc, test_confusion, test_debug, score, target = test_result\n",
    "        \n",
    "        # calculate the test accuracies for final model on much longer sequence\n",
    "        last_test_result = check_accuracy(model, longer_test_loader, config, repeat=30)\n",
    "        longer_test_acc = last_test_result[0]\n",
    "        \n",
    "        # save the model\n",
    "        if save_model:\n",
    "            path = os.path.join(save_path, f'{config[\"model\"]}')\n",
    "            torch.save(model_state, path)\n",
    "            \n",
    "        # leave the message\n",
    "        wandb.log({'Test Accuracy': test_acc,\n",
    "                   '(Best / Last) Test Accuracy': ('Best' if last_test_acc < best_test_acc else 'Last', \n",
    "                                                   round(best_test_acc, 2), round(last_test_acc, 2)),\n",
    "                   'Confusion Matrix (Array)': test_confusion,\n",
    "                   'Test Accuracy (Longer)': longer_test_acc, \n",
    "                   'Test Debug Table/Serial': test_debug[0], \n",
    "                   'Test Debug Table/EDF': test_debug[1], \n",
    "                   'Test Debug Table/Pred': test_debug[2], \n",
    "                   'Test Debug Table/GT': test_debug[3]})\n",
    "        \n",
    "        if 'lr_search' in config:\n",
    "            draw_learning_rate_record(config['lr_search'], use_wandb=True)\n",
    "            \n",
    "        \n",
    "        if draw_result:\n",
    "            draw_roc_curve(score, target, use_wandb=True)\n",
    "            draw_confusion(test_confusion, use_wandb=True)\n",
    "            draw_debug_table(test_debug, use_wandb=True)\n",
    "            wandb.log({\"Confusion Matrix\": wandb.plot.confusion_matrix(y_true=target, \n",
    "                                                                              preds=score.argmax(axis=-1), \n",
    "                                                                              class_names=class_label_to_type)})\n",
    "            wandb.log({\"ROC Curve\": wandb.plot.roc_curve(target, score, labels=class_label_to_type)})\n",
    "            \n",
    "            \n",
    "    print('\\n' + '-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
