{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 1D CNN\n",
    "\n",
    "1D Convolution을 기본 구성 요소로 하는 EEG classifier를 학습해보는 노트북.\n",
    "\n",
    "- Three-way SoftMax classifier of normal, non-vascular MCI, and non-vascular dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from itertools import cycle\n",
    "\n",
    "# custom package\n",
    "from utils.eeg_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook name\n",
    "def get_notebook_name():\n",
    "    import ipynbname\n",
    "    return ipynbname.name()\n",
    "\n",
    "nb_fname = get_notebook_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n",
    "#  'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n",
    "#  'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n",
    "#  'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', \n",
    "#  'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic' # for Hangul in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.9.0\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 78,\n",
      " 'birth': '1940-06-02',\n",
      " 'dx1': 'mci_rf',\n",
      " 'edfname': '00001809_261018',\n",
      " 'events': [[0, 'Start Recording'],\n",
      "            [0, 'New Montage - Montage 002'],\n",
      "            [36396, 'Eyes Open'],\n",
      "            [72518, 'Eyes Closed'],\n",
      "            [73862, 'Eyes Open'],\n",
      "            [75248, 'Eyes Closed'],\n",
      "            [76728, 'swallowing'],\n",
      "            [77978, 'Eyes Open'],\n",
      "            [79406, 'Eyes Closed'],\n",
      "            [79996, 'Photic On - 3.0 Hz'],\n",
      "            [80288, 'Eyes Open'],\n",
      "            [81296, 'Eyes Closed'],\n",
      "            [82054, 'Photic Off'],\n",
      "            [84070, 'Photic On - 6.0 Hz'],\n",
      "            [84488, 'Eyes Open'],\n",
      "            [85538, 'Eyes Closed'],\n",
      "            [86086, 'Photic Off'],\n",
      "            [88144, 'Photic On - 9.0 Hz'],\n",
      "            [90160, 'Photic Off'],\n",
      "            [91458, 'Eyes Open'],\n",
      "            [92218, 'Photic On - 12.0 Hz'],\n",
      "            [92762, 'Eyes Closed'],\n",
      "            [94198, 'Photic Off'],\n",
      "            [94742, 'Eyes Open'],\n",
      "            [95708, 'Eyes Closed'],\n",
      "            [96256, 'Photic On - 15.0 Hz'],\n",
      "            [98272, 'Photic Off'],\n",
      "            [100330, 'Photic On - 18.0 Hz'],\n",
      "            [102346, 'Photic Off'],\n",
      "            [102596, 'Eyes Open'],\n",
      "            [103856, 'Eyes Closed'],\n",
      "            [104361, 'Photic On - 21.0 Hz'],\n",
      "            [106420, 'Photic Off'],\n",
      "            [106880, 'Eyes Open'],\n",
      "            [107804, 'Eyes Closed'],\n",
      "            [108435, 'Photic On - 24.0 Hz'],\n",
      "            [110452, 'Photic Off'],\n",
      "            [111080, 'Eyes Open'],\n",
      "            [112004, 'Eyes Closed'],\n",
      "            [112509, 'Photic On - 27.0 Hz'],\n",
      "            [114528, 'Photic Off'],\n",
      "            [114864, 'Eyes Open'],\n",
      "            [116124, 'Eyes Closed'],\n",
      "            [116544, 'Photic On - 30.0 Hz'],\n",
      "            [118602, 'Photic Off'],\n",
      "            [126672, 'artifact'],\n",
      "            [134030, 'Move'],\n",
      "            [135584, 'Eyes Open'],\n",
      "            [136668, 'Eyes Closed'],\n",
      "            [139818, 'Eyes Open'],\n",
      "            [141414, 'Eyes Closed'],\n",
      "            [145000, 'Paused']],\n",
      " 'label': ['mci', 'mci_amnestic', 'mci_amnestic_rf'],\n",
      " 'record': '2018-10-26T15:46:26',\n",
      " 'serial': '00001'}\n"
     ]
    }
   ],
   "source": [
    "# Data file path\n",
    "data_path = r'dataset/02_Curated_Data/'\n",
    "meta_path = os.path.join(data_path, 'metadata_debug.json')\n",
    "\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label_to_type: ['Normal', 'Non-vascular MCI', 'Non-vascular dementia']\n"
     ]
    }
   ],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        # inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 463 data belonging to Normal\n",
      "- There are 347 data belonging to Non-vascular MCI\n",
      "- There are 229 data belonging to Non-vascular dementia\n"
     ]
    }
   ],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size\t\t: 831\n",
      "Validation data size\t: 104\n",
      "Test data size\t\t: 104\n",
      "\n",
      " --- Recheck --- \n",
      "\n",
      "Train data label distribution\t: [370 278 183] 831\n",
      "Val data label distribution\t: [46 35 23] 104\n",
      "Test data label distribution\t: [47 34 23] 104\n"
     ]
    }
   ],
   "source": [
    "# random seed\n",
    "random.seed(0)\n",
    "\n",
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())\n",
    "\n",
    "# random seed\n",
    "random.seed()\n",
    "\n",
    "# print([m['serial']  for m in metadata_train[:15]])\n",
    "# print([m['serial']  for m in metadata_val[:15]])\n",
    "# print([m['serial']  for m in metadata_test[:15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap the splitted data using PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age mean and standard deviation:\n",
      "69.92779783393502 9.817569889945597\n"
     ]
    }
   ],
   "source": [
    "ages = []\n",
    "for m in metadata_train:\n",
    "    ages.append(m['age'])\n",
    "\n",
    "ages = np.array(ages)\n",
    "age_mean = np.mean(ages)\n",
    "age_std = np.std(ages)\n",
    "\n",
    "print('Age mean and standard deviation:')\n",
    "print(age_mean, age_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2000])\n",
      "{'signal': tensor([[-0.0935, -0.0935, -0.0935,  ...,  0.2002,  0.0925,  0.0044],\n",
      "        [ 0.6007,  0.4740,  0.3474,  ..., -0.3176, -0.3493, -0.3493],\n",
      "        [ 0.5080,  0.3644,  0.2926,  ...,  0.2926,  0.3285,  0.1849],\n",
      "        ...,\n",
      "        [ 0.9873,  0.5781,  0.2712,  ...,  1.6010,  1.2941,  1.0896],\n",
      "        [-0.3849, -0.5918, -0.7470,  ...,  0.9084,  0.8049,  0.7532],\n",
      "        [-0.0808, -0.0808, -0.0738,  ..., -0.1781, -0.2684, -0.3170]]), 'age': tensor(-1.2149), 'class_label': tensor(0), 'metadata': {'serial': '01012', 'edfname': '01212635_270515', 'birth': '1956-06-01', 'record': '2015-05-27T09:37:24', 'age': 58, 'dx1': 'cb_normal', 'label': ['normal', 'cb_normal'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [400, 'Eyes Open'], [7918, 'Eyes Closed'], [14091, 'Eyes Open'], [18208, 'Eyes Closed'], [24256, 'Eyes Open'], [30724, 'Eyes Closed'], [36562, 'Eyes Open'], [42190, 'Eyes Closed'], [48910, 'Eyes Open'], [55126, 'Eyes Closed'], [60417, 'Eyes Open'], [66004, 'Eyes Closed'], [71968, 'Eyes Open'], [78310, 'Eyes Closed'], [84442, 'Eyes Open'], [90070, 'Eyes Closed'], [96076, 'Eyes Open'], [102082, 'Eyes Closed'], [108844, 'Eyes Open'], [113674, 'Eyes Closed'], [120000, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 2000])\n",
      "{'signal': tensor([[ 0.3037,  0.1469,  0.0489,  ...,  1.0875,  1.1659,  1.1659],\n",
      "        [ 0.2688,  0.3306,  0.3924,  ...,  0.2379,  0.4850,  0.5777],\n",
      "        [ 0.2640,  0.2640,  0.2040,  ..., -0.3360, -0.2160, -0.2760],\n",
      "        ...,\n",
      "        [-0.5574, -0.8252, -1.0930,  ...,  0.9155,  1.0494,  1.0494],\n",
      "        [ 0.3288,  0.3288,  0.3288,  ...,  0.6647,  0.6647,  0.6647],\n",
      "        [-0.2742, -0.2662, -0.2420,  ..., -0.0246, -0.0246,  0.0237]]), 'age': tensor(0.7204), 'class_label': tensor(1), 'metadata': {'serial': '00700', 'edfname': '00985401_011117', 'birth': '1940-09-09', 'record': '2017-11-01T14:20:48', 'age': 77, 'dx1': 'mci amnestic', 'label': ['mci', 'mci_amnestic'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1705, 'Eyes Open'], [5402, 'Eyes Closed'], [13046, 'Eyes Open'], [17666, 'Eyes Closed'], [30308, 'Eyes Closed'], [36272, 'Eyes Open'], [41774, 'Eyes Closed'], [48958, 'Eyes Open'], [55510, 'Eyes Closed'], [61641, 'Eyes Open'], [66766, 'Eyes Closed'], [72730, 'Eyes Open'], [78988, 'Eyes Closed'], [87770, 'Eyes Open'], [90542, 'Eyes Closed'], [97096, 'Eyes Open'], [102178, 'Eyes Closed'], [110872, 'Eyes Open'], [113728, 'Eyes Closed'], [122052, 'Photic On - 3.0 Hz'], [122428, 'Eyes Open'], [123309, 'Eyes Closed'], [124068, 'Photic Off'], [126126, 'Photic On - 6.0 Hz'], [128142, 'Photic Off'], [130158, 'Photic On - 9.0 Hz'], [132216, 'Photic Off'], [132718, 'Eyes Open'], [133600, 'Eyes Closed'], [134232, 'Photic On - 12.0 Hz'], [136248, 'Photic Off'], [138306, 'Photic On - 15.0 Hz'], [140322, 'Photic Off'], [142380, 'Photic On - 18.0 Hz'], [142630, 'Eyes Open'], [143302, 'Eyes Closed'], [144396, 'Photic Off'], [146412, 'Photic On - 21.0 Hz'], [148428, 'Photic Off'], [150486, 'Photic On - 24.0 Hz'], [152502, 'Photic Off'], [152710, 'Eyes Open'], [153550, 'Eyes Closed'], [154560, 'Photic On - 27.0 Hz'], [156576, 'Photic Off'], [158592, 'Photic On - 30.0 Hz'], [160608, 'Photic Off'], [160942, 'Eyes Open'], [161698, 'Eyes Closed'], [169132, 'Eyes Open'], [170098, 'Eyes Closed'], [173600, 'Paused']], 'class_type': 'Non-vascular MCI', 'class_label': 1}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 2000])\n",
      "{'signal': tensor([[-0.5036, -0.5713, -0.5036,  ..., -0.5713, -0.5713, -0.5713],\n",
      "        [-2.2502, -2.3362, -2.2502,  ..., -0.7876, -0.7015, -0.7015],\n",
      "        [-3.3935, -3.3935, -3.3257,  ..., -0.2545, -0.1867, -0.1642],\n",
      "        ...,\n",
      "        [-0.4359, -0.4359, -0.1803,  ..., -0.4359, -0.3507, -0.2655],\n",
      "        [-0.7529, -0.7529, -0.6740,  ..., -0.8317, -0.7529, -0.7529],\n",
      "        [-0.0267, -0.0254, -0.0241,  ...,  0.5850,  0.4564,  0.2476]]), 'age': tensor(1.0259), 'class_label': tensor(0), 'metadata': {'serial': '00299', 'edfname': '00671212_160819', 'birth': '1938-08-17', 'record': '2019-08-16T10:57:03', 'age': 80, 'dx1': 'smi', 'label': ['normal', 'smi'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 005'], [1773, 'Eyes Closed'], [6000, 'Cz check'], [7612, 'Eyes Open'], [12912, 'Eyes Closed'], [18078, 'Eyes Open'], [23958, 'Eyes Closed'], [29288, 'Eyes Open'], [35934, 'Eyes Closed'], [41856, 'Eyes Open'], [47862, 'Eyes Closed'], [54460, 'Eyes Open'], [59962, 'Eyes Closed'], [66178, 'Eyes Open'], [71008, 'Eyes Closed'], [73948, 'Photic On - 3.0 Hz'], [74158, 'Eyes Open'], [75166, 'Eyes Closed'], [75964, 'Photic Off'], [77980, 'Photic On - 6.0 Hz'], [78358, 'Eyes Open'], [79282, 'Eyes Closed'], [80038, 'Photic Off'], [82054, 'Photic On - 9.0 Hz'], [84070, 'Photic Off'], [86128, 'Photic On - 12.0 Hz'], [87640, 'Eyes Open'], [88144, 'Photic Off'], [88396, 'Eyes Closed'], [90202, 'Photic On - 15.0 Hz'], [92218, 'Photic Off'], [92722, 'Eyes Open'], [93772, 'Eyes Closed'], [94234, 'Photic On - 18.0 Hz'], [96250, 'Photic Off'], [98308, 'Photic On - 21.0 Hz'], [100324, 'Photic Off'], [102382, 'Photic On - 24.0 Hz'], [104398, 'Photic Off'], [106414, 'Photic On - 27.0 Hz'], [108430, 'Photic Off'], [110488, 'Photic On - 30.0 Hz'], [111580, 'Eyes Open'], [111790, 'Photic Off'], [112420, 'Eyes Closed'], [113200, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n"
     ]
    }
   ],
   "source": [
    "crop_length = 200 * 10 # 10 seconds\n",
    "\n",
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=crop_length),\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGToTensor()])\n",
    "\n",
    "train_dataset = EEGDataset(data_path, metadata_train, composed)\n",
    "val_dataset = EEGDataset(data_path, metadata_val, composed)\n",
    "test_dataset = EEGDataset(data_path, metadata_test, composed)\n",
    "\n",
    "print(train_dataset[0]['signal'].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0]['signal'].shape)\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0]['signal'].shape)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device: cuda\n",
      "0 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "1 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "2 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "3 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "4 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n"
     ]
    }
   ],
   "source": [
    "print('Current PyTorch device:', device)\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(i_batch, \n",
    "          sample_batched['signal'].shape, \n",
    "          sample_batched['age'].shape, \n",
    "          sample_batched['class_label'].shape, \n",
    "          len(sample_batched['metadata']))\n",
    "    \n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation, test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        num_workers=num_workers, \n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=eeg_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=32, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory,\n",
    "                         collate_fn=eeg_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Define 1D CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def visualize_network_tensorboard(model, name):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter('runs/' + nb_fname + '_' + name)\n",
    "\n",
    "    for batch_i, sample_batched in enumerate(train_loader):\n",
    "        # pull up the batch data\n",
    "        x = sample_batched['signal'].to(device)\n",
    "        age = sample_batched['age'].to(device)\n",
    "        target = sample_batched['class_label'].to(device)\n",
    "\n",
    "        # apply model on whole batch directly on device\n",
    "        writer.add_graph(model, (x, age))\n",
    "        output = model(x, age, print_shape=True)\n",
    "        break\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D Tiny CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, n_input=20, n_output=3, stride=7, n_channel=64, \n",
    "                 use_age=True, final_pool='average'):\n",
    "        super().__init__()\n",
    "        \n",
    "        if final_pool not in {'average', 'max'}:\n",
    "            raise ValueError(\"final_pool must be set to one of ['average', 'max']\")\n",
    "        \n",
    "        self.use_age = use_age\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=35, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=7)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        if final_pool == 'average':\n",
    "            self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        elif final_pool == 'max':\n",
    "            self.final_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        if self.use_age:        \n",
    "            self.fc1 = nn.Linear(n_channel + 1, n_channel)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(n_channel, n_channel)\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bnfc1 = nn.BatchNorm1d(n_channel)\n",
    "        self.fc2 = nn.Linear(n_channel, n_output)\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "\n",
    "    def forward(self, x, age, print_shape=False):\n",
    "        # conv-bn-relu-pool \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        if print_shape:\n",
    "            print('Shape right before squeezing:', x.shape)\n",
    "\n",
    "        x = self.final_pool(x).squeeze()\n",
    "        if self.use_age:\n",
    "            x = torch.cat((x, age.reshape(-1, 1)), dim=1)\n",
    "\n",
    "        # fc-bn-dropout-relu-fc\n",
    "        x = self.fc1(x)\n",
    "        x = self.bnfc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        # return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_TinyCNN():\n",
    "    return TinyCNN(n_input=train_dataset[0]['signal'].shape[0], \n",
    "                   n_output=3, use_age=True, final_pool='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyCNN(\n",
      "  (conv1): Conv1d(20, 64, kernel_size=(35,), stride=(7,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (final_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=65, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (bnfc1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IPIS-Minjae\\anaconda3\\envs\\EEG_Project\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape right before squeezing: torch.Size([32, 64, 32])\n",
      "The Number of parameters of the model: 78,403\n"
     ]
    }
   ],
   "source": [
    "model = generate_TinyCNN()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# tensorboard visualization\n",
    "visualize_network_tensorboard(model, 'TinyCNN')\n",
    "\n",
    "# number of parameters\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=20, n_output=3, stride=4, n_channel=256, \n",
    "                 use_age=True, final_pool='average'):\n",
    "        super().__init__()\n",
    "        \n",
    "        if final_pool not in {'average', 'max'}:\n",
    "            raise ValueError(\"final_pool must be set to one of ['average', 'max']\")\n",
    "        \n",
    "        self.use_age = use_age\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=41, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=11)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=11)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=11)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=11)\n",
    "        self.bn5 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool5 = nn.MaxPool1d(2)\n",
    "        \n",
    "        if final_pool == 'average':\n",
    "            self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        elif final_pool == 'max':\n",
    "            self.final_pool = nn.AdaptiveMaxPool1d(1)\n",
    "    \n",
    "        if self.use_age:        \n",
    "            self.fc1 = nn.Linear(2 * n_channel + 1, 2 * n_channel)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(2 * n_channel, 2 * n_channel)\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bnfc1 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.fc2 = nn.Linear(2 * n_channel, n_output)\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "\n",
    "    def forward(self, x, age, print_shape=False):\n",
    "        # conv-bn-relu-pool \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.bn5(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        if print_shape:\n",
    "            print('Shape right before squeezing:', x.shape)\n",
    "    \n",
    "        x = self.final_pool(x).squeeze()\n",
    "        if self.use_age:\n",
    "            x = torch.cat((x, age.reshape(-1, 1)), dim=1)\n",
    "\n",
    "        # fc-bn-dropout-relu-fc\n",
    "        x = self.fc1(x)\n",
    "        x = self.bnfc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        # return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_M5():\n",
    "    return M5(n_input=train_dataset[0]['signal'].shape[0], \n",
    "              n_output=3, use_age=True, final_pool='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(20, 256, kernel_size=(41,), stride=(2,))\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(11,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(256, 512, kernel_size=(11,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (final_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=513, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (bnfc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Shape right before squeezing: torch.Size([32, 512, 21])\n",
      "The Number of parameters of the model: 8,411,651\n"
     ]
    }
   ],
   "source": [
    "model = generate_M5()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# tensorboard visualization\n",
    "visualize_network_tensorboard(model, 'M5')\n",
    "\n",
    "# number of parameters\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M5-like model without the usage of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_M5_no_age():\n",
    "    return M5(n_input=train_dataset[0]['signal'].shape[0], \n",
    "              n_output=3, use_age=False, final_pool='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(20, 256, kernel_size=(41,), stride=(2,))\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(11,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(256, 512, kernel_size=(11,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n",
      "  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (final_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (bnfc1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Shape right before squeezing: torch.Size([32, 512, 21])\n",
      "The Number of parameters of the model: 8,411,139\n"
     ]
    }
   ],
   "source": [
    "model = generate_M5_no_age()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# tensorboard visualization\n",
    "visualize_network_tensorboard(model, 'M5-no-age')\n",
    "\n",
    "# number of parameters\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=c_in, out_channels=c_out, \n",
    "                               kernel_size=kernel_size, stride=stride, \n",
    "                               padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(c_out)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=c_out, out_channels=c_out, \n",
    "                               kernel_size=kernel_size, stride=1, \n",
    "                               padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(c_out)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.downsample = None\n",
    "        if stride != 1 or c_in != c_out:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=c_in, out_channels=c_out, \n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(c_out)\n",
    "            )\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        x = self.relu(x + identity)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    expansion: int = 4\n",
    "        \n",
    "    def __init__(self, c_in, c_out, kernel_size, stride) -> None:\n",
    "        super().__init__()\n",
    "        width = c_out\n",
    "        self.conv1 = nn.Conv1d(in_channels=c_in, out_channels=width, \n",
    "                               kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(width)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=width, out_channels=width, \n",
    "                               kernel_size=kernel_size, stride=stride, \n",
    "                               padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(width)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=width, out_channels=c_out*self.expansion, \n",
    "                               kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(c_out*self.expansion)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.downsample = None\n",
    "        if stride != 1 or c_in != c_out*self.expansion:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=c_in, out_channels=c_out*self.expansion, \n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(c_out*self.expansion)\n",
    "            )\n",
    "                    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "            \n",
    "        x = self.relu(x + identity)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 block: Type[Union[BasicResBlock, BottleneckBlock]], \n",
    "                 conv_layers: List[int],\n",
    "                 n_fc: int,\n",
    "                 n_input=20,\n",
    "                 n_output=3,\n",
    "                 n_start=64,\n",
    "                 kernel_size=9, \n",
    "                 use_age=True, \n",
    "                 final_pool='average') -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if final_pool not in {'average', 'max'}:\n",
    "            raise ValueError(\"final_pool must be set to one of ['average', 'max']\")        \n",
    "\n",
    "        self.c_current = n_start\n",
    "        self.use_age = use_age\n",
    "        \n",
    "        self.input_stage = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_input, out_channels=n_start, \n",
    "                      kernel_size=kernel_size*3, stride=2,\n",
    "                      padding=(kernel_size*3)//2, bias=False),\n",
    "            nn.BatchNorm1d(n_start), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.conv_stage1 = self._make_conv_layer(block, conv_layers[0], n_start, kernel_size, stride=3)\n",
    "        self.conv_stage2 = self._make_conv_layer(block, conv_layers[1], n_start*2, kernel_size, stride=3)\n",
    "        self.conv_stage3 = self._make_conv_layer(block, conv_layers[2], n_start*4, kernel_size, stride=3)\n",
    "        self.conv_stage4 = self._make_conv_layer(block, conv_layers[3], n_start*8, kernel_size, stride=3)\n",
    "        \n",
    "        if final_pool == 'average':\n",
    "            self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        elif final_pool == 'max':\n",
    "            self.final_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        fc_layers = []        \n",
    "        if self.use_age:\n",
    "            self.c_current = self.c_current + 1\n",
    "        \n",
    "        for l in range(n_fc):\n",
    "            layer = nn.Sequential(nn.Linear(self.c_current, self.c_current // 2, bias=False),\n",
    "                                  nn.Dropout(p=0.1),\n",
    "                                  nn.BatchNorm1d(self.c_current // 2), \n",
    "                                  nn.ReLU())\n",
    "            self.c_current = self.c_current // 2\n",
    "            fc_layers.append(layer)\n",
    "        fc_layers.append(nn.Linear(self.c_current, n_output))\n",
    "        self.fc_stage = nn.Sequential(*fc_layers)\n",
    "        \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "        \n",
    "    def _make_conv_layer(self, block: Type[Union[BasicResBlock, BottleneckBlock]], \n",
    "                         n_block: int, c_out: int, kernel_size: int, stride: int = 1) -> nn.Sequential:\n",
    "        layers = []\n",
    "        c_in = self.c_current\n",
    "        layers.append(block(c_in, c_out, kernel_size, stride=1))\n",
    "\n",
    "        c_in = c_out * block.expansion\n",
    "        self.c_current = c_in\n",
    "        for _ in range(1, n_block):\n",
    "            layers.append(block(c_in, c_out, kernel_size, stride=1))\n",
    "            \n",
    "        layers.append(nn.MaxPool1d(kernel_size=stride))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, age, print_shape=False):\n",
    "        x = self.input_stage(x)\n",
    "        \n",
    "        x = self.conv_stage1(x)\n",
    "        x = self.conv_stage2(x)\n",
    "        x = self.conv_stage3(x)\n",
    "        x = self.conv_stage4(x)\n",
    "\n",
    "        if print_shape:\n",
    "            print('Shape right before squeezing:', x.shape)\n",
    "                \n",
    "        x = self.final_pool(x).squeeze()\n",
    "        if self.use_age:\n",
    "            x = torch.cat((x, age.reshape(-1, 1)), dim=1)\n",
    "        x = self.fc_stage(x)\n",
    "        \n",
    "        return x\n",
    "        # return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ResNet():\n",
    "    return ResNet(block=BottleneckBlock, \n",
    "                  conv_layers=[2, 2, 2, 2], \n",
    "                  n_fc=3, \n",
    "                  n_input=train_dataset[0]['signal'].shape[0], \n",
    "                  n_output=3, \n",
    "                  n_start=64,\n",
    "                  kernel_size=9, \n",
    "                  use_age=True, \n",
    "                  final_pool='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (input_stage): Sequential(\n",
      "    (0): Conv1d(20, 64, kernel_size=(27,), stride=(2,), padding=(13,), bias=False)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_stage1): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage2): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage3): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage4): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (final_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc_stage): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=2049, out_features=1024, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Shape right before squeezing: torch.Size([32, 2048, 12])\n",
      "The Number of parameters of the model: 16,729,219\n"
     ]
    }
   ],
   "source": [
    "model = generate_ResNet()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# tensorboard visualization\n",
    "visualize_network_tensorboard(model, '1D-ResNet')\n",
    "\n",
    "# number of parameters\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN: ResNet-like model without the usage of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ResNet_no_age():\n",
    "    return ResNet(block=BottleneckBlock, \n",
    "                  conv_layers=[2, 2, 2, 2], \n",
    "                  n_fc=3, \n",
    "                  n_input=train_dataset[0]['signal'].shape[0], \n",
    "                  n_output=3, \n",
    "                  n_start=64,\n",
    "                  kernel_size=9, \n",
    "                  use_age=False, \n",
    "                  final_pool='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (input_stage): Sequential(\n",
      "    (0): Conv1d(20, 64, kernel_size=(27,), stride=(2,), padding=(13,), bias=False)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_stage1): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage2): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(128, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage3): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage4): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv1d(512, 2048, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (bn3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (final_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc_stage): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Shape right before squeezing: torch.Size([32, 2048, 12])\n",
      "The Number of parameters of the model: 16,728,195\n"
     ]
    }
   ],
   "source": [
    "model = generate_ResNet_no_age()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# tensorboard visualization\n",
    "visualize_network_tensorboard(model, '1D-ResNet-no-age')\n",
    "\n",
    "# number of parameters\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D Tiny ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_TinyResNet():\n",
    "    return ResNet(block=BasicResBlock, \n",
    "                  conv_layers=[1, 1, 1, 1], \n",
    "                  n_fc=3, \n",
    "                  n_input=train_dataset[0]['signal'].shape[0], \n",
    "                  n_output=3, \n",
    "                  n_start=64,\n",
    "                  kernel_size=9, \n",
    "                  use_age=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (input_stage): Sequential(\n",
      "    (0): Conv1d(20, 64, kernel_size=(27,), stride=(2,), padding=(13,), bias=False)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_stage1): Sequential(\n",
      "    (0): BasicResBlock(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage2): Sequential(\n",
      "    (0): BasicResBlock(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage3): Sequential(\n",
      "    (0): BasicResBlock(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_stage4): Sequential(\n",
      "    (0): BasicResBlock(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (final_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc_stage): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=False)\n",
      "      (1): Dropout(p=0.1, inplace=False)\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "The Number of parameters of the model: 5,104,067\n"
     ]
    }
   ],
   "source": [
    "model = generate_TinyResNet()\n",
    "model = model.to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(f'The Number of parameters of the model: {n:,}')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Some useful functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val_accuracy(model, repeat=1):\n",
    "    model.eval()\n",
    "    correct, total = (0, 0)\n",
    "        \n",
    "    C = len(class_label_to_type)\n",
    "    val_confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    for k in range(repeat):\n",
    "        for sample_batched in val_loader:\n",
    "            # pull up the data\n",
    "            x = sample_batched['signal'].to(device)\n",
    "            age = sample_batched['age'].to(device)\n",
    "            target = sample_batched['class_label'].to(device)\n",
    "\n",
    "            # apply model on whole batch directly on device\n",
    "            output = model(x, age)\n",
    "            pred = F.log_softmax(output, dim=1)\n",
    "\n",
    "            # val accuracy\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            correct += pred.squeeze().eq(target).sum().item()\n",
    "            total += pred.shape[0]\n",
    "            \n",
    "            # confusion matrix\n",
    "            val_confusion += calculate_confusion_matrix(pred, target)\n",
    "            \n",
    "    val_accuracy = 100.0 * correct / total\n",
    "    return (val_accuracy, val_confusion)\n",
    "\n",
    "    \n",
    "def check_test_accuracy(model, repeat=1):\n",
    "    model.eval()\n",
    "    correct, total = (0, 0)\n",
    "    \n",
    "    C = len(class_label_to_type)\n",
    "    test_confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    test_debug = {data['metadata']['serial']: \n",
    "                  {'GT': data['class_label'].item(), \n",
    "                   'Acc': 0, \n",
    "                   'Pred': [0] * C} for data in test_dataset}\n",
    "    \n",
    "    score = None\n",
    "    target = None\n",
    "        \n",
    "    for k in range(repeat):\n",
    "        for sample_batched in test_loader:\n",
    "            # pull up the data\n",
    "            x = sample_batched['signal'].to(device)\n",
    "            age = sample_batched['age'].to(device)\n",
    "            y = sample_batched['class_label'].to(device)\n",
    "\n",
    "            # apply model on whole batch directly on device\n",
    "            output = model(x, age)\n",
    "            s = F.softmax(output, dim=1)\n",
    "            pred = F.log_softmax(output, dim=1)\n",
    "            \n",
    "            # test accuracy\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            correct += pred.squeeze().eq(y).sum().item()\n",
    "            total += pred.shape[0]\n",
    "            \n",
    "            if score is None:\n",
    "                score = s.detach().cpu().numpy()\n",
    "                target = y.detach().cpu().numpy()\n",
    "            else:\n",
    "                score = np.concatenate((score, s.detach().cpu().numpy()), axis=0)\n",
    "                target = np.concatenate((target, y.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "            # confusion matrix\n",
    "            test_confusion += calculate_confusion_matrix(pred, y)\n",
    "            \n",
    "            # test debug\n",
    "            for n in range(pred.shape[0]):\n",
    "                serial = sample_batched['metadata'][n]['serial']\n",
    "                test_debug[serial]['edfname'] = sample_batched['metadata'][n]['edfname']\n",
    "                test_debug[serial]['Pred'][pred[n].item()] += 1\n",
    "                acc = test_debug[serial]['Pred'][y[n].item()] / np.sum(test_debug[serial]['Pred']) * 100\n",
    "                test_debug[serial]['Acc'] = f'{acc:>6.02f}%'\n",
    "        \n",
    "    test_accuracy = 100.0 * correct / total\n",
    "    return (test_accuracy, test_confusion, test_debug, score, target)\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(pred, target):\n",
    "    N = target.shape[0]\n",
    "    C = len(class_label_to_type)\n",
    "    confusion = np.zeros((C, C), dtype=np.int32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        r = target[i]\n",
    "        c = pred[i]\n",
    "        confusion[r, c] += 1\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_plot(losses, lr_decay_step):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.0, 3.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    N = len(losses)\n",
    "    x = np.arange(1, N + 1)\n",
    "    ax.plot(x, losses)\n",
    "        \n",
    "    x2 = np.arange(lr_decay_step, N, lr_decay_step)\n",
    "    ax.vlines(x2, 0, 1, transform=ax.get_xaxis_transform(), \n",
    "              colors='m', alpha=0.5, linestyle='solid')\n",
    "    # ax.vlines([1, N], 0, 1, transform=ax.get_xaxis_transform(), \n",
    "    #           colors='k', alpha=0.7, linestyle='solid')\n",
    "    \n",
    "    ax.set_xlim(left=0)\n",
    "    ax.set_title('Loss Plot')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Training Loss')\n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def draw_accuracy_history(train_acc_history, val_acc_history, history_interval, lr_decay_step):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.0, 3.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    N = len(train_acc_history) * history_interval\n",
    "    x = np.arange(history_interval, N + 1, history_interval)\n",
    "    ax.plot(x, train_acc_history, 'r-', label='Train accuracy')\n",
    "    ax.plot(x, val_acc_history, 'b-', label='Validation accuracy')\n",
    "    \n",
    "    x2 = np.arange(lr_decay_step, N + 1, lr_decay_step)\n",
    "    ax.vlines(x2, 0, 1, transform=ax.get_xaxis_transform(), \n",
    "              colors='m', alpha=0.5, linestyle='solid')\n",
    "    # ax.vlines([history_interval, N], 0, 1, transform=ax.get_xaxis_transform(), \n",
    "    #           colors='k', alpha=0.7, linestyle='solid')\n",
    "    \n",
    "    ax.set_xlim(left=0)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_title('Accuracy Plot during Training')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion(confusion):\n",
    "    C = len(class_label_to_type)\n",
    "    \n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    plt.rcParams['image.cmap'] = 'jet' # 'nipy_spectral'\n",
    "\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(4.0, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    im = ax.imshow(confusion, alpha=0.8)\n",
    "\n",
    "    ax.set_xticks(np.arange(C))\n",
    "    ax.set_yticks(np.arange(C))\n",
    "    ax.set_xticklabels(class_label_to_type)\n",
    "    ax.set_yticklabels(class_label_to_type)\n",
    "    \n",
    "    for r in range(C):\n",
    "        for c in range(C):\n",
    "            text = ax.text(c, r, confusion[r, c],\n",
    "                           ha=\"center\", va=\"center\", color='k')\n",
    "    \n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Prediction')\n",
    "    ax.set_ylabel('Ground Truth')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "\n",
    "def draw_roc_curve(score, target):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "    \n",
    "    # Binarize the output\n",
    "    n_classes = len(class_label_to_type)\n",
    "    target = label_binarize(target, classes=np.arange(n_classes))\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(target[:, i], score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(target.ravel(), score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # draw class-agnostic ROC curve\n",
    "    fig = plt.figure(num=1, clear=True, figsize=(8.5, 4.0), constrained_layout=True)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    lw = 1.5\n",
    "    colors = cycle(['limegreen', 'mediumpurple', 'darkorange', \n",
    "                    'dodgerblue', 'lightcoral', 'goldenrod', \n",
    "                    'indigo', 'darkgreen', 'navy', 'brown'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                label='{0} (area = {1:0.2f})'\n",
    "                ''.format(class_label_to_type[i], roc_auc[i]))    \n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Class-Wise ROC Curves')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Plot class-aware ROC curves\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle='-', linewidth=lw)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle='-', linewidth=lw)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Class-Agnostic ROC Curves')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_search(model, min_log_lr, max_log_lr, trials, iters):\n",
    "    learning_rate_record = []\n",
    "    for t in tqdm(range(trials)):\n",
    "        log_lr = np.random.uniform(min_log_lr, max_log_lr)\n",
    "        lr = 10 ** log_lr\n",
    "        \n",
    "        model.reset_weights()\n",
    "        model.train()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "        correct, total = (0, 0)\n",
    "        \n",
    "        i = 1\n",
    "        while True:\n",
    "            for sample_batched in train_loader:\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                target = sample_batched['class_label'].to(device)\n",
    "\n",
    "                output = model(x, age)\n",
    "                pred = F.log_softmax(output, dim=1)\n",
    "                loss = F.nll_loss(pred, target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = pred.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(target).sum().item()\n",
    "                total += pred.shape[0]\n",
    "                \n",
    "                i += 1\n",
    "                if i >= iters:\n",
    "                    break\n",
    "            if i >= iters:\n",
    "                break\n",
    "        \n",
    "        train_accuracy = 100.0 * correct / total\n",
    "        \n",
    "        # Train accuracy for the final epoch is stored\n",
    "        learning_rate_record.append((log_lr, train_accuracy))\n",
    "    \n",
    "    return learning_rate_record\n",
    "\n",
    "\n",
    "def draw_learning_rate_record(learning_rate_record):\n",
    "    plt.style.use('default') # default, ggplot, fivethirtyeight, classic\n",
    "\n",
    "    fig = plt.figure(num=1, clear=True, constrained_layout=True) # figsize=(6.0, 6.0)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    \n",
    "    ax.set_title('Learning Rate Search')\n",
    "    ax.set_xlabel('Learning rate in log-scale')\n",
    "    ax.set_ylabel('Train accuracy')\n",
    "    \n",
    "\n",
    "    ax.scatter(*max(learning_rate_record, key=lambda x: x[1]), \n",
    "               s=150, c='w', marker='o', edgecolors='limegreen')\n",
    "    \n",
    "    for log_lr, val_accuracy in learning_rate_record:\n",
    "        ax.scatter(log_lr, val_accuracy, c='r',\n",
    "                   alpha=0.5, edgecolors='none')\n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    fig.clear()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'TinyCNN', 'generator': <function generate_TinyCNN at 0x00000230452015E0>, 'lr_start': None},\n",
      " {'name': 'M5', 'generator': <function generate_M5 at 0x000002302217A940>, 'lr_start': None},\n",
      " {'name': 'M5-no-age', 'generator': <function generate_M5_no_age at 0x000002302419AD30>, 'lr_start': None},\n",
      " {'name': '1D-ResNet', 'generator': <function generate_ResNet at 0x000002302419A820>, 'lr_start': None},\n",
      " {'name': '1D-ResNet-no-age', 'generator': <function generate_ResNet_no_age at 0x000002302419AF70>, 'lr_start': None},\n",
      " {'name': '1D-TinyResNet-no-age', 'generator': <function generate_TinyCNN at 0x00000230452015E0>, 'lr_start': None}]\n"
     ]
    }
   ],
   "source": [
    "model_pool = []\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = 'TinyCNN'\n",
    "model_dict['generator'] = generate_TinyCNN\n",
    "model_dict['lr_start'] = 10 ** (-2.3)\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = 'M5'\n",
    "model_dict['generator'] = generate_M5\n",
    "model_dict['lr_start'] = 10 ** (-3.3)\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = 'M5-no-age'\n",
    "model_dict['generator'] = generate_M5_no_age\n",
    "model_dict['lr_start'] = 10 ** (-3.3)\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = '1D-ResNet'\n",
    "model_dict['generator'] = generate_ResNet\n",
    "model_dict['lr_start'] = 10 ** (-3.0)\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = '1D-ResNet-no-age'\n",
    "model_dict['generator'] = generate_ResNet_no_age\n",
    "model_dict['lr_start'] = 10 ** (-3.0)\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "model_dict = {}\n",
    "model_dict['name'] = '1D-TinyResNet-no-age'\n",
    "model_dict['generator'] = generate_TinyCNN\n",
    "model_dict['lr_start'] = 10 ** (-2.5)\n",
    "model_pool.append(model_dict)\n",
    "\n",
    "pprint.pp(model_pool, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyCNN LR searching..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044c827015ca4afca7687a7873910077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_dict in model_pool:\n",
    "    if model_dict['lr_start'] is None:\n",
    "        print(f'{model_dict[\"name\"]} LR searching..')\n",
    "        model = model_dict['generator']().to(device)\n",
    "        model.train()\n",
    "        \n",
    "        record = learning_rate_search(model, min_log_lr=-4.5, max_log_lr=-1.4, \n",
    "                                      trials=500, iters=30)\n",
    "        \n",
    "        draw_learning_rate_record(record)\n",
    "        best_log_lr = record[np.argmax(np.array([v for lr, v in record]))][0]\n",
    "        model_dict['lr_start'] = 10 ** best_log_lr\n",
    "        \n",
    "        print(f'best lr {model_dict[\"lr_start\"]:.5e} / log_lr {best_log_lr}')\n",
    "    else:\n",
    "        print(f'{model_dict[\"name\"]}: {model_dict[\"lr_start\"]:.5e}')\n",
    "        \n",
    "    print('-' * 100)\n",
    "        \n",
    "pprint.pp(model_pool, width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_check = True\n",
    "save_model = True\n",
    "draw_result = True\n",
    "\n",
    "# log path\n",
    "log_path = f'history_temp/{nb_fname}/'\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "# train iterations\n",
    "n_repeats = 1\n",
    "n_iters = 20000\n",
    "# n_iters = 12500 * ((200 * 60) // crop_length)\n",
    "history_interval = n_iters // 400\n",
    "lr_decay_step = round(n_iters * 1000) ### NO LR DECAY ###\n",
    "\n",
    "print(f'# iterations: {n_iters}, # steps for linear LR decay: {lr_decay_step}')\n",
    "\n",
    "# progress bar\n",
    "pbar = tqdm(total=len(model_pool) * n_iters * n_repeats)\n",
    "\n",
    "# train process on model_pool\n",
    "for model_dict in model_pool:\n",
    "    print(f'{\"*\"*40} {model_dict[\"name\"]} train starts {\"*\"*40}')\n",
    "    best_r_test_acc = 0\n",
    "    \n",
    "    for r in range(n_repeats):\n",
    "        if file_check:\n",
    "            endwith = '' if n_repeats == 1 else f'_r{r:02d}'\n",
    "            path = os.path.join(log_path, f'{model_dict[\"name\"]}_log{endwith}')\n",
    "            if os.path.isfile(path):\n",
    "                log_dict = torch.load(path)\n",
    "                # loss and accuracy plots\n",
    "                if draw_result:\n",
    "                    draw_loss_plot(log_dict[\"losses\"], log_dict[\"lr_decay_step\"])\n",
    "                    draw_accuracy_history(log_dict[\"train_acc_history\"], log_dict[\"val_acc_history\"], \n",
    "                                          log_dict[\"history_interval\"], log_dict[\"lr_decay_step\"])\n",
    "                script = f'- {r:02d} train accuracy {log_dict[\"train_acc_history\"][-1]:.2f}%, '\\\n",
    "                f'best / last test accuracies {log_dict[\"best_test_accuracy\"]:.2f}% / {log_dict[\"last_test_accuracy\"]:.2f}% - file exists'\n",
    "                print()\n",
    "                print(script)\n",
    "                print()\n",
    "                pbar.update(n_iters)\n",
    "                continue\n",
    "\n",
    "        # load the model dict\n",
    "        model = model_dict['generator']().to(device)\n",
    "        model.train()\n",
    "        lr_start = model_dict['lr_start']\n",
    "\n",
    "        # configure for training\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr_start, weight_decay=0.0001)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)\n",
    "\n",
    "        # log during training\n",
    "        losses = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "        best_val_acc = 0\n",
    "        correct, total = (0, 0)\n",
    "\n",
    "        i = 0    \n",
    "        while True:\n",
    "            for sample_batched in train_loader:\n",
    "                model.train()\n",
    "\n",
    "                # load the data\n",
    "                x = sample_batched['signal'].to(device)\n",
    "                age = sample_batched['age'].to(device)\n",
    "                target = sample_batched['class_label'].to(device)\n",
    "\n",
    "                # forward pass\n",
    "                output = model(x, age)\n",
    "                pred = F.log_softmax(output, dim=1)\n",
    "                loss = F.nll_loss(pred, target)\n",
    "\n",
    "                # backward and update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "                # train accuracy\n",
    "                pred = pred.argmax(dim=-1)\n",
    "                correct += pred.squeeze().eq(target).sum().item()\n",
    "                total += pred.shape[0]\n",
    "\n",
    "                # log\n",
    "                losses.append(loss.item())\n",
    "                pbar.update(1)\n",
    "                i += 1\n",
    "\n",
    "                # history\n",
    "                if i % history_interval == 0:\n",
    "                    train_acc = 100.0 * correct / total\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    correct, total = (0, 0)\n",
    "\n",
    "                    val_acc, _ = check_val_accuracy(model, repeat=5)\n",
    "                    val_acc_history.append(val_acc)\n",
    "\n",
    "                    if best_val_acc < val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_model_state = deepcopy(model.state_dict())\n",
    "\n",
    "                if i >= n_iters:\n",
    "                    break\n",
    "            if i >= n_iters:\n",
    "                break\n",
    "        \n",
    "        # loss and accuracy plots\n",
    "        if draw_result:\n",
    "            draw_loss_plot(losses, lr_decay_step)\n",
    "            draw_accuracy_history(train_acc_history, val_acc_history, history_interval, lr_decay_step)\n",
    "            \n",
    "        # calculate the test accuracies for best and last models\n",
    "        last_model_state = deepcopy(model.state_dict())\n",
    "        last_test_acc, last_test_confusion, last_test_debug, _, _ = check_test_accuracy(model, repeat=30)\n",
    "        \n",
    "        model.load_state_dict(best_model_state)\n",
    "        best_test_acc, best_test_confusion, best_test_debug, _, _ = check_test_accuracy(model, repeat=30)\n",
    "\n",
    "        # save the model if it is best among repeatedly trained models\n",
    "        if save_model and best_r_test_acc < max(last_test_acc, best_test_acc):\n",
    "            best_r_test_acc = max(last_test_acc, best_test_acc)\n",
    "            model_state = last_model_state if best_test_acc < last_test_acc else best_model_state\n",
    "            path = os.path.join(log_path, f'{model_dict[\"name\"]}')\n",
    "            torch.save(model_state, path)\n",
    "\n",
    "        # leave the log\n",
    "        endwith = '' if n_repeats == 1 else f'_r{r:02d}'\n",
    "        path = os.path.join(log_path, f'{model_dict[\"name\"]}_log{endwith}')\n",
    "        log_dict = {}\n",
    "        log_dict['model'] = model_dict['name']\n",
    "        log_dict['starting_lr'] = lr_start\n",
    "        log_dict['final_lr'] = optimizer.param_groups[-1][\"lr\"]\n",
    "        log_dict['history_interval'] = history_interval\n",
    "        log_dict['lr_decay_step'] = lr_decay_step\n",
    "        log_dict['losses'] = losses\n",
    "        log_dict['train_acc_history'] = train_acc_history\n",
    "        log_dict['val_acc_history'] = val_acc_history\n",
    "        log_dict['best_test_accuracy'] = best_test_acc\n",
    "        log_dict['best_test_confusion'] = best_test_confusion\n",
    "        log_dict['best_test_debug'] = best_test_debug\n",
    "        log_dict['last_test_accuracy'] = last_test_acc\n",
    "        log_dict['last_test_confusion'] = last_test_confusion\n",
    "        log_dict['last_test_debug'] = last_test_debug\n",
    "        torch.save(log_dict, path)\n",
    "\n",
    "        script = f'- {r:02d} train accuracy {train_acc:.2f}%, '\\\n",
    "        f'best / last test accuracies {best_test_acc:.2f}% / {last_test_acc:.2f}%'\n",
    "        print()\n",
    "        print(script)\n",
    "        print()\n",
    "    \n",
    "    if draw_result and save_model:\n",
    "        model = model_dict['generator']().to(device)\n",
    "        path = os.path.join(log_path, f'{model_dict[\"name\"]}')\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        temp_result = check_test_accuracy(model, repeat=30)\n",
    "        test_acc, test_confusion, test_debug, score, target = temp_result\n",
    "        draw_roc_curve(score, target)\n",
    "        draw_confusion(test_confusion)\n",
    "        print('\\n' * 2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
