{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "PyTorch의 EEG 데이터를 Dataset class 및 DataLoader class로 처리해보는 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# custom package\n",
    "from utils.eeg_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n",
    "#  'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n",
    "#  'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n",
    "#  'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', \n",
    "#  'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic' # for Hangul in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.10.1+cu113\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "root_path = r'dataset/02_Curated_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 78,\n",
      " 'birth': '1940-06-02',\n",
      " 'dx1': 'mci_rf',\n",
      " 'edfname': '00001809_261018',\n",
      " 'events': [[0, 'Start Recording'],\n",
      "            [0, 'New Montage - Montage 002'],\n",
      "            [36396, 'Eyes Open'],\n",
      "            [72518, 'Eyes Closed'],\n",
      "            [73862, 'Eyes Open'],\n",
      "            [75248, 'Eyes Closed'],\n",
      "            [76728, 'swallowing'],\n",
      "            [77978, 'Eyes Open'],\n",
      "            [79406, 'Eyes Closed'],\n",
      "            [79996, 'Photic On - 3.0 Hz'],\n",
      "            [80288, 'Eyes Open'],\n",
      "            [81296, 'Eyes Closed'],\n",
      "            [82054, 'Photic Off'],\n",
      "            [84070, 'Photic On - 6.0 Hz'],\n",
      "            [84488, 'Eyes Open'],\n",
      "            [85538, 'Eyes Closed'],\n",
      "            [86086, 'Photic Off'],\n",
      "            [88144, 'Photic On - 9.0 Hz'],\n",
      "            [90160, 'Photic Off'],\n",
      "            [91458, 'Eyes Open'],\n",
      "            [92218, 'Photic On - 12.0 Hz'],\n",
      "            [92762, 'Eyes Closed'],\n",
      "            [94198, 'Photic Off'],\n",
      "            [94742, 'Eyes Open'],\n",
      "            [95708, 'Eyes Closed'],\n",
      "            [96256, 'Photic On - 15.0 Hz'],\n",
      "            [98272, 'Photic Off'],\n",
      "            [100330, 'Photic On - 18.0 Hz'],\n",
      "            [102346, 'Photic Off'],\n",
      "            [102596, 'Eyes Open'],\n",
      "            [103856, 'Eyes Closed'],\n",
      "            [104361, 'Photic On - 21.0 Hz'],\n",
      "            [106420, 'Photic Off'],\n",
      "            [106880, 'Eyes Open'],\n",
      "            [107804, 'Eyes Closed'],\n",
      "            [108435, 'Photic On - 24.0 Hz'],\n",
      "            [110452, 'Photic Off'],\n",
      "            [111080, 'Eyes Open'],\n",
      "            [112004, 'Eyes Closed'],\n",
      "            [112509, 'Photic On - 27.0 Hz'],\n",
      "            [114528, 'Photic Off'],\n",
      "            [114864, 'Eyes Open'],\n",
      "            [116124, 'Eyes Closed'],\n",
      "            [116544, 'Photic On - 30.0 Hz'],\n",
      "            [118602, 'Photic Off'],\n",
      "            [126672, 'artifact'],\n",
      "            [134030, 'Move'],\n",
      "            [135584, 'Eyes Open'],\n",
      "            [136668, 'Eyes Closed'],\n",
      "            [139818, 'Eyes Open'],\n",
      "            [141414, 'Eyes Closed'],\n",
      "            [145000, 'Paused']],\n",
      " 'label': ['mci', 'mci_amnestic', 'mci_amnestic_rf'],\n",
      " 'record': '2018-10-26T15:46:26',\n",
      " 'serial': '00001'}\n"
     ]
    }
   ],
   "source": [
    "meta_path = os.path.join(root_path, 'metadata_debug.json')\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label_to_type: ['Normal', 'Non-vascular MCI', 'Non-vascular dementia']\n"
     ]
    }
   ],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        # inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 463 data belonging to Normal\n",
      "- There are 347 data belonging to Non-vascular MCI\n",
      "- There are 229 data belonging to Non-vascular dementia\n"
     ]
    }
   ],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size\t\t: 831\n",
      "Validation data size\t: 104\n",
      "Test data size\t\t: 104\n",
      "\n",
      " --- Recheck --- \n",
      "\n",
      "Train data label distribution\t: [370 278 183] 831\n",
      "Val data label distribution\t: [46 35 23] 104\n",
      "Test data label distribution\t: [47 34 23] 104\n"
     ]
    }
   ],
   "source": [
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Test TorchVision Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-44. -41. -34.]\n",
      " [-10. -11. -11.]\n",
      " [ 17.   6. -14.]\n",
      " [ 36.  34.  14.]\n",
      " [ 21.  34.  35.]\n",
      " [  4.   9.  26.]\n",
      " [-24. -28. -18.]\n",
      " [ 13.  -9. -24.]\n",
      " [ 21.  19.  15.]\n",
      " [  8.  18.  24.]\n",
      " [-21. -10.   3.]\n",
      " [-20.  -7.   5.]\n",
      " [  3.  20.  28.]\n",
      " [-28. -21.  -2.]\n",
      " [-30. -20.   0.]\n",
      " [-16.  -4.  12.]\n",
      " [-27. -37. -34.]\n",
      " [ 26.   0. -28.]\n",
      " [ 29.  17.  -3.]\n",
      " [ 45.  40.  32.]\n",
      " [724. 652. 429.]]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[[  8.   4.   3.]\n",
      " [  2.   1.   1.]\n",
      " [  3.   7.  10.]\n",
      " [  4.   9.  11.]\n",
      " [ -7.  -6.  -5.]\n",
      " [ -5.  -9. -13.]\n",
      " [  2.   0.  -1.]\n",
      " [ -3.   5.  12.]\n",
      " [ -8.  -4.  -3.]\n",
      " [ -3.  -5.  -6.]\n",
      " [  1.  -3.  -6.]\n",
      " [  8.   3.  -3.]\n",
      " [-15. -20. -21.]\n",
      " [ 13.   4.  -1.]\n",
      " [  0. -10. -19.]\n",
      " [  4.  -5. -13.]\n",
      " [  0.   7.  12.]\n",
      " [ -1.  11.  18.]\n",
      " [ -1.   8.  12.]\n",
      " [102.  90.  81.]\n",
      " [ -1.   0.  -1.]]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    dataset = EEGDataset(root_path, metadata_train, EEGRandomCrop(crop_length=3))\n",
    "    print(dataset[0]['signal'])\n",
    "    print('\\n')\n",
    "    print('-' * 100)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 80.,  87., 104.],\n",
      "       [ 38.,  40.,  53.],\n",
      "       [  2.,   1.,  -6.],\n",
      "       [ -2.,  -4.,  -1.],\n",
      "       [ 33.,  35.,  36.],\n",
      "       [162., 203., 277.],\n",
      "       [-25., -19.,   3.],\n",
      "       [ 20.,  17.,   7.],\n",
      "       [ -3.,  -2.,  -3.],\n",
      "       [ -8.,  -6.,  -9.],\n",
      "       [-20., -18., -18.],\n",
      "       [-26., -24., -28.],\n",
      "       [-37., -35., -35.],\n",
      "       [-14., -11.,  -4.],\n",
      "       [-15., -19., -23.],\n",
      "       [ -6.,  -2.,  -2.],\n",
      "       [ 43.,  45.,  37.],\n",
      "       [ 19.,  11.,   4.],\n",
      "       [  1.,  -9., -12.],\n",
      "       [ 20.,  18.,  22.],\n",
      "       [  2.,   2.,   0.]], dtype=float32), array([[ 21.,  21.,  17.],\n",
      "       [ 16.,  14.,  10.],\n",
      "       [  2.,  -4.,  -5.],\n",
      "       [ -1.,  -3.,  -2.],\n",
      "       [-10., -10.,  -8.],\n",
      "       [ 16.,  21.,  21.],\n",
      "       [  2.,   3.,   2.],\n",
      "       [  2.,   4.,   3.],\n",
      "       [  2.,   4.,   6.],\n",
      "       [ -6.,  -4.,  -2.],\n",
      "       [  1.,   0.,  -4.],\n",
      "       [  8.,   4.,   2.],\n",
      "       [ 11.,  11.,  11.],\n",
      "       [-14.,  -8.,  -4.],\n",
      "       [ -5.,  -1.,   3.],\n",
      "       [ -4.,  -1.,   1.],\n",
      "       [ 12.,  11.,   8.],\n",
      "       [ -5.,  -7.,  -7.],\n",
      "       [-12., -14., -12.],\n",
      "       [-46., -44., -46.],\n",
      "       [ -1.,  -1.,   3.]], dtype=float32)]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[array([[-12., -15., -16.],\n",
      "       [ -4., -12., -15.],\n",
      "       [ -4., -11., -13.],\n",
      "       [ -3.,   0.,   3.],\n",
      "       [  0.,   5.,   7.],\n",
      "       [-33., -31., -28.],\n",
      "       [ -1.,  -1.,  -1.],\n",
      "       [  3.,   8.,  10.],\n",
      "       [  1.,   4.,   4.],\n",
      "       [ -4.,  -4.,  -5.],\n",
      "       [ -2.,  -7.,  -9.],\n",
      "       [  2.,  -6.,  -9.],\n",
      "       [ 12.,  16.,  19.],\n",
      "       [-10., -12., -15.],\n",
      "       [ 19.,  16.,  13.],\n",
      "       [  8.,   6.,   1.],\n",
      "       [ -9.,  -7.,  -4.],\n",
      "       [ -4.,   2.,   6.],\n",
      "       [ -4.,   3.,   9.],\n",
      "       [ 27.,  13.,  16.],\n",
      "       [  0.,   0.,   0.]], dtype=float32), array([[ -2.,  -1.,   0.],\n",
      "       [ 12.,  12.,  15.],\n",
      "       [  0.,  -1.,   1.],\n",
      "       [-12., -15., -12.],\n",
      "       [-15., -16., -15.],\n",
      "       [ 28.,  30.,  30.],\n",
      "       [  7.,  10.,  10.],\n",
      "       [  0.,  -1.,  -6.],\n",
      "       [-15., -18., -20.],\n",
      "       [ -9., -11., -13.],\n",
      "       [ 17.,  19.,  19.],\n",
      "       [ 11.,  13.,  14.],\n",
      "       [  0.,   1.,   3.],\n",
      "       [  1.,   5.,   4.],\n",
      "       [ 19.,  20.,  18.],\n",
      "       [  4.,   4.,   1.],\n",
      "       [ -1.,   3.,   4.],\n",
      "       [ -5.,  -8.,  -8.],\n",
      "       [-13., -17., -16.],\n",
      "       [-30., -29., -26.],\n",
      "       [  0.,  -1.,  -1.]], dtype=float32)]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    dataset = EEGDataset(root_path, metadata_train, EEGRandomCrop(crop_length=3, multiple=2))\n",
    "    print(dataset[0]['signal'])\n",
    "    print('\\n')\n",
    "    print('-' * 100)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization per signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'signal': array([[ 8.8514559e-02,  6.4116552e-02,  4.4598151e-02, ...,\n",
      "         9.8273762e-02,  5.4357354e-02,  2.9959347e-02],\n",
      "       [ 2.6334402e-01,  4.3795574e-01,  3.2154793e-01, ...,\n",
      "        -5.8061254e-01, -6.3881642e-01, -6.0971451e-01],\n",
      "       [ 1.2442030e+00,  1.3330292e+00,  1.3330292e+00, ...,\n",
      "        -7.9879951e-01, -7.0997328e-01, -3.5466847e-01],\n",
      "       ...,\n",
      "       [ 4.0791610e-01,  7.3437029e-01,  9.7921097e-01, ...,\n",
      "         8.9759737e-01,  1.1424381e+00,  1.1424381e+00],\n",
      "       [-1.6785289e-01, -1.6109554e-02, -7.2015002e-02, ...,\n",
      "        -1.4389342e-01, -1.5187991e-01, -1.6785289e-01],\n",
      "       [ 8.5052430e-05,  5.2320277e-03,  1.0379002e-02, ...,\n",
      "         8.5052430e-05,  8.5052430e-05, -5.0619226e-03]], dtype=float32), 'age': 59, 'class_label': 2, 'metadata': {'serial': '00130', 'edfname': '00430821_031116', 'birth': '1957-08-25', 'record': '2016-11-03T09:47:13', 'age': 59, 'dx1': 'eoad', 'label': ['dementia', 'ad', 'eoad'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1214, 'Eyes Open'], [3320, 'Eyes Closed'], [14869, 'Eyes Open'], [15919, 'Eyes Closed'], [19910, 'Eyes Open'], [21128, 'Eyes Closed'], [27470, 'HV - Dur: 157.3 sec. - On'], [40070, 'Eyes Open'], [41330, 'Eyes Closed'], [57336, 'Eyes Open'], [58386, 'Eyes Closed'], [58932, 'HV - Off'], [59141, 'HV fair'], [60779, 'Eyes Open'], [61788, 'Eyes Closed'], [64979, 'Eyes Open'], [66492, 'Eyes Closed'], [69558, 'Photic On - 3.0 Hz'], [69768, 'Eyes Open'], [70860, 'Eyes Closed'], [71574, 'Photic Off'], [73632, 'Photic On - 6.0 Hz'], [74052, 'Eyes Open'], [75144, 'Eyes Closed'], [75648, 'Photic Off'], [77664, 'Photic On - 9.0 Hz'], [79680, 'Photic Off'], [79890, 'Eyes Open'], [80898, 'Eyes Closed'], [81738, 'Photic On - 12.0 Hz'], [83754, 'Photic Off'], [85816, 'Photic On - 15.0 Hz'], [87832, 'Photic Off'], [88124, 'Eyes Open'], [89048, 'Eyes Closed'], [89848, 'Photic On - 18.0 Hz'], [91864, 'Photic Off'], [93922, 'Photic On - 21.0 Hz'], [95938, 'Photic Off'], [97996, 'Photic On - 24.0 Hz'], [100012, 'Photic Off'], [100682, 'Eyes Open'], [101438, 'Eyes Closed'], [102028, 'Photic On - 27.0 Hz'], [104046, 'Photic Off'], [106104, 'Photic On - 30.0 Hz'], [108120, 'Photic Off'], [112992, 'Eyes Open'], [113874, 'Eyes Closed'], [146676, 'Eyes Open'], [147390, 'Eyes Closed'], [173800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Mean: [ 3.95077970e-10  2.54605803e-09 -1.19840315e-08 -1.33887534e-09\n",
      " -1.37399336e-08  2.80944334e-09 -1.97538985e-09 -5.26770627e-09\n",
      "  9.24043508e-09 -2.94113600e-09 -9.30628108e-09 -3.62154806e-10\n",
      " -7.81376386e-09  1.49251678e-09  1.10182858e-08  3.95077970e-09\n",
      " -4.21416502e-09  6.62853017e-09  1.28180853e-08 -2.19487761e-11\n",
      " -4.10579304e-09]\n",
      "Std: [1.         0.99999994 1.         0.9999999  0.9999998  0.9999999\n",
      " 1.0000001  1.         0.9999999  1.         1.         1.\n",
      " 1.         1.         1.0000001  0.99999994 0.99999994 0.99999994\n",
      " 1.         1.0000001  0.9999999 ]\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, EEGNormalizePerSignal())\n",
    "print(dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('Mean:', np.mean(dataset[0]['signal'], axis=1))\n",
    "print('Std:', np.std(dataset[0]['signal'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age mean and standard deviation:\t 70.26594464500602 ,\t 9.855772571874425\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "before:\n",
      "59\n",
      "64\n",
      "62\n",
      "76\n",
      "88\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after:\n",
      "-1.1430808240974444\n",
      "-0.6357639234218535\n",
      "-0.8386906836920899\n",
      "0.5817966381995646\n",
      "1.7993571998209827\n"
     ]
    }
   ],
   "source": [
    "ages = []\n",
    "for m in metadata_train:\n",
    "    ages.append(m['age'])\n",
    "\n",
    "ages = np.array(ages)\n",
    "age_mean = np.mean(ages)\n",
    "age_std = np.std(ages)\n",
    "\n",
    "print('Age mean and standard deviation:\\t', age_mean, ',\\t', age_std)\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('before:')\n",
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "for i in range(5):\n",
    "    print(dataset[i]['age'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('after:')\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGNormalizeAge(mean=age_mean, std=age_std))\n",
    "for i in range(5):\n",
    "    print(dataset[i]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop EKG channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (21, 173800)\n",
      "[[ 18.  13.   9. ...  20.  11.   6.]\n",
      " [  9.  15.  11. ... -20. -22. -21.]\n",
      " [ 14.  15.  15. ...  -9.  -8.  -4.]\n",
      " ...\n",
      " [  5.   9.  12. ...  11.  14.  14.]\n",
      " [-21.  -2.  -9. ... -18. -19. -21.]\n",
      " [  0.   1.   2. ...   0.   0.  -1.]]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after: (20, 173800)\n",
      "[[ 18.  13.   9. ...  20.  11.   6.]\n",
      " [  9.  15.  11. ... -20. -22. -21.]\n",
      " [ 14.  15.  15. ...  -9.  -8.  -4.]\n",
      " ...\n",
      " [  2.   0.   1. ...   0.   1.   4.]\n",
      " [  5.   9.  12. ...  11.  14.  14.]\n",
      " [  0.   1.   2. ...   0.   0.  -1.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "print('before:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGDropEKGChannel())\n",
    "print('after:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop photic stimulation channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (21, 173800)\n",
      "[[ 18.  13.   9. ...  20.  11.   6.]\n",
      " [  9.  15.  11. ... -20. -22. -21.]\n",
      " [ 14.  15.  15. ...  -9.  -8.  -4.]\n",
      " ...\n",
      " [  5.   9.  12. ...  11.  14.  14.]\n",
      " [-21.  -2.  -9. ... -18. -19. -21.]\n",
      " [  0.   1.   2. ...   0.   0.  -1.]]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after: (20, 173800)\n",
      "[[ 18.  13.   9. ...  20.  11.   6.]\n",
      " [  9.  15.  11. ... -20. -22. -21.]\n",
      " [ 14.  15.  15. ...  -9.  -8.  -4.]\n",
      " ...\n",
      " [  2.   0.   1. ...   0.   1.   4.]\n",
      " [  5.   9.  12. ...  11.  14.  14.]\n",
      " [-21.  -2.  -9. ... -18. -19. -21.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "print('before:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGDropPhoticChannel())\n",
    "print('after:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "{'signal': array([[ 18.,  13.,   9., ...,  20.,  11.,   6.],\n",
      "       [  9.,  15.,  11., ..., -20., -22., -21.],\n",
      "       [ 14.,  15.,  15., ...,  -9.,  -8.,  -4.],\n",
      "       ...,\n",
      "       [  5.,   9.,  12., ...,  11.,  14.,  14.],\n",
      "       [-21.,  -2.,  -9., ..., -18., -19., -21.],\n",
      "       [  0.,   1.,   2., ...,   0.,   0.,  -1.]], dtype=float32), 'age': 59, 'class_label': 2, 'metadata': {'serial': '00130', 'edfname': '00430821_031116', 'birth': '1957-08-25', 'record': '2016-11-03T09:47:13', 'age': 59, 'dx1': 'eoad', 'label': ['dementia', 'ad', 'eoad'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1214, 'Eyes Open'], [3320, 'Eyes Closed'], [14869, 'Eyes Open'], [15919, 'Eyes Closed'], [19910, 'Eyes Open'], [21128, 'Eyes Closed'], [27470, 'HV - Dur: 157.3 sec. - On'], [40070, 'Eyes Open'], [41330, 'Eyes Closed'], [57336, 'Eyes Open'], [58386, 'Eyes Closed'], [58932, 'HV - Off'], [59141, 'HV fair'], [60779, 'Eyes Open'], [61788, 'Eyes Closed'], [64979, 'Eyes Open'], [66492, 'Eyes Closed'], [69558, 'Photic On - 3.0 Hz'], [69768, 'Eyes Open'], [70860, 'Eyes Closed'], [71574, 'Photic Off'], [73632, 'Photic On - 6.0 Hz'], [74052, 'Eyes Open'], [75144, 'Eyes Closed'], [75648, 'Photic Off'], [77664, 'Photic On - 9.0 Hz'], [79680, 'Photic Off'], [79890, 'Eyes Open'], [80898, 'Eyes Closed'], [81738, 'Photic On - 12.0 Hz'], [83754, 'Photic Off'], [85816, 'Photic On - 15.0 Hz'], [87832, 'Photic Off'], [88124, 'Eyes Open'], [89048, 'Eyes Closed'], [89848, 'Photic On - 18.0 Hz'], [91864, 'Photic Off'], [93922, 'Photic On - 21.0 Hz'], [95938, 'Photic Off'], [97996, 'Photic On - 24.0 Hz'], [100012, 'Photic Off'], [100682, 'Eyes Open'], [101438, 'Eyes Closed'], [102028, 'Photic On - 27.0 Hz'], [104046, 'Photic Off'], [106104, 'Photic On - 30.0 Hz'], [108120, 'Photic Off'], [112992, 'Eyes Open'], [113874, 'Eyes Closed'], [146676, 'Eyes Open'], [147390, 'Eyes Closed'], [173800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after:\n",
      "{'signal': tensor([[ 18.,  13.,   9.,  ...,  20.,  11.,   6.],\n",
      "        [  9.,  15.,  11.,  ..., -20., -22., -21.],\n",
      "        [ 14.,  15.,  15.,  ...,  -9.,  -8.,  -4.],\n",
      "        ...,\n",
      "        [  5.,   9.,  12.,  ...,  11.,  14.,  14.],\n",
      "        [-21.,  -2.,  -9.,  ..., -18., -19., -21.],\n",
      "        [  0.,   1.,   2.,  ...,   0.,   0.,  -1.]]), 'age': tensor(59.), 'class_label': tensor(2), 'metadata': {'serial': '00130', 'edfname': '00430821_031116', 'birth': '1957-08-25', 'record': '2016-11-03T09:47:13', 'age': 59, 'dx1': 'eoad', 'label': ['dementia', 'ad', 'eoad'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1214, 'Eyes Open'], [3320, 'Eyes Closed'], [14869, 'Eyes Open'], [15919, 'Eyes Closed'], [19910, 'Eyes Open'], [21128, 'Eyes Closed'], [27470, 'HV - Dur: 157.3 sec. - On'], [40070, 'Eyes Open'], [41330, 'Eyes Closed'], [57336, 'Eyes Open'], [58386, 'Eyes Closed'], [58932, 'HV - Off'], [59141, 'HV fair'], [60779, 'Eyes Open'], [61788, 'Eyes Closed'], [64979, 'Eyes Open'], [66492, 'Eyes Closed'], [69558, 'Photic On - 3.0 Hz'], [69768, 'Eyes Open'], [70860, 'Eyes Closed'], [71574, 'Photic Off'], [73632, 'Photic On - 6.0 Hz'], [74052, 'Eyes Open'], [75144, 'Eyes Closed'], [75648, 'Photic Off'], [77664, 'Photic On - 9.0 Hz'], [79680, 'Photic Off'], [79890, 'Eyes Open'], [80898, 'Eyes Closed'], [81738, 'Photic On - 12.0 Hz'], [83754, 'Photic Off'], [85816, 'Photic On - 15.0 Hz'], [87832, 'Photic Off'], [88124, 'Eyes Open'], [89048, 'Eyes Closed'], [89848, 'Photic On - 18.0 Hz'], [91864, 'Photic Off'], [93922, 'Photic On - 21.0 Hz'], [95938, 'Photic Off'], [97996, 'Photic On - 24.0 Hz'], [100012, 'Photic Off'], [100682, 'Eyes Open'], [101438, 'Eyes Closed'], [102028, 'Photic On - 27.0 Hz'], [104046, 'Photic Off'], [106104, 'Photic On - 30.0 Hz'], [108120, 'Photic Off'], [112992, 'Eyes Open'], [113874, 'Eyes Closed'], [146676, 'Eyes Open'], [147390, 'Eyes Closed'], [173800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "print('before:')\n",
    "print(dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGToTensor())\n",
    "print('after:')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short time Fourier transform (STFT or spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 101, 3477]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[-2.8540e+03, -2.5239e+02, -1.3384e+02,  ...,  4.9411e+00,\n",
      "          9.5575e+00,  1.2000e+01],\n",
      "        [-4.7620e+03, -1.0710e+02, -2.0297e+02,  ...,  5.1596e+00,\n",
      "          2.4075e+00,  8.0000e+00],\n",
      "        [-2.0290e+03, -5.7747e+01, -9.6218e+01,  ..., -8.3456e+00,\n",
      "         -9.1753e+00, -3.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  3.8263e+02, -6.8193e+01,  ..., -2.0900e-01,\n",
      "          5.2983e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  4.1983e+03,  6.2755e+03,  ..., -2.8503e+00,\n",
      "         -7.9858e-01,  0.0000e+00],\n",
      "        [ 0.0000e+00, -2.1638e+00, -1.5501e+01,  ...,  1.4234e+00,\n",
      "          1.2831e+00,  0.0000e+00]])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([21, 101, 3477]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[2.8540e+03, 6.9932e+02, 1.3459e+02,  ..., 4.9506e+00, 9.5628e+00,\n",
      "         1.2000e+01],\n",
      "        [4.7620e+03, 3.2330e+02, 2.6337e+02,  ..., 5.3616e+00, 4.8672e+00,\n",
      "         8.0000e+00],\n",
      "        [2.0290e+03, 1.8884e+02, 1.5219e+02,  ..., 8.3816e+00, 9.2068e+00,\n",
      "         3.0000e+00],\n",
      "        ...,\n",
      "        [1.3060e+03, 4.3429e+02, 8.6072e+01,  ..., 6.4274e+00, 7.1433e+00,\n",
      "         4.0000e+00],\n",
      "        [9.8500e+02, 4.5018e+03, 6.2894e+03,  ..., 1.5706e+01, 1.0020e+01,\n",
      "         1.1000e+01],\n",
      "        [1.4000e+01, 2.2332e+00, 2.0151e+01,  ..., 1.4276e+00, 1.9705e+00,\n",
      "         2.0000e+00]])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([21, 101, 3477]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[-2.8540e+03, -2.5239e+02, -1.3384e+02,  ...,  4.9411e+00,\n",
      "          9.5575e+00,  1.2000e+01],\n",
      "        [-4.7620e+03, -1.0710e+02, -2.0297e+02,  ...,  5.1596e+00,\n",
      "          2.4075e+00,  8.0000e+00],\n",
      "        [-2.0290e+03, -5.7747e+01, -9.6218e+01,  ..., -8.3456e+00,\n",
      "         -9.1753e+00, -3.0000e+00],\n",
      "        ...,\n",
      "        [-1.3060e+03,  2.0543e+02, -5.2518e+01,  ..., -6.4240e+00,\n",
      "         -4.7911e+00, -4.0000e+00],\n",
      "        [ 9.8500e+02,  1.6251e+03, -4.1746e+02,  ..., -1.5445e+01,\n",
      "         -9.9878e+00, -1.1000e+01],\n",
      "        [-1.4000e+01,  5.5256e-01,  1.2876e+01,  ...,  1.0934e-01,\n",
      "         -1.4955e+00, -2.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='as_real')])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='power')])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='remove')])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose some at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 12000])\n",
      "{'signal': tensor([[-4.2218e-02, -1.3023e-02, -7.1412e-02,  ..., -6.8450e-01,\n",
      "         -7.1369e-01, -5.6772e-01],\n",
      "        [-4.2921e-01, -4.8261e-01, -6.4281e-01,  ..., -8.0301e-01,\n",
      "         -8.0301e-01, -5.8941e-01],\n",
      "        [ 7.5856e-03, -6.9897e-01, -1.4055e+00,  ...,  7.5856e-03,\n",
      "          4.7862e-01,  3.6086e-01],\n",
      "        ...,\n",
      "        [ 2.3564e+00,  1.6056e+00,  7.4754e-01,  ..., -1.1050e-01,\n",
      "          8.5480e-01,  8.5480e-01],\n",
      "        [ 3.3584e+00,  2.9515e+00,  2.3410e+00,  ..., -9.1455e-01,\n",
      "          1.0852e-03, -1.0065e-01],\n",
      "        [ 1.5178e+00,  1.4114e+00,  1.3213e+00,  ...,  7.0717e-01,\n",
      "          1.1248e+00,  1.4851e+00]]), 'age': tensor(-1.1431), 'class_label': tensor(2), 'metadata': {'serial': '00130', 'edfname': '00430821_031116', 'birth': '1957-08-25', 'record': '2016-11-03T09:47:13', 'age': 59, 'dx1': 'eoad', 'label': ['dementia', 'ad', 'eoad'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1214, 'Eyes Open'], [3320, 'Eyes Closed'], [14869, 'Eyes Open'], [15919, 'Eyes Closed'], [19910, 'Eyes Open'], [21128, 'Eyes Closed'], [27470, 'HV - Dur: 157.3 sec. - On'], [40070, 'Eyes Open'], [41330, 'Eyes Closed'], [57336, 'Eyes Open'], [58386, 'Eyes Closed'], [58932, 'HV - Off'], [59141, 'HV fair'], [60779, 'Eyes Open'], [61788, 'Eyes Closed'], [64979, 'Eyes Open'], [66492, 'Eyes Closed'], [69558, 'Photic On - 3.0 Hz'], [69768, 'Eyes Open'], [70860, 'Eyes Closed'], [71574, 'Photic Off'], [73632, 'Photic On - 6.0 Hz'], [74052, 'Eyes Open'], [75144, 'Eyes Closed'], [75648, 'Photic Off'], [77664, 'Photic On - 9.0 Hz'], [79680, 'Photic Off'], [79890, 'Eyes Open'], [80898, 'Eyes Closed'], [81738, 'Photic On - 12.0 Hz'], [83754, 'Photic Off'], [85816, 'Photic On - 15.0 Hz'], [87832, 'Photic Off'], [88124, 'Eyes Open'], [89048, 'Eyes Closed'], [89848, 'Photic On - 18.0 Hz'], [91864, 'Photic Off'], [93922, 'Photic On - 21.0 Hz'], [95938, 'Photic Off'], [97996, 'Photic On - 24.0 Hz'], [100012, 'Photic Off'], [100682, 'Eyes Open'], [101438, 'Eyes Closed'], [102028, 'Photic On - 27.0 Hz'], [104046, 'Photic Off'], [106104, 'Photic On - 30.0 Hz'], [108120, 'Photic Off'], [112992, 'Eyes Open'], [113874, 'Eyes Closed'], [146676, 'Eyes Open'], [147390, 'Eyes Closed'], [173800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 12000])\n",
      "{'signal': tensor([[ 0.6673,  0.5770,  0.5770,  ..., -0.1002, -0.1453,  0.0804],\n",
      "        [ 1.0894,  0.9251,  0.9251,  ..., -0.9640, -0.7998, -0.5534],\n",
      "        [ 0.0770, -0.0338,  0.0770,  ...,  0.8526,  1.0742,  1.1850],\n",
      "        ...,\n",
      "        [ 0.3132,  0.4213,  0.5295,  ..., -1.6334, -1.6334, -1.4171],\n",
      "        [ 0.0059,  0.0059,  0.0059,  ..., -1.0529, -0.9016, -0.7504],\n",
      "        [-0.3402, -0.5531, -0.6032,  ...,  0.0981,  0.1357,  0.1232]]), 'age': tensor(0.0745), 'class_label': tensor(1), 'metadata': {'serial': '01148', 'edfname': '01286604_220218', 'birth': '1946-11-17', 'record': '2018-02-22T09:57:41', 'age': 71, 'dx1': 'mci encoding failure', 'label': ['mci', 'mci_amnestic', 'mci_amnestic_ef'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [6336, 'Eyes Closed'], [12258, 'Eyes Open'], [18180, 'Eyes Closed'], [23988, 'Eyes Open'], [29745, 'Eyes Closed'], [36172, 'Eyes Open'], [52678, 'Eyes Closed'], [60238, 'Eyes Open'], [66160, 'Eyes Closed'], [78172, 'Eyes Open'], [83926, 'Eyes Closed'], [90940, 'Eyes Open'], [95182, 'Eyes Closed'], [102196, 'Eyes Open'], [108874, 'Eyes Closed'], [113914, 'Eyes Open'], [119290, 'Eyes Closed'], [183000, 'Paused']], 'class_type': 'Non-vascular MCI', 'class_label': 1}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 12000])\n",
      "{'signal': tensor([[ 0.2443,  0.2766,  0.2766,  ...,  0.4381,  0.4381,  0.4381],\n",
      "        [ 1.5417,  1.6935,  1.7695,  ...,  1.1619,  1.1619,  1.2379],\n",
      "        [ 0.8977,  0.8977,  1.0552,  ..., -0.3621, -0.3621, -0.3621],\n",
      "        ...,\n",
      "        [ 1.1830,  1.1830,  1.1830,  ...,  1.8402,  1.8402,  1.8402],\n",
      "        [ 0.5440,  0.6786,  0.8132,  ..., -0.9369, -1.0715, -1.0715],\n",
      "        [ 0.4397,  0.4306,  0.4672,  ..., -0.1831, -0.3114, -0.3480]]), 'age': tensor(0.2774), 'class_label': tensor(2), 'metadata': {'serial': '00018', 'edfname': '00128526_180817', 'birth': '1944-02-11', 'record': '2017-08-18T09:45:06', 'age': 73, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1340, 'Eyes Open'], [5456, 'Eyes Closed'], [12480, 'Eyes Open'], [19962, 'Eyes Closed'], [26100, 'Eyes Open'], [30300, 'Eyes Closed'], [48360, 'Eyes Open'], [54576, 'Eyes Closed'], [60834, 'Eyes Open'], [66000, 'Eyes Closed'], [82716, 'Eyes Open'], [89354, 'Eyes Closed'], [97628, 'Eyes Open'], [103172, 'Eyes Closed'], [109009, 'Eyes Open'], [112790, 'Eyes Closed'], [121402, 'Photic On - 3.0 Hz'], [121861, 'Eyes Open'], [122492, 'Eyes Closed'], [123418, 'Photic Off'], [125476, 'Photic On - 6.0 Hz'], [127492, 'Photic Off'], [128078, 'Eyes Open'], [128750, 'Eyes Closed'], [129508, 'Photic On - 9.0 Hz'], [131524, 'Photic Off'], [133582, 'Photic On - 12.0 Hz'], [135598, 'Photic Off'], [136016, 'Eyes Open'], [136940, 'Eyes Closed'], [137656, 'Photic On - 15.0 Hz'], [139672, 'Photic Off'], [141688, 'Photic On - 18.0 Hz'], [143746, 'Photic Off'], [145762, 'Photic On - 21.0 Hz'], [147778, 'Photic Off'], [148196, 'Eyes Open'], [148994, 'Eyes Closed'], [149836, 'Photic On - 24.0 Hz'], [151852, 'Photic Off'], [153868, 'Photic On - 27.0 Hz'], [155926, 'Photic Off'], [157942, 'Photic On - 30.0 Hz'], [159958, 'Photic Off'], [168692, 'Eyes Open'], [169490, 'Eyes Closed'], [175800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=200*60), # 1 minute\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGToTensor()])\n",
    "\n",
    "train_dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "val_dataset = EEGDataset(root_path, metadata_val, composed)\n",
    "test_dataset = EEGDataset(root_path, metadata_test, composed)\n",
    "\n",
    "print(train_dataset[0]['signal'].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0]['signal'].shape)\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0]['signal'].shape)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4000])\n",
      "tensor([-9.5367e-10,  2.0027e-08,  5.7220e-09, -3.8147e-09,  4.0531e-09,\n",
      "         5.9605e-09,  2.8610e-09,  6.7949e-09, -8.3447e-09,  6.4373e-09,\n",
      "         1.2875e-08,  7.3910e-09,  1.2994e-08, -3.3379e-09, -1.4067e-08,\n",
      "         1.4156e-08, -1.2875e-08, -5.9605e-09,  9.5367e-09,  4.5300e-09])\n",
      "tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 4000])\n",
      "tensor([-6.5565e-09,  4.4107e-09, -1.3113e-09,  1.1921e-08,  7.1526e-09,\n",
      "        -5.3644e-09, -4.5896e-09, -8.5831e-09, -2.1458e-09,  1.4305e-08,\n",
      "        -5.2452e-09, -1.2636e-08,  6.6757e-09, -7.7486e-10,  6.6757e-09,\n",
      "        -1.4782e-08, -5.6028e-09,  6.0797e-09, -6.1989e-09, -2.9802e-09])\n",
      "tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 4000])\n",
      "{'signal': [tensor([[-0.9174, -0.9174, -1.0033,  ...,  1.2725,  1.0149,  0.7143],\n",
      "        [-0.3607, -0.4671, -0.6268,  ...,  0.9169,  0.5443,  0.3313],\n",
      "        [-0.9445, -1.0394, -0.5648,  ...,  0.7639,  0.2893,  0.5740],\n",
      "        ...,\n",
      "        [-2.1353, -1.8360, -0.9381,  ...,  1.0572,  0.5584,  0.1593],\n",
      "        [-1.8027, -1.1298, -0.0081,  ...,  0.4406,  0.6649,  0.7771],\n",
      "        [ 2.2982,  2.2413,  2.1762,  ..., -0.3765, -0.4497, -0.4741]]), tensor([[ 0.2593,  0.1659,  0.1192,  ...,  3.2001,  3.1068,  2.9667],\n",
      "        [-0.8292, -0.7328, -0.5881,  ..., -0.2024, -0.1541, -0.1059],\n",
      "        [-1.5720, -1.3929, -0.8557,  ..., -0.7661, -0.4080, -0.1394],\n",
      "        ...,\n",
      "        [-0.1683, -0.4917, -0.1683,  ..., -0.2761, -0.5996, -0.4917],\n",
      "        [-1.9314, -1.9314, -1.3335,  ...,  0.4600,  0.1013, -0.2574],\n",
      "        [-0.4089,  0.5887,  0.9861,  ...,  0.6049,  0.6049,  0.5562]])], 'age': tensor(-1.1431), 'class_label': tensor(2), 'metadata': {'serial': '00130', 'edfname': '00430821_031116', 'birth': '1957-08-25', 'record': '2016-11-03T09:47:13', 'age': 59, 'dx1': 'eoad', 'label': ['dementia', 'ad', 'eoad'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1214, 'Eyes Open'], [3320, 'Eyes Closed'], [14869, 'Eyes Open'], [15919, 'Eyes Closed'], [19910, 'Eyes Open'], [21128, 'Eyes Closed'], [27470, 'HV - Dur: 157.3 sec. - On'], [40070, 'Eyes Open'], [41330, 'Eyes Closed'], [57336, 'Eyes Open'], [58386, 'Eyes Closed'], [58932, 'HV - Off'], [59141, 'HV fair'], [60779, 'Eyes Open'], [61788, 'Eyes Closed'], [64979, 'Eyes Open'], [66492, 'Eyes Closed'], [69558, 'Photic On - 3.0 Hz'], [69768, 'Eyes Open'], [70860, 'Eyes Closed'], [71574, 'Photic Off'], [73632, 'Photic On - 6.0 Hz'], [74052, 'Eyes Open'], [75144, 'Eyes Closed'], [75648, 'Photic Off'], [77664, 'Photic On - 9.0 Hz'], [79680, 'Photic Off'], [79890, 'Eyes Open'], [80898, 'Eyes Closed'], [81738, 'Photic On - 12.0 Hz'], [83754, 'Photic Off'], [85816, 'Photic On - 15.0 Hz'], [87832, 'Photic Off'], [88124, 'Eyes Open'], [89048, 'Eyes Closed'], [89848, 'Photic On - 18.0 Hz'], [91864, 'Photic Off'], [93922, 'Photic On - 21.0 Hz'], [95938, 'Photic Off'], [97996, 'Photic On - 24.0 Hz'], [100012, 'Photic Off'], [100682, 'Eyes Open'], [101438, 'Eyes Closed'], [102028, 'Photic On - 27.0 Hz'], [104046, 'Photic Off'], [106104, 'Photic On - 30.0 Hz'], [108120, 'Photic Off'], [112992, 'Eyes Open'], [113874, 'Eyes Closed'], [146676, 'Eyes Open'], [147390, 'Eyes Closed'], [173800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "{'signal': [tensor([[-0.1071, -0.1071, -0.1373,  ..., -0.1825, -0.1976, -0.0316],\n",
      "        [-0.2993, -0.2993, -0.2993,  ...,  0.1436, -0.1148, -0.0410],\n",
      "        [ 1.2778,  1.5173,  1.7569,  ..., -0.3992, -0.3992, -0.7187],\n",
      "        ...,\n",
      "        [ 1.2870,  1.1517,  1.0615,  ...,  0.2949,  0.3400,  0.3851],\n",
      "        [-0.1513, -0.2698, -0.2698,  ..., -1.3361, -1.0991, -0.9806],\n",
      "        [-0.2896, -0.4642, -0.4642,  ...,  0.3217, -0.0027, -0.0526]]), tensor([[-0.2607, -0.2785, -0.2963,  ...,  0.2370,  0.3081,  0.1481],\n",
      "        [-0.5684, -0.5684, -0.5684,  ..., -0.4521, -0.4521, -0.2583],\n",
      "        [ 0.2289,  0.3274,  0.4259,  ..., -0.0666,  0.1304,  0.3274],\n",
      "        ...,\n",
      "        [-1.8307, -1.9342, -1.8307,  ..., -0.0724, -0.1758,  0.0311],\n",
      "        [-0.1241, -0.2516, -0.3791,  ..., -0.7618, -0.6342, -0.5067],\n",
      "        [-0.2017, -0.2639, -0.4256,  ..., -0.3137, -0.6247, -0.6495]])], 'age': tensor(0.0745), 'class_label': tensor(1), 'metadata': {'serial': '01148', 'edfname': '01286604_220218', 'birth': '1946-11-17', 'record': '2018-02-22T09:57:41', 'age': 71, 'dx1': 'mci encoding failure', 'label': ['mci', 'mci_amnestic', 'mci_amnestic_ef'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [6336, 'Eyes Closed'], [12258, 'Eyes Open'], [18180, 'Eyes Closed'], [23988, 'Eyes Open'], [29745, 'Eyes Closed'], [36172, 'Eyes Open'], [52678, 'Eyes Closed'], [60238, 'Eyes Open'], [66160, 'Eyes Closed'], [78172, 'Eyes Open'], [83926, 'Eyes Closed'], [90940, 'Eyes Open'], [95182, 'Eyes Closed'], [102196, 'Eyes Open'], [108874, 'Eyes Closed'], [113914, 'Eyes Open'], [119290, 'Eyes Closed'], [183000, 'Paused']], 'class_type': 'Non-vascular MCI', 'class_label': 1}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "{'signal': [tensor([[ 0.0675,  0.0466,  0.0466,  ..., -0.3927, -0.4555, -0.4764],\n",
      "        [-0.4968, -0.5531, -0.5531,  ..., -0.1593, -0.1593, -0.1593],\n",
      "        [-0.1485, -0.0228,  0.1029,  ..., -1.5316, -1.4059, -1.4059],\n",
      "        ...,\n",
      "        [ 0.7649,  0.7649,  0.9133,  ..., -1.4622, -1.3138, -1.3138],\n",
      "        [ 0.5182,  0.5182,  0.6500,  ..., -0.0090, -0.0090,  0.1228],\n",
      "        [ 0.4509,  0.6223,  0.9290,  ...,  0.3877,  0.4148,  0.4599]]), tensor([[-0.1805, -0.2248, -0.1805,  ...,  1.2819,  1.1933,  1.1933],\n",
      "        [-0.0479, -0.0479,  0.0418,  ...,  1.9257,  2.0154,  2.0154],\n",
      "        [ 0.4415,  0.2876,  0.4415,  ...,  1.3645,  1.5183,  1.5183],\n",
      "        ...,\n",
      "        [ 1.7872,  1.7872,  1.7872,  ...,  0.3499,  0.3499,  0.3499],\n",
      "        [ 0.2571,  0.2571,  0.2571,  ..., -0.4828, -0.4828, -0.4828],\n",
      "        [ 0.1454,  0.1727,  0.0817,  ...,  0.0999,  0.1909,  0.0726]])], 'age': tensor(0.2774), 'class_label': tensor(2), 'metadata': {'serial': '00018', 'edfname': '00128526_180817', 'birth': '1944-02-11', 'record': '2017-08-18T09:45:06', 'age': 73, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1340, 'Eyes Open'], [5456, 'Eyes Closed'], [12480, 'Eyes Open'], [19962, 'Eyes Closed'], [26100, 'Eyes Open'], [30300, 'Eyes Closed'], [48360, 'Eyes Open'], [54576, 'Eyes Closed'], [60834, 'Eyes Open'], [66000, 'Eyes Closed'], [82716, 'Eyes Open'], [89354, 'Eyes Closed'], [97628, 'Eyes Open'], [103172, 'Eyes Closed'], [109009, 'Eyes Open'], [112790, 'Eyes Closed'], [121402, 'Photic On - 3.0 Hz'], [121861, 'Eyes Open'], [122492, 'Eyes Closed'], [123418, 'Photic Off'], [125476, 'Photic On - 6.0 Hz'], [127492, 'Photic Off'], [128078, 'Eyes Open'], [128750, 'Eyes Closed'], [129508, 'Photic On - 9.0 Hz'], [131524, 'Photic Off'], [133582, 'Photic On - 12.0 Hz'], [135598, 'Photic Off'], [136016, 'Eyes Open'], [136940, 'Eyes Closed'], [137656, 'Photic On - 15.0 Hz'], [139672, 'Photic Off'], [141688, 'Photic On - 18.0 Hz'], [143746, 'Photic Off'], [145762, 'Photic On - 21.0 Hz'], [147778, 'Photic Off'], [148196, 'Eyes Open'], [148994, 'Eyes Closed'], [149836, 'Photic On - 24.0 Hz'], [151852, 'Photic Off'], [153868, 'Photic On - 27.0 Hz'], [155926, 'Photic Off'], [157942, 'Photic On - 30.0 Hz'], [159958, 'Photic Off'], [168692, 'Eyes Open'], [169490, 'Eyes Closed'], [175800, 'Paused']], 'class_type': 'Non-vascular dementia', 'class_label': 2}}\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGToTensor()\n",
    "                              ])\n",
    "\n",
    "train_dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "val_dataset = EEGDataset(root_path, metadata_val, composed)\n",
    "test_dataset = EEGDataset(root_path, metadata_test, composed)\n",
    "\n",
    "print(train_dataset[0]['signal'][0].shape)\n",
    "print(torch.mean(train_dataset[0]['signal'][0], axis=-1))\n",
    "print(torch.std(train_dataset[0]['signal'][0], axis=-1))\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(train_dataset[0]['signal'][1].shape)\n",
    "print(torch.mean(train_dataset[0]['signal'][1], axis=-1))\n",
    "print(torch.std(train_dataset[0]['signal'][1], axis=-1))\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(train_dataset[0]['signal'][1].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 101, 121]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[9.0801e+01, 1.1099e+01, 4.6555e+00,  ..., 1.4541e-01, 7.3999e-02,\n",
      "         1.2837e-01],\n",
      "        [8.6186e+01, 7.7931e+00, 1.3505e+01,  ..., 1.0877e-01, 2.7455e-01,\n",
      "         0.0000e+00],\n",
      "        [4.0296e+01, 2.2620e+01, 1.8139e+01,  ..., 3.7149e-01, 2.6048e-01,\n",
      "         2.9259e-01],\n",
      "        ...,\n",
      "        [2.4472e+01, 1.7197e+01, 1.9991e+01,  ..., 1.2253e+00, 8.6219e-01,\n",
      "         7.5294e-01],\n",
      "        [5.0536e+01, 1.8511e+01, 1.5670e+01,  ..., 1.1077e+00, 1.4598e+00,\n",
      "         3.7300e-01],\n",
      "        [6.5496e+00, 3.7919e+01, 4.9597e+01,  ..., 1.7998e-01, 1.7055e-01,\n",
      "         1.6366e-01]])\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=200*60), # 1 minutes\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGToTensor(),\n",
    "                               EEGSpectrogram(n_fft=200, complex_mode='power', hop_length=200 // 2)])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device: cuda\n",
      "0 torch.Size([64, 20, 4000]) torch.Size([64]) torch.Size([64]) 64\n",
      "1 torch.Size([64, 20, 4000]) torch.Size([64]) torch.Size([64]) 64\n",
      "2 torch.Size([64, 20, 4000]) torch.Size([64]) torch.Size([64]) 64\n",
      "3 torch.Size([64, 20, 4000]) torch.Size([64]) torch.Size([64]) 64\n",
      "4 torch.Size([64, 20, 4000]) torch.Size([64]) torch.Size([64]) 64\n",
      "CPU times: total: 11.7 s\n",
      "Wall time: 996 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Current PyTorch device:', device)\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    \n",
    "composed = transforms.Compose([\n",
    "    EEGRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "    EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "    EEGDropPhoticChannel(),\n",
    "    EEGNormalizePerSignal(),\n",
    "    EEGToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "val_dataset = EEGDataset(root_path, metadata_val, composed)\n",
    "test_dataset = EEGDataset(root_path, metadata_test, composed)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inplate the minibatch size\n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(i_batch, \n",
    "          sample_batched['signal'].shape, \n",
    "          sample_batched['age'].shape, \n",
    "          sample_batched['class_label'].shape, \n",
    "          len(sample_batched['metadata']))\n",
    "    \n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation, test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        num_workers=num_workers, \n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=eeg_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=32, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory,\n",
    "                         collate_fn=eeg_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6559e+00,  1.6458e+00,  1.6458e+00,  ..., -1.5815e-01,\n",
      "          -1.8855e-01, -2.7976e-01],\n",
      "         [ 1.2338e+00,  1.3052e+00,  1.2338e+00,  ...,  2.0896e-02,\n",
      "           2.0896e-02,  5.6571e-02],\n",
      "         [-9.5145e-01, -1.1137e+00, -1.1948e+00,  ...,  4.2773e-01,\n",
      "           4.2773e-01,  3.4660e-01],\n",
      "         ...,\n",
      "         [-2.6364e+00, -2.7283e+00, -3.0037e+00,  ...,  3.0184e-01,\n",
      "           2.1002e-01,  2.1002e-01],\n",
      "         [-9.4228e-01, -8.3814e-01, -7.8606e-01,  ...,  4.6370e-01,\n",
      "           4.6370e-01,  4.6370e-01],\n",
      "         [ 7.6880e-01,  9.6164e-01,  8.6015e-01,  ...,  7.5865e-01,\n",
      "           8.1955e-01,  7.5865e-01]],\n",
      "\n",
      "        [[ 5.5445e-01,  5.2110e-01,  6.5449e-01,  ..., -5.7934e-01,\n",
      "          -5.1265e-01, -4.1261e-01],\n",
      "         [-6.3392e-01, -6.3392e-01, -5.0142e-01,  ..., -1.2964e+00,\n",
      "          -8.9890e-01, -6.3392e-01],\n",
      "         [-6.8440e-01, -4.7576e-01, -2.6711e-01,  ..., -1.1017e+00,\n",
      "          -1.3103e+00, -1.5190e+00],\n",
      "         ...,\n",
      "         [-4.3910e-01, -3.5768e-01, -3.5768e-01,  ..., -6.0194e-01,\n",
      "          -6.0194e-01, -6.8336e-01],\n",
      "         [-5.1665e-01, -5.1665e-01, -6.0195e-01,  ..., -4.8197e-03,\n",
      "           8.0485e-02,  8.0485e-02],\n",
      "         [-8.1997e-03, -9.7977e-02, -2.3763e-01,  ...,  3.4093e-01,\n",
      "           3.6088e-01,  2.3121e-01]],\n",
      "\n",
      "        [[-7.3805e-01, -9.5219e-01, -9.8278e-01,  ..., -7.9923e-01,\n",
      "          -7.0746e-01, -7.0746e-01],\n",
      "         [-4.8890e-01, -2.5130e-01, -1.3249e-01,  ..., -3.7010e-01,\n",
      "          -4.2950e-01, -4.2950e-01],\n",
      "         [-8.5390e-01, -4.2367e-01,  6.5610e-03,  ..., -5.3123e-01,\n",
      "          -4.2367e-01, -3.1611e-01],\n",
      "         ...,\n",
      "         [ 1.4179e-01, -1.5079e-01, -2.9708e-01,  ...,  4.3437e-01,\n",
      "           3.6122e-01,  2.1494e-01],\n",
      "         [ 1.0373e+00,  6.5230e-01,  5.5605e-01,  ...,  4.5979e-01,\n",
      "           6.5230e-01,  7.4856e-01],\n",
      "         [-3.5046e-02, -2.9624e-03,  7.7322e-03,  ..., -9.9214e-02,\n",
      "          -1.2060e-01, -1.5269e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.0327e-01, -1.7211e-01,  1.4814e-02,  ..., -4.8366e-01,\n",
      "          -5.7713e-01, -6.3943e-01],\n",
      "         [ 1.2612e+00,  3.2332e-01,  6.8404e-01,  ..., -1.0953e-01,\n",
      "          -2.5381e-01, -3.9810e-01],\n",
      "         [ 3.2466e-01,  3.2466e-01, -2.6285e-01,  ..., -2.6285e-01,\n",
      "          -2.7848e-02,  5.5966e-01],\n",
      "         ...,\n",
      "         [-5.4740e-01, -2.5485e-01,  3.7702e-02,  ..., -1.7176e+00,\n",
      "          -1.2788e+00, -9.8622e-01],\n",
      "         [-1.2266e+00, -8.9997e-01, -7.3666e-01,  ..., -2.0431e+00,\n",
      "          -1.3899e+00, -1.0633e+00],\n",
      "         [ 5.4101e-03,  5.4101e-03,  5.4101e-03,  ...,  5.4101e-03,\n",
      "          -1.7980e+00,  5.4101e-03]],\n",
      "\n",
      "        [[ 5.4359e-01,  4.8864e-01,  5.6191e-01,  ..., -5.9210e-03,\n",
      "          -4.2555e-02, -6.0872e-02],\n",
      "         [-3.1663e-01, -5.7501e-01, -7.8172e-01,  ...,  8.2024e-01,\n",
      "           8.2024e-01,  7.6857e-01],\n",
      "         [-7.7137e-01, -8.6994e-01, -9.6851e-01,  ..., -6.7279e-01,\n",
      "          -6.7279e-01, -8.6994e-01],\n",
      "         ...,\n",
      "         [-8.7714e-01, -4.7500e-01, -1.7340e-01,  ...,  7.3141e-01,\n",
      "           7.3141e-01,  4.2981e-01],\n",
      "         [ 4.7111e-01,  8.8533e-01,  1.0924e+00,  ..., -2.5376e-01,\n",
      "          -3.5731e-01, -6.6797e-01],\n",
      "         [-7.0474e-01, -4.8146e-01, -1.6684e-01,  ...,  1.1733e-01,\n",
      "           2.3912e-01,  1.3763e-01]],\n",
      "\n",
      "        [[ 9.5265e-01,  8.9687e-01,  8.7827e-01,  ..., -1.2588e-01,\n",
      "          -3.2900e-02,  4.2909e-03],\n",
      "         [ 3.9289e-01,  3.9289e-01,  3.9289e-01,  ...,  7.6817e-01,\n",
      "           7.9944e-01,  7.0562e-01],\n",
      "         [-6.4852e-01, -6.4852e-01, -6.4852e-01,  ..., -4.3241e-01,\n",
      "          -5.7648e-01, -7.2056e-01],\n",
      "         ...,\n",
      "         [-1.1894e+00, -1.0623e+00, -9.3521e-01,  ..., -1.5708e+00,\n",
      "          -1.6979e+00, -1.6979e+00],\n",
      "         [ 2.7342e-01,  3.9109e-01,  3.3225e-01,  ..., -3.7376e-01,\n",
      "          -6.6793e-01, -7.2677e-01],\n",
      "         [ 1.0390e+00,  9.0230e-01,  1.0601e+00,  ..., -6.1227e-01,\n",
      "          -7.1745e-01, -5.1761e-01]]], device='cuda:0')\n",
      "tensor([ 0.4803,  0.4803,  0.2774,  0.2774, -0.7372, -0.7372,  0.6833,  0.6833,\n",
      "        -1.3460, -1.3460,  0.3789,  0.3789, -0.6358, -0.6358,  0.0745,  0.0745,\n",
      "        -0.1284, -0.1284,  1.2920,  1.2920,  0.2774,  0.2774,  0.9877,  0.9877,\n",
      "        -0.7372, -0.7372, -0.1284, -0.1284,  0.1759,  0.1759,  0.2774,  0.2774,\n",
      "         0.8862,  0.8862, -0.7372, -0.7372,  1.0891,  1.0891, -0.4328, -0.4328,\n",
      "        -0.2299, -0.2299, -0.6358, -0.6358,  0.8862,  0.8862, -1.1431, -1.1431,\n",
      "         0.9877,  0.9877, -0.0270, -0.0270, -0.1284, -0.1284, -0.0270, -0.0270,\n",
      "        -1.7519, -1.7519,  0.6833,  0.6833,  0.1759,  0.1759,  0.0745,  0.0745],\n",
      "       device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_i, sample_batched in enumerate(train_loader):\n",
    "    # pull up the batch data\n",
    "    x = sample_batched['signal'].to(device)\n",
    "    age = sample_batched['age'].to(device)\n",
    "    target = sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(x)\n",
    "    print(age)\n",
    "    print(target)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
