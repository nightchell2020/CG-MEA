{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "PyTorch의 EEG 데이터를 Dataset class 및 DataLoader class로 처리해보는 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# custom package\n",
    "from datasets.cau_eeg_dataset import *\n",
    "from datasets.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n",
    "#  'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n",
    "#  'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n",
    "#  'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', \n",
    "#  'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic' # for Hangul in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.11.0\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "root_path = r'local/dataset/02_Curated_Data_220317/' # 02_Curated_Data_210705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 78,\n",
      " 'birth': '1940-06-02',\n",
      " 'dx1': 'mci_rf',\n",
      " 'edfname': '00001809_261018',\n",
      " 'label': ['mci', 'mci_amnestic', 'mci_amnestic_rf'],\n",
      " 'record': '2018-10-26T15:46:26',\n",
      " 'serial': '00001'}\n"
     ]
    }
   ],
   "source": [
    "meta_path = os.path.join(root_path, 'metadata_debug.json')\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label_to_type: ['Normal', 'Non-vascular MCI', 'Non-vascular dementia']\n"
     ]
    }
   ],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        # inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 458 data belonging to Normal\n",
      "- There are 350 data belonging to Non-vascular MCI\n",
      "- There are 233 data belonging to Non-vascular dementia\n"
     ]
    }
   ],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size\t\t: 832\n",
      "Validation data size\t: 104\n",
      "Test data size\t\t: 105\n",
      "\n",
      " --- Recheck --- \n",
      "\n",
      "Train data label distribution\t: [366 280 186] 832\n",
      "Val data label distribution\t: [46 35 23] 104\n",
      "Test data label distribution\t: [46 35 24] 105\n"
     ]
    }
   ],
   "source": [
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Test TorchVision Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'signal': array([[-27, -28, -26],\n",
      "       [  5,   4,   6],\n",
      "       [ -8,  -8,  -8],\n",
      "       [ 13,  14,  15],\n",
      "       [  6,   8,  10],\n",
      "       [-40, -42, -44],\n",
      "       [-15, -10, -13],\n",
      "       [ 18,  15,   6],\n",
      "       [  6,   5,   2],\n",
      "       [ 13,  13,  13],\n",
      "       [ -3,  -2,   0],\n",
      "       [  4,   6,  13],\n",
      "       [  5,  11,  18],\n",
      "       [-20, -22, -23],\n",
      "       [ 17,   9,  11],\n",
      "       [ 12,  12,  12],\n",
      "       [-49, -51, -52],\n",
      "       [  0,   0,  -2],\n",
      "       [ -4,  -4,  -6],\n",
      "       [ 12,  30,  17],\n",
      "       [ -1,   0,   0]]), 'age': 76, 'class_label': 2, 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic'], 'event': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1442, 'Eyes Open'], [5264, 'Eyes Closed'], [11186, 'Eyes Open'], [17908, 'Eyes Closed'], [23918, 'Eyes Open'], [29672, 'Eyes Closed'], [36392, 'Eyes Open'], [41936, 'Eyes Closed'], [50630, 'Eyes Open'], [53905, 'Eyes Open'], [53990, 'Eyes Closed'], [60038, 'Eyes Open'], [65078, 'Eyes Closed'], [72218, 'Eyes Open'], [77300, 'Eyes Closed'], [83938, 'Eyes Open'], [89860, 'Eyes Closed'], [95992, 'Eyes Open'], [100276, 'Eyes Closed'], [108340, 'Eyes Open'], [113464, 'Eyes Closed'], [123796, 'Eyes Open'], [125685, 'Eyes Closed'], [129804, 'Photic On - 3.0 Hz'], [130096, 'Eyes Open'], [131104, 'Eyes Closed'], [131820, 'Photic Off'], [133878, 'Photic On - 6.0 Hz'], [135894, 'Photic Off'], [136144, 'Eyes Open'], [137952, 'Photic On - 9.0 Hz'], [138538, 'Eyes Closed'], [139968, 'Photic Off'], [141984, 'Photic On - 12.0 Hz'], [144042, 'Photic Off'], [144418, 'Eyes Open'], [145300, 'Eyes Closed'], [146058, 'Photic On - 15.0 Hz'], [148074, 'Photic Off'], [148408, 'Eyes Open'], [149374, 'Eyes Closed'], [150132, 'Photic On - 18.0 Hz'], [152148, 'Photic Off'], [154164, 'Photic On - 21.0 Hz'], [156222, 'Photic Off'], [156556, 'Eyes Open'], [157564, 'Eyes Closed'], [158238, 'Photic On - 24.0 Hz'], [160254, 'Photic Off'], [162312, 'Photic On - 27.0 Hz'], [164328, 'Photic Off'], [166386, 'Photic On - 30.0 Hz'], [168402, 'Photic Off'], [168652, 'Eyes Open'], [169576, 'Eyes Closed'], [175600, 'Paused']]}}\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "{'signal': array([[-27, -26, -28],\n",
      "       [ -1,   1,  -1],\n",
      "       [ 25,  29,  32],\n",
      "       [  1,   6,  10],\n",
      "       [ -5,   0,   5],\n",
      "       [-10,  -9,  -9],\n",
      "       [ -6,  -8,  -9],\n",
      "       [  0,  -5,  -9],\n",
      "       [  3,   0,  -3],\n",
      "       [ -1,  -2,  -2],\n",
      "       [  0,   0,   0],\n",
      "       [ -8,  -6,  -3],\n",
      "       [ -5,  -2,   2],\n",
      "       [-18, -20, -21],\n",
      "       [  9,   3,  -2],\n",
      "       [  0,  -1,  -3],\n",
      "       [  7,   6,   5],\n",
      "       [  6,   7,   7],\n",
      "       [ -7,  -8,  -7],\n",
      "       [ 23,   1,  -8],\n",
      "       [ -1,   0,   0]]), 'age': 76, 'class_label': 2, 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic'], 'event': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1442, 'Eyes Open'], [5264, 'Eyes Closed'], [11186, 'Eyes Open'], [17908, 'Eyes Closed'], [23918, 'Eyes Open'], [29672, 'Eyes Closed'], [36392, 'Eyes Open'], [41936, 'Eyes Closed'], [50630, 'Eyes Open'], [53905, 'Eyes Open'], [53990, 'Eyes Closed'], [60038, 'Eyes Open'], [65078, 'Eyes Closed'], [72218, 'Eyes Open'], [77300, 'Eyes Closed'], [83938, 'Eyes Open'], [89860, 'Eyes Closed'], [95992, 'Eyes Open'], [100276, 'Eyes Closed'], [108340, 'Eyes Open'], [113464, 'Eyes Closed'], [123796, 'Eyes Open'], [125685, 'Eyes Closed'], [129804, 'Photic On - 3.0 Hz'], [130096, 'Eyes Open'], [131104, 'Eyes Closed'], [131820, 'Photic Off'], [133878, 'Photic On - 6.0 Hz'], [135894, 'Photic Off'], [136144, 'Eyes Open'], [137952, 'Photic On - 9.0 Hz'], [138538, 'Eyes Closed'], [139968, 'Photic Off'], [141984, 'Photic On - 12.0 Hz'], [144042, 'Photic Off'], [144418, 'Eyes Open'], [145300, 'Eyes Closed'], [146058, 'Photic On - 15.0 Hz'], [148074, 'Photic Off'], [148408, 'Eyes Open'], [149374, 'Eyes Closed'], [150132, 'Photic On - 18.0 Hz'], [152148, 'Photic Off'], [154164, 'Photic On - 21.0 Hz'], [156222, 'Photic Off'], [156556, 'Eyes Open'], [157564, 'Eyes Closed'], [158238, 'Photic On - 24.0 Hz'], [160254, 'Photic Off'], [162312, 'Photic On - 27.0 Hz'], [164328, 'Photic Off'], [166386, 'Photic On - 30.0 Hz'], [168402, 'Photic Off'], [168652, 'Eyes Open'], [169576, 'Eyes Closed'], [175600, 'Paused']]}}\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    dataset = CauEegDataset(root_path, metadata_train, load_event=True, \n",
    "                            transform=EegRandomCrop(crop_length=3))\n",
    "    print(dataset[0])\n",
    "    print('\\n')\n",
    "    print('-' * 100)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  3,   7,  10],\n",
      "       [ 11,  13,  13],\n",
      "       [  7,   9,   8],\n",
      "       [ -6,  -6,  -6],\n",
      "       [-15, -16, -16],\n",
      "       [ 16,  16,  17],\n",
      "       [  2,   2,   4],\n",
      "       [ 10,  11,  16],\n",
      "       [ -7,  -6,  -5],\n",
      "       [ -5,  -4,  -4],\n",
      "       [ 78,  80,  79],\n",
      "       [-13, -16, -17],\n",
      "       [-11, -14, -15],\n",
      "       [-13, -14, -15],\n",
      "       [  0,  -8, -12],\n",
      "       [  1,   0,  -2],\n",
      "       [-19, -17, -17],\n",
      "       [-13, -10,  -8],\n",
      "       [ -6,  -4,  -3],\n",
      "       [ 17,  42,  65],\n",
      "       [  1,   1,   0]]), array([[  3,   4,   3],\n",
      "       [  7,   7,   7],\n",
      "       [-16, -12, -10],\n",
      "       [-21, -19, -17],\n",
      "       [ -5,  -6,  -4],\n",
      "       [  3,   3,   2],\n",
      "       [  6,   7,   6],\n",
      "       [ -5,  -7,  -7],\n",
      "       [ -1,  -1,  -1],\n",
      "       [  5,   6,   5],\n",
      "       [  9,   8,   7],\n",
      "       [  4,   0,   1],\n",
      "       [ -8, -11, -11],\n",
      "       [ 18,  17,  15],\n",
      "       [ 10,  12,   9],\n",
      "       [  1,  -2,  -4],\n",
      "       [-13, -11, -10],\n",
      "       [ 10,  12,  14],\n",
      "       [ -2,  -1,   1],\n",
      "       [108,  96, 101],\n",
      "       [  0,   0,  -1]])]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[array([[ 10,   9,   9],\n",
      "       [ -7,  -9, -10],\n",
      "       [ 11,  11,  10],\n",
      "       [ 10,  11,  11],\n",
      "       [ 10,  10,   8],\n",
      "       [  3,   1,  -4],\n",
      "       [  5,   5,   2],\n",
      "       [ -4,  -2,   1],\n",
      "       [  2,   3,   6],\n",
      "       [  3,   3,   5],\n",
      "       [  5,   8,   8],\n",
      "       [  2,   7,   3],\n",
      "       [  9,  12,   9],\n",
      "       [-47, -49, -49],\n",
      "       [  6,  -3,   0],\n",
      "       [  2,  -1,   2],\n",
      "       [  6,   6,   4],\n",
      "       [-10, -10,  -9],\n",
      "       [ -2,  -2,   0],\n",
      "       [ 26,  -2, -17],\n",
      "       [  0,  -3,  -3]]), array([[ 11,   8,  10],\n",
      "       [  6,  -2,  -1],\n",
      "       [  6,   3,   3],\n",
      "       [  7,   8,   8],\n",
      "       [ -1,   1,   1],\n",
      "       [  6,   8,   8],\n",
      "       [ -4,  -2,  -4],\n",
      "       [ -4,  -3,  -6],\n",
      "       [ -5,  -4,  -5],\n",
      "       [ -2,  -1,  -3],\n",
      "       [ -5,  -5,  -5],\n",
      "       [ -6,  -6,   1],\n",
      "       [ -2,  -1,   1],\n",
      "       [  3,   3,   2],\n",
      "       [ -2,  -3,  -4],\n",
      "       [ -8,  -7,  -8],\n",
      "       [ 16,  17,  17],\n",
      "       [  0,   0,   0],\n",
      "       [  2,   3,   3],\n",
      "       [-45, -45, -33],\n",
      "       [  0,  -1,  -1]])]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    dataset = CauEegDataset(root_path, metadata_train, load_event=False, \n",
    "                            transform=EegRandomCrop(crop_length=3, multiple=2))\n",
    "    print(dataset[0]['signal'])\n",
    "    print('\\n')\n",
    "    print('-' * 100)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization per signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'signal': array([[ 1.02104732e+00,  9.58473534e-01,  9.58473534e-01, ...,\n",
      "        -4.27070916e-02, -2.18491620e-02, -4.27070916e-02],\n",
      "       [ 1.78706960e+00,  1.66245676e+00,  2.20244571e+00, ...,\n",
      "         7.07091698e-01,  8.73242144e-01,  7.90166921e-01],\n",
      "       [ 3.34353951e+00,  3.46288107e+00,  3.22419795e+00, ...,\n",
      "        -1.01242755e+00, -1.01242755e+00, -9.52756771e-01],\n",
      "       ...,\n",
      "       [ 2.67948738e+00,  2.84720273e+00,  3.01491807e+00, ...,\n",
      "        -6.74819496e-01, -5.07104152e-01, -5.07104152e-01],\n",
      "       [ 1.03733280e+00,  7.56602549e-01,  3.41610004e-01, ...,\n",
      "        -8.55882048e-02,  2.42627630e-02,  9.74967416e-02],\n",
      "       [ 1.04657419e-02,  1.04657419e-02,  7.36901822e-05, ...,\n",
      "         7.36901822e-05,  1.56617677e-02,  1.56617677e-02]]), 'age': 76, 'class_label': 2, 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Mean: [ 7.03993955e-18  3.75736308e-17  2.45579287e-17  3.40536611e-17\n",
      " -2.61951239e-18 -2.98788132e-18  3.43811001e-18  5.40274430e-18\n",
      " -1.76817086e-17 -1.47347572e-18 -3.27439049e-19  3.27439049e-18\n",
      "  6.87622002e-18 -3.50359782e-17 -9.90503122e-18 -1.14603667e-18\n",
      "  3.76554906e-18 -3.99475639e-17 -1.80091477e-17 -9.77712535e-18\n",
      " -7.32261154e-20]\n",
      "Std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "dataset = CauEegDataset(root_path, metadata_train, load_event=False, \n",
    "                        transform=EegNormalizePerSignal())\n",
    "\n",
    "print(dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('Mean:', np.mean(dataset[0]['signal'], axis=1))\n",
    "print('Std:', np.std(dataset[0]['signal'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age mean and standard deviation:\t 70.12860576923077 ,\t 9.986242891827843\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "before:\n",
      "76\n",
      "83\n",
      "84\n",
      "87\n",
      "74\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after:\n",
      "0.5879482692829902\n",
      "1.288912592784349\n",
      "1.389050353284543\n",
      "1.6894636347851253\n",
      "0.387672748282602\n"
     ]
    }
   ],
   "source": [
    "ages = []\n",
    "for m in metadata_train:\n",
    "    ages.append(m['age'])\n",
    "\n",
    "ages = np.array(ages)\n",
    "age_mean = np.mean(ages)\n",
    "age_std = np.std(ages)\n",
    "\n",
    "print('Age mean and standard deviation:\\t', age_mean, ',\\t', age_std)\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('before:')\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=None)\n",
    "for i in range(5):\n",
    "    print(dataset[i]['age'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('after:')\n",
    "dataset = CauEegDataset(root_path, metadata_train, load_event=False, \n",
    "                        transform=EegNormalizeAge(mean=age_mean, std=age_std))\n",
    "for i in range(5):\n",
    "    print(dataset[i]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop EKG channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (21, 173600)\n",
      "[[ 49  46  46 ...  -2  -1  -2]\n",
      " [ 43  40  53 ...  17  21  19]\n",
      " [ 56  58  54 ... -17 -17 -16]\n",
      " ...\n",
      " [ 16  17  18 ...  -4  -3  -3]\n",
      " [ 85  62  28 ...  -7   2   8]\n",
      " [  2   2   0 ...   0   3   3]]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after: (20, 173600)\n",
      "[[ 49  46  46 ...  -2  -1  -2]\n",
      " [ 43  40  53 ...  17  21  19]\n",
      " [ 56  58  54 ... -17 -17 -16]\n",
      " ...\n",
      " [ 24  25  27 ... -13 -12 -10]\n",
      " [ 16  17  18 ...  -4  -3  -3]\n",
      " [  2   2   0 ...   0   3   3]]\n"
     ]
    }
   ],
   "source": [
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=None)\n",
    "print('before:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=EegDropEKGChannel())\n",
    "print('after:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop photic stimulation channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (21, 173600)\n",
      "[[ 49  46  46 ...  -2  -1  -2]\n",
      " [ 43  40  53 ...  17  21  19]\n",
      " [ 56  58  54 ... -17 -17 -16]\n",
      " ...\n",
      " [ 16  17  18 ...  -4  -3  -3]\n",
      " [ 85  62  28 ...  -7   2   8]\n",
      " [  2   2   0 ...   0   3   3]]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after: (20, 173600)\n",
      "[[ 49  46  46 ...  -2  -1  -2]\n",
      " [ 43  40  53 ...  17  21  19]\n",
      " [ 56  58  54 ... -17 -17 -16]\n",
      " ...\n",
      " [ 24  25  27 ... -13 -12 -10]\n",
      " [ 16  17  18 ...  -4  -3  -3]\n",
      " [ 85  62  28 ...  -7   2   8]]\n"
     ]
    }
   ],
   "source": [
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=None)\n",
    "print('before:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=EegDropPhoticChannel())\n",
    "print('after:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "{'signal': array([[ 49,  46,  46, ...,  -2,  -1,  -2],\n",
      "       [ 43,  40,  53, ...,  17,  21,  19],\n",
      "       [ 56,  58,  54, ..., -17, -17, -16],\n",
      "       ...,\n",
      "       [ 16,  17,  18, ...,  -4,  -3,  -3],\n",
      "       [ 85,  62,  28, ...,  -7,   2,   8],\n",
      "       [  2,   2,   0, ...,   0,   3,   3]]), 'age': 76, 'class_label': 2, 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after:\n",
      "{'signal': tensor([[ 49.,  46.,  46.,  ...,  -2.,  -1.,  -2.],\n",
      "        [ 43.,  40.,  53.,  ...,  17.,  21.,  19.],\n",
      "        [ 56.,  58.,  54.,  ..., -17., -17., -16.],\n",
      "        ...,\n",
      "        [ 16.,  17.,  18.,  ...,  -4.,  -3.,  -3.],\n",
      "        [ 85.,  62.,  28.,  ...,  -7.,   2.,   8.],\n",
      "        [  2.,   2.,   0.,  ...,   0.,   3.,   3.]]), 'age': tensor(76.), 'class_label': tensor(2), 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n"
     ]
    }
   ],
   "source": [
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=None)\n",
    "print('before:')\n",
    "print(dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=EegToTensor())\n",
    "print('after:')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short time Fourier transform (STFT or spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 101, 3473]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[-8.8490e+03,  7.7273e+02,  3.9726e+01,  ..., -1.8506e+01,\n",
      "         -1.6672e+01, -2.3000e+01],\n",
      "        [-7.4850e+03,  2.8430e+02, -6.1568e+01,  ..., -5.0885e+00,\n",
      "         -1.1210e+00,  5.0000e+00],\n",
      "        [ 9.1100e+02, -4.7487e+01, -1.7347e+02,  ...,  5.7645e+00,\n",
      "         -2.9938e+00,  3.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -3.5455e+02, -2.7713e+02,  ...,  9.7990e-01,\n",
      "         -6.4330e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  1.3124e+03, -8.3939e+02,  ..., -4.4814e+00,\n",
      "         -1.3693e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  9.8977e+00, -1.1967e+01,  ...,  2.9848e+00,\n",
      "          5.0672e-01,  0.0000e+00]])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([21, 101, 3473]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[8.8490e+03, 1.3565e+03, 4.9238e+02,  ..., 1.8506e+01, 1.6763e+01,\n",
      "         2.3000e+01],\n",
      "        [7.4850e+03, 3.5771e+02, 1.7835e+02,  ..., 6.5364e+00, 2.4492e+00,\n",
      "         5.0000e+00],\n",
      "        [9.1100e+02, 6.9631e+02, 1.8994e+02,  ..., 6.9155e+00, 4.0560e+00,\n",
      "         3.0000e+00],\n",
      "        ...,\n",
      "        [2.3730e+03, 3.5468e+02, 2.8842e+02,  ..., 4.9043e+00, 1.1631e+01,\n",
      "         9.0000e+00],\n",
      "        [2.0400e+02, 3.6145e+03, 2.0868e+03,  ..., 1.3101e+02, 1.3761e+02,\n",
      "         1.3200e+02],\n",
      "        [9.0000e+00, 1.1252e+01, 1.2486e+01,  ..., 3.1236e+00, 2.5907e+00,\n",
      "         5.0000e+00]])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([21, 101, 3473]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[-8.8490e+03,  7.7273e+02,  3.9726e+01,  ..., -1.8506e+01,\n",
      "         -1.6672e+01, -2.3000e+01],\n",
      "        [-7.4850e+03,  2.8430e+02, -6.1568e+01,  ..., -5.0885e+00,\n",
      "         -1.1210e+00,  5.0000e+00],\n",
      "        [ 9.1100e+02, -4.7487e+01, -1.7347e+02,  ...,  5.7645e+00,\n",
      "         -2.9938e+00,  3.0000e+00],\n",
      "        ...,\n",
      "        [ 2.3730e+03, -9.6947e+00,  7.9884e+01,  ...,  4.8055e+00,\n",
      "          9.6905e+00,  9.0000e+00],\n",
      "        [-2.0400e+02, -3.3679e+03, -1.9105e+03,  ...,  1.3093e+02,\n",
      "          1.3760e+02,  1.3200e+02],\n",
      "        [-9.0000e+00, -5.3522e+00,  3.5615e+00,  ..., -9.2095e-01,\n",
      "         -2.5407e+00, -5.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EegToTensor(), EegSpectrogram(n_fft=200, complex_mode='as_real')])\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "composed = transforms.Compose([EegToTensor(), EegSpectrogram(n_fft=200, complex_mode='power')])\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "composed = transforms.Compose([EegToTensor(), EegSpectrogram(n_fft=200, complex_mode='remove')])\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose some at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 12000])\n",
      "{'signal': tensor([[-1.1255, -1.1464, -1.1883,  ..., -0.4138, -0.4766, -0.4347],\n",
      "        [-2.0817, -1.8845, -2.4270,  ..., -0.3556, -0.6022, -0.6515],\n",
      "        [ 0.5920,  0.5278,  0.5920,  ...,  0.3351,  0.2708,  0.2066],\n",
      "        ...,\n",
      "        [ 0.4626,  0.4626,  0.3357,  ...,  1.3505,  1.3505,  1.3505],\n",
      "        [ 2.3842,  2.5376,  2.3842,  ...,  0.8498,  0.8498,  0.6964],\n",
      "        [ 0.4344,  0.5190,  0.6519,  ..., -0.6286, -0.7252, -0.5924]]), 'age': tensor(0.5879), 'class_label': tensor(2), 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 12000])\n",
      "{'signal': tensor([[-1.3439, -1.3742, -1.2833,  ...,  0.2612,  0.2612,  0.3217],\n",
      "        [-0.0906,  0.0917,  0.1525,  ..., -0.3336, -0.4551, -0.4551],\n",
      "        [ 1.5353,  1.6870,  1.9905,  ..., -2.2586, -2.1069, -1.9551],\n",
      "        ...,\n",
      "        [ 0.5124,  0.5124,  0.5124,  ...,  0.1755,  0.1755,  0.3439],\n",
      "        [ 0.4409,  0.2853,  0.2853,  ..., -1.7377, -1.5821, -1.1152],\n",
      "        [ 0.1807,  0.1112,  0.2086,  ...,  0.1390,  0.0695,  0.0278]]), 'age': tensor(1.0886), 'class_label': tensor(2), 'metadata': {'serial': '00649', 'edfname': '00951066_131217', 'birth': '1936-03-29', 'record': '2017-12-13T09:46:47', 'age': 81, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 12000])\n",
      "{'signal': tensor([[ 0.4166,  0.4166,  0.5159,  ..., -0.2290, -0.2290, -0.1793],\n",
      "        [ 0.7372,  0.7821,  0.8270,  ...,  1.0966,  1.0966,  1.0067],\n",
      "        [-2.6858, -2.6858, -2.6858,  ..., -0.9962, -1.3342, -1.5031],\n",
      "        ...,\n",
      "        [-1.4313, -1.6388, -2.0536,  ..., -1.0165, -0.8090, -0.8090],\n",
      "        [-1.0415, -1.0415, -1.2516,  ..., -1.0415, -1.0415, -1.0415],\n",
      "        [-0.0843, -0.1236, -0.0449,  ..., -0.5500, -0.6287, -0.7533]]), 'age': tensor(0.6881), 'class_label': tensor(0), 'metadata': {'serial': '00416', 'edfname': '00744618_290319', 'birth': '1941-11-03', 'record': '2019-03-29T14:11:07', 'age': 77, 'dx1': 'smi', 'label': ['normal', 'smi'], 'class_type': 'Normal', 'class_label': 0, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EegNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EegDropPhoticChannel(),\n",
    "                               EegRandomCrop(crop_length=200*60), # 1 minute\n",
    "                               EegNormalizePerSignal(),\n",
    "                               EegToTensor()])\n",
    "\n",
    "train_dataset = CauEegDataset(root_path, metadata_train, \n",
    "                              load_event=False, transform=composed)\n",
    "val_dataset = CauEegDataset(root_path, metadata_val, \n",
    "                            load_event=False, transform=composed)\n",
    "test_dataset = CauEegDataset(root_path, metadata_test, \n",
    "                             load_event=False, transform=composed)\n",
    "\n",
    "print(train_dataset[0]['signal'].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0]['signal'].shape)\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0]['signal'].shape)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4000])\n",
      "tensor([-5.4836e-09,  2.4915e-08,  2.3842e-09,  1.5259e-08, -7.1526e-09,\n",
      "         4.6492e-09, -1.0729e-08, -7.9274e-09, -5.2452e-09,  4.5300e-09,\n",
      "         5.3048e-09,  2.3842e-09, -2.3842e-10, -1.9073e-09, -1.4305e-09,\n",
      "        -3.3379e-09, -4.7684e-09,  1.0371e-08, -9.0599e-09,  7.1526e-10])\n",
      "tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 4000])\n",
      "tensor([-1.6689e-09,  1.4305e-09, -9.0599e-09, -1.2040e-08, -1.1086e-08,\n",
      "        -3.5167e-09,  2.6226e-09, -9.0599e-09, -4.7684e-09,  9.5367e-10,\n",
      "         4.0531e-09,  4.5300e-09,  1.1921e-08, -8.8215e-09,  7.8678e-09,\n",
      "        -4.7684e-09,  8.5831e-09, -3.2187e-09, -1.1921e-09, -1.1921e-09])\n",
      "tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([20, 4000])\n",
      "{'signal': [tensor([[ 1.1918,  1.0809,  1.0809,  ..., -0.1385, -0.0646, -0.0276],\n",
      "        [-0.4792, -0.5980, -0.9943,  ..., -0.1225, -0.1225, -0.1225],\n",
      "        [-0.6516, -0.8571, -1.1310,  ..., -0.9940, -0.9256, -0.7886],\n",
      "        ...,\n",
      "        [ 1.3737,  1.3737,  1.7990,  ...,  1.7990,  1.3737,  1.2320],\n",
      "        [-0.8847, -0.6890, -0.1019,  ...,  0.4852,  0.4852,  0.4852],\n",
      "        [-0.6197, -0.6812, -0.8165,  ...,  1.2618,  1.3233,  1.2372]]), tensor([[ 0.4546,  0.5094,  0.6192,  ...,  0.3997,  0.6741,  1.0583],\n",
      "        [-0.4826, -0.3215, -0.1604,  ...,  0.6452,  0.4841,  0.9674],\n",
      "        [ 0.8490,  0.8490,  0.8490,  ..., -0.2561, -0.1911, -0.1911],\n",
      "        ...,\n",
      "        [ 0.0620,  0.0620,  0.0620,  ...,  0.6893,  0.9402,  1.0657],\n",
      "        [-2.0967, -2.4664, -2.8362,  ...,  1.4160,  1.7857,  1.7857],\n",
      "        [-0.3102,  0.1434,  0.3821,  ...,  0.1553,  0.0717, -0.0118]])], 'age': tensor(0.5879), 'class_label': tensor(2), 'metadata': {'serial': '01203', 'edfname': '01312293_120417', 'birth': '1941-02-26', 'record': '2017-04-12T13:48:31', 'age': 76, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "{'signal': [tensor([[-0.0936,  0.3413,  0.3848,  ...,  0.6893,  0.6458,  0.5153],\n",
      "        [ 0.7234,  0.7964,  0.4315,  ...,  1.5993,  1.5263,  1.3073],\n",
      "        [-0.2284, -0.2284, -0.5745,  ...,  1.5024,  1.5024,  1.1562],\n",
      "        ...,\n",
      "        [-0.4874, -0.1396,  0.3821,  ..., -0.1396, -0.3135, -0.3135],\n",
      "        [-1.8658, -1.7826, -1.6994,  ..., -0.7007, -0.4511, -0.3678],\n",
      "        [ 0.7098,  0.7098,  0.7098,  ..., -0.0908, -0.0632, -0.0632]]), tensor([[-0.1770, -0.2531, -0.3292,  ...,  0.2797,  0.2543,  0.2036],\n",
      "        [-0.1242, -0.1685, -0.1685,  ..., -1.2308, -1.1866, -1.0980],\n",
      "        [ 3.3280,  3.4978,  3.1583,  ...,  0.6123,  0.6123,  0.6123],\n",
      "        ...,\n",
      "        [-0.1803, -0.3710, -1.1335,  ...,  0.7728,  0.7728,  0.2009],\n",
      "        [ 0.6238,  0.4528,  0.1107,  ...,  0.1107, -0.0603, -0.5734],\n",
      "        [-0.1489, -0.1761, -0.1625,  ...,  0.5293,  0.4072,  0.2580]])], 'age': tensor(1.0886), 'class_label': tensor(2), 'metadata': {'serial': '00649', 'edfname': '00951066_131217', 'birth': '1936-03-29', 'record': '2017-12-13T09:46:47', 'age': 81, 'dx1': 'load', 'label': ['dementia', 'ad', 'load'], 'class_type': 'Non-vascular dementia', 'class_label': 2, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "{'signal': [tensor([[ 0.4792,  0.5049,  0.4792,  ..., -0.1131, -0.0874, -0.1131],\n",
      "        [ 1.0958,  1.0958,  1.0309,  ...,  0.5763,  0.5114,  0.3166],\n",
      "        [ 0.8681,  0.7023,  0.5365,  ...,  0.3707,  0.5365,  0.0391],\n",
      "        ...,\n",
      "        [ 2.0638,  1.6849,  1.3060,  ..., -0.0200, -0.2094, -0.3988],\n",
      "        [ 0.5335,  0.1948, -0.1438,  ...,  0.1948,  0.1948,  0.0255],\n",
      "        [-0.0258, -0.0457, -0.0258,  ...,  0.8270,  0.7477,  0.6948]]), tensor([[-0.3518, -0.3518, -0.4391,  ...,  0.9570,  0.8698,  0.8698],\n",
      "        [ 0.1887,  0.1887,  0.1446,  ..., -0.4714, -0.4714, -0.5154],\n",
      "        [-1.0860, -0.6902, -0.6902,  ...,  1.2890,  1.4869,  1.4869],\n",
      "        ...,\n",
      "        [-0.6399, -0.6399, -1.3435,  ...,  0.2983,  0.7675,  1.0020],\n",
      "        [-0.1571, -0.1571, -0.5982,  ...,  0.2839,  0.7250,  0.9455],\n",
      "        [ 0.6690,  0.8141,  0.8933,  ...,  0.0752, -0.0171, -0.0567]])], 'age': tensor(0.6881), 'class_label': tensor(0), 'metadata': {'serial': '00416', 'edfname': '00744618_290319', 'birth': '1941-11-03', 'record': '2019-03-29T14:11:07', 'age': 77, 'dx1': 'smi', 'label': ['normal', 'smi'], 'class_type': 'Normal', 'class_label': 0, 'channel': ['Fp1-AVG', 'F3-AVG', 'C3-AVG', 'P3-AVG', 'O1-AVG', 'Fp2-AVG', 'F4-AVG', 'C4-AVG', 'P4-AVG', 'O2-AVG', 'F7-AVG', 'T3-AVG', 'T5-AVG', 'F8-AVG', 'T4-AVG', 'T6-AVG', 'FZ-AVG', 'CZ-AVG', 'PZ-AVG', 'EKG', 'Photic']}}\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EegNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EegDropPhoticChannel(),\n",
    "                               EegRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "                               EegNormalizePerSignal(),\n",
    "                               EegToTensor()\n",
    "                              ])\n",
    "\n",
    "train_dataset = CauEegDataset(root_path, metadata_train, \n",
    "                              load_event=False, transform=composed)\n",
    "val_dataset = CauEegDataset(root_path, metadata_val, \n",
    "                            load_event=False, transform=composed)\n",
    "test_dataset = CauEegDataset(root_path, metadata_test, \n",
    "                             load_event=False, transform=composed)\n",
    "\n",
    "print(train_dataset[0]['signal'][0].shape)\n",
    "print(torch.mean(train_dataset[0]['signal'][0], axis=-1))\n",
    "print(torch.std(train_dataset[0]['signal'][0], axis=-1))\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(train_dataset[0]['signal'][1].shape)\n",
    "print(torch.mean(train_dataset[0]['signal'][1], axis=-1))\n",
    "print(torch.std(train_dataset[0]['signal'][1], axis=-1))\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(train_dataset[0]['signal'][1].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 101, 121]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[6.4232e+01, 2.0913e+01, 7.2749e+00,  ..., 3.5906e-01, 3.8525e-01,\n",
      "         4.0100e-01],\n",
      "        [3.7788e+02, 6.1324e+01, 1.9016e+01,  ..., 9.4126e-01, 7.6489e-01,\n",
      "         7.4011e-01],\n",
      "        [1.4823e+02, 2.5660e+01, 1.2974e+01,  ..., 8.1064e-01, 5.4665e-01,\n",
      "         6.3792e-01],\n",
      "        ...,\n",
      "        [2.0942e+02, 1.9779e+01, 6.6603e+00,  ..., 5.0642e-01, 6.9401e-01,\n",
      "         5.6524e-01],\n",
      "        [2.1950e+01, 2.6849e+01, 1.7988e+01,  ..., 8.0672e-01, 9.7667e-01,\n",
      "         2.0130e+00],\n",
      "        [1.1486e+01, 2.4720e+01, 7.5637e+00,  ..., 3.3016e-01, 3.7622e-01,\n",
      "         4.2370e-01]])\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EegNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EegDropPhoticChannel(),\n",
    "                               EegRandomCrop(crop_length=200*60), # 1 minutes\n",
    "                               EegNormalizePerSignal(),\n",
    "                               EegToTensor(),\n",
    "                               EegSpectrogram(n_fft=200, complex_mode='power', hop_length=200 // 2)])\n",
    "dataset = CauEegDataset(root_path, metadata_train, \n",
    "                        load_event=False, transform=composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device: cuda\n",
      "CPU times: total: 4min 45s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Current PyTorch device:', device)\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    \n",
    "composed = transforms.Compose([\n",
    "    EegRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "    EegNormalizeAge(mean=age_mean, std=age_std),\n",
    "    EegDropPhoticChannel(),\n",
    "    EegNormalizePerSignal(),\n",
    "    EegToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CauEegDataset(root_path, metadata_train, \n",
    "                              load_event=False, transform=composed)\n",
    "val_dataset = CauEegDataset(root_path, metadata_val, \n",
    "                            load_event=False, transform=composed)\n",
    "test_dataset = CauEegDataset(root_path, metadata_test, \n",
    "                             load_event=False, transform=composed)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for k in range(5):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        sample_batched['signal'].to(device)\n",
    "        sample_batched['age'].to(device)\n",
    "        sample_batched['class_label'].to(device)\n",
    "        \n",
    "# pprint.pprint(sample_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch device: cuda\n",
      "CPU times: total: 5min 2s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Current PyTorch device:', device)\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "    \n",
    "composed = transforms.Compose([\n",
    "    EegRandomCrop(crop_length=200*20, multiple=2), # 20 seconds\n",
    "    EegNormalizeAge(mean=age_mean, std=age_std),\n",
    "    EegDropPhoticChannel(),\n",
    "    EegNormalizePerSignal(),\n",
    "    EegToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CauEegDataset(root_path, metadata_train, \n",
    "                              load_event=True, transform=composed)\n",
    "val_dataset = CauEegDataset(root_path, metadata_val, \n",
    "                            load_event=True, transform=composed)\n",
    "test_dataset = CauEegDataset(root_path, metadata_test, \n",
    "                             load_event=True, transform=composed)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, # Random crop will inflate the minibatch size\n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for k in range(5):\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        sample_batched['signal'].to(device)\n",
    "        sample_batched['age'].to(device)\n",
    "        sample_batched['class_label'].to(device)\n",
    "        \n",
    "# pprint.pprint(sample_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation, test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        num_workers=num_workers, \n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=eeg_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=32, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory,\n",
    "                         collate_fn=eeg_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5979e+00,  2.7526e+00,  2.8651e+00,  ...,  1.9264e-01,\n",
      "           1.9264e-01,  1.6451e-01],\n",
      "         [ 1.8266e+00,  1.9315e+00,  2.1762e+00,  ...,  2.1836e-01,\n",
      "           1.8340e-01,  1.1348e-01],\n",
      "         [-3.1936e-01, -1.3874e-01,  4.0313e-01,  ..., -1.0419e+00,\n",
      "          -1.0419e+00, -9.5154e-01],\n",
      "         ...,\n",
      "         [-1.7913e+00, -1.7913e+00, -1.6425e+00,  ..., -1.0472e+00,\n",
      "          -8.9843e-01, -7.4962e-01],\n",
      "         [-2.1856e+00, -2.2934e+00, -2.2934e+00,  ..., -1.5386e+00,\n",
      "          -1.5386e+00, -1.4307e+00],\n",
      "         [-3.3934e-01, -3.3934e-01, -6.7292e-01,  ...,  1.3286e+00,\n",
      "          -3.3934e-01, -1.3401e+00]],\n",
      "\n",
      "        [[-1.2820e-01, -1.4334e-01, -1.7362e-01,  ..., -1.3546e+00,\n",
      "          -1.2941e+00, -1.2638e+00],\n",
      "         [ 2.9552e-01,  2.2524e-01,  1.9010e-01,  ..., -3.3703e-01,\n",
      "          -4.7759e-01, -4.7759e-01],\n",
      "         [ 5.0425e-01,  3.4115e-01,  3.4115e-01,  ...,  5.8580e-01,\n",
      "           5.8580e-01,  6.6734e-01],\n",
      "         ...,\n",
      "         [ 1.6797e+00,  1.5358e+00,  1.2480e+00,  ...,  5.2844e-01,\n",
      "           6.7235e-01,  8.1626e-01],\n",
      "         [ 7.9493e-01,  6.8919e-01,  4.7771e-01,  ...,  1.9581e+00,\n",
      "           1.8523e+00,  1.8523e+00],\n",
      "         [ 4.0697e-03, -3.0898e-01, -6.2204e-01,  ...,  1.2563e+00,\n",
      "           9.4323e-01, -3.0898e-01]],\n",
      "\n",
      "        [[-5.5527e-01, -4.8018e-01, -4.0510e-01,  ...,  1.9558e-01,\n",
      "           4.5407e-02, -2.9677e-02],\n",
      "         [-2.8040e-01, -1.7627e-01, -7.2137e-02,  ...,  9.6918e-01,\n",
      "           8.6505e-01,  6.5678e-01],\n",
      "         [-1.2509e+00, -1.4125e+00, -1.0892e+00,  ...,  2.3064e+00,\n",
      "           2.4681e+00,  2.4681e+00],\n",
      "         ...,\n",
      "         [-9.8015e-01, -1.4519e+00, -1.4519e+00,  ...,  2.9511e+00,\n",
      "           2.7939e+00,  2.4794e+00],\n",
      "         [-5.5583e-01, -7.0810e-01, -1.0126e+00,  ...,  1.8805e+00,\n",
      "           2.1851e+00,  2.3373e+00],\n",
      "         [ 6.8347e-01,  7.3119e-01,  5.0852e-01,  ...,  8.7433e-01,\n",
      "           9.3795e-01,  9.2205e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 9.5210e-01,  9.5210e-01,  9.5210e-01,  ..., -4.7636e+00,\n",
      "          -4.7636e+00, -4.7824e+00],\n",
      "         [-6.8570e-02, -5.0413e-02, -5.0413e-02,  ..., -6.6595e+00,\n",
      "          -6.6777e+00, -6.6959e+00],\n",
      "         [ 9.4222e-01,  9.4222e-01,  9.4222e-01,  ..., -6.6577e+00,\n",
      "          -6.6820e+00, -6.7063e+00],\n",
      "         ...,\n",
      "         [ 2.1344e-01,  1.9152e-01,  1.9152e-01,  ..., -6.7326e+00,\n",
      "          -6.7545e+00, -6.7764e+00],\n",
      "         [ 1.1475e+00,  1.1475e+00,  1.1475e+00,  ..., -6.5064e+00,\n",
      "          -6.5064e+00, -6.5310e+00],\n",
      "         [ 1.0098e-01,  7.3906e-02,  4.1419e-02,  ..., -3.1593e-01,\n",
      "          -2.8345e-01, -2.1847e-01]],\n",
      "\n",
      "        [[-2.6802e-01, -2.6802e-01, -2.5423e-01,  ...,  3.9829e-01,\n",
      "           4.2586e-01,  3.5693e-01],\n",
      "         [-2.4654e-01, -2.1960e-01, -2.7348e-01,  ...,  3.9559e+00,\n",
      "           3.5518e+00,  3.6865e+00],\n",
      "         [-1.3650e-01, -3.0468e-01, -3.0468e-01,  ..., -4.6773e+00,\n",
      "          -4.5091e+00, -5.0137e+00],\n",
      "         ...,\n",
      "         [ 3.8017e-01,  4.3417e-01,  3.8017e-01,  ..., -2.8599e+00,\n",
      "          -2.9679e+00, -3.1299e+00],\n",
      "         [ 5.4143e-01,  5.4143e-01,  4.1980e-01,  ..., -4.4456e+00,\n",
      "          -4.3848e+00, -4.3848e+00],\n",
      "         [-1.1264e+00, -9.0033e-01, -6.7426e-01,  ...,  2.3003e-01,\n",
      "          -4.4819e-01, -6.7426e-01]],\n",
      "\n",
      "        [[ 2.2951e-01, -5.9454e-02, -2.0394e-01,  ...,  1.1686e+00,\n",
      "           1.0242e+00,  8.7968e-01],\n",
      "         [ 2.2907e-01, -3.9634e-02, -3.9634e-02,  ...,  1.3039e+00,\n",
      "           1.3039e+00,  1.3039e+00],\n",
      "         [-3.7080e-01, -6.5913e-01, -6.5913e-01,  ...,  2.0800e+00,\n",
      "           2.2242e+00,  2.2242e+00],\n",
      "         ...,\n",
      "         [-1.0197e+00, -5.6692e-01, -2.6509e-01,  ...,  3.6749e-02,\n",
      "           4.8950e-01,  7.9134e-01],\n",
      "         [ 2.1628e-01,  3.1902e-01,  6.2723e-01,  ...,  4.2176e-01,\n",
      "           3.1902e-01,  3.1902e-01],\n",
      "         [-9.9502e-01, -9.9502e-01, -1.3270e+00,  ..., -1.3270e+00,\n",
      "          -9.9502e-01, -1.3270e+00]]], device='cuda:0')\n",
      "tensor([-2.4162, -2.4162,  1.7896,  1.7896,  0.0873,  0.0873,  0.4878,  0.4878,\n",
      "         0.6881,  0.6881,  0.9885,  0.9885,  0.7882,  0.7882,  1.3891,  1.3891,\n",
      "         0.5879,  0.5879,  0.9885,  0.9885, -1.8154, -1.8154,  0.9885,  0.9885,\n",
      "        -0.2132, -0.2132,  0.4878,  0.4878,  0.2875,  0.2875,  0.0873,  0.0873,\n",
      "        -0.6137, -0.6137,  1.6895,  1.6895, -2.0156, -2.0156,  1.6895,  1.6895,\n",
      "         1.1888,  1.1888,  0.7882,  0.7882,  0.2875,  0.2875, -1.5149, -1.5149,\n",
      "         0.1874,  0.1874, -1.3147, -1.3147, -0.5136, -0.5136,  0.1874,  0.1874,\n",
      "         0.7882,  0.7882, -1.8154, -1.8154, -0.8140, -0.8140,  0.9885,  0.9885],\n",
      "       device='cuda:0')\n",
      "tensor([0, 0, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0,\n",
      "        2, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_i, sample_batched in enumerate(train_loader):\n",
    "    # pull up the batch data\n",
    "    x = sample_batched['signal'].to(device)\n",
    "    age = sample_batched['age'].to(device)\n",
    "    target = sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(x)\n",
    "    print(age)\n",
    "    print(target)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}