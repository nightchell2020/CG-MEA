{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "PyTorch의 EEG 데이터를 Dataset class 및 DataLoader class로 처리해보는 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# custom package\n",
    "from utils.eeg_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # cleaner text\n",
    "\n",
    "plt.style.use('default') \n",
    "# ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', \n",
    "#  'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', \n",
    "#  'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', \n",
    "#  'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', \n",
    "#  'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n",
    "\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic' # for Hangul in Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.9.0\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "root_path = r'dataset/02_Curated_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 78,\n",
      " 'birth': '1940-06-02',\n",
      " 'dx1': 'mci_rf',\n",
      " 'edfname': '00001809_261018',\n",
      " 'events': [[0, 'Start Recording'],\n",
      "            [0, 'New Montage - Montage 002'],\n",
      "            [36396, 'Eyes Open'],\n",
      "            [72518, 'Eyes Closed'],\n",
      "            [73862, 'Eyes Open'],\n",
      "            [75248, 'Eyes Closed'],\n",
      "            [76728, 'swallowing'],\n",
      "            [77978, 'Eyes Open'],\n",
      "            [79406, 'Eyes Closed'],\n",
      "            [79996, 'Photic On - 3.0 Hz'],\n",
      "            [80288, 'Eyes Open'],\n",
      "            [81296, 'Eyes Closed'],\n",
      "            [82054, 'Photic Off'],\n",
      "            [84070, 'Photic On - 6.0 Hz'],\n",
      "            [84488, 'Eyes Open'],\n",
      "            [85538, 'Eyes Closed'],\n",
      "            [86086, 'Photic Off'],\n",
      "            [88144, 'Photic On - 9.0 Hz'],\n",
      "            [90160, 'Photic Off'],\n",
      "            [91458, 'Eyes Open'],\n",
      "            [92218, 'Photic On - 12.0 Hz'],\n",
      "            [92762, 'Eyes Closed'],\n",
      "            [94198, 'Photic Off'],\n",
      "            [94742, 'Eyes Open'],\n",
      "            [95708, 'Eyes Closed'],\n",
      "            [96256, 'Photic On - 15.0 Hz'],\n",
      "            [98272, 'Photic Off'],\n",
      "            [100330, 'Photic On - 18.0 Hz'],\n",
      "            [102346, 'Photic Off'],\n",
      "            [102596, 'Eyes Open'],\n",
      "            [103856, 'Eyes Closed'],\n",
      "            [104361, 'Photic On - 21.0 Hz'],\n",
      "            [106420, 'Photic Off'],\n",
      "            [106880, 'Eyes Open'],\n",
      "            [107804, 'Eyes Closed'],\n",
      "            [108435, 'Photic On - 24.0 Hz'],\n",
      "            [110452, 'Photic Off'],\n",
      "            [111080, 'Eyes Open'],\n",
      "            [112004, 'Eyes Closed'],\n",
      "            [112509, 'Photic On - 27.0 Hz'],\n",
      "            [114528, 'Photic Off'],\n",
      "            [114864, 'Eyes Open'],\n",
      "            [116124, 'Eyes Closed'],\n",
      "            [116544, 'Photic On - 30.0 Hz'],\n",
      "            [118602, 'Photic Off'],\n",
      "            [126672, 'artifact'],\n",
      "            [134030, 'Move'],\n",
      "            [135584, 'Eyes Open'],\n",
      "            [136668, 'Eyes Closed'],\n",
      "            [139818, 'Eyes Open'],\n",
      "            [141414, 'Eyes Closed'],\n",
      "            [145000, 'Paused']],\n",
      " 'label': ['mci', 'mci_amnestic', 'mci_amnestic_rf'],\n",
      " 'record': '2018-10-26T15:46:26',\n",
      " 'serial': '00001'}\n"
     ]
    }
   ],
   "source": [
    "meta_path = os.path.join(root_path, 'metadata_debug.json')\n",
    "with open(meta_path, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "pprint.pprint(metadata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data Filtering by Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Vascular Dementia, Non-Vascular MCI, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label_to_type: ['Normal', 'Non-vascular MCI', 'Non-vascular dementia']\n"
     ]
    }
   ],
   "source": [
    "diagnosis_filter = [\n",
    "    # Normal\n",
    "    {'type': 'Normal',\n",
    "     'include': ['normal'], \n",
    "     'exclude': []},\n",
    "    # Non-vascular MCI\n",
    "    {'type': 'Non-vascular MCI',\n",
    "     'include': ['mci'], \n",
    "     'exclude': ['mci_vascular']},\n",
    "    # Non-vascular dementia\n",
    "    {'type': 'Non-vascular dementia',\n",
    "     'include': ['dementia'], \n",
    "     'exclude': ['vd']},\n",
    "]\n",
    "\n",
    "def generate_class_label(label):\n",
    "    for c, f in enumerate(diagnosis_filter):\n",
    "        # inc = set(f['include']) & set(label) == set(f['include'])\n",
    "        inc = len(set(f['include']) & set(label)) > 0        \n",
    "        exc = len(set(f['exclude']) & set(label)) == 0\n",
    "        if  inc and exc:\n",
    "            return (c, f['type'])\n",
    "    return (-1, 'The others')\n",
    "\n",
    "class_label_to_type = [d_f['type'] for d_f in diagnosis_filter]\n",
    "print('class_label_to_type:', class_label_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 463 data belonging to Normal\n",
      "- There are 347 data belonging to Non-vascular MCI\n",
      "- There are 229 data belonging to Non-vascular dementia\n"
     ]
    }
   ],
   "source": [
    "splitted_metadata = [[] for i in diagnosis_filter]\n",
    "\n",
    "for m in metadata:\n",
    "    c, n = generate_class_label(m['label'])\n",
    "    if c >= 0:\n",
    "        m['class_type'] = n\n",
    "        m['class_label'] = c\n",
    "        splitted_metadata[c].append(m)\n",
    "        \n",
    "for i, split in enumerate(splitted_metadata):\n",
    "    if len(split) == 0:\n",
    "        print(f'(Warning) Split group {i} has no data.')\n",
    "    else:\n",
    "        print(f'- There are {len(split):} data belonging to {split[0][\"class_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Configure the Train, Validation, and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the filtered dataset and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size\t\t: 831\n",
      "Validation data size\t: 104\n",
      "Test data size\t\t: 104\n",
      "\n",
      " --- Recheck --- \n",
      "\n",
      "Train data label distribution\t: [370 278 183] 831\n",
      "Val data label distribution\t: [46 35 23] 104\n",
      "Test data label distribution\t: [47 34 23] 104\n"
     ]
    }
   ],
   "source": [
    "# Train : Val : Test = 8 : 1 : 1\n",
    "ratio1 = 0.8\n",
    "ratio2 = 0.1\n",
    "\n",
    "metadata_train = []\n",
    "metadata_val = []\n",
    "metadata_test = []\n",
    "\n",
    "for split in splitted_metadata:\n",
    "    random.shuffle(split)\n",
    "    \n",
    "    n1 = round(len(split) * ratio1)\n",
    "    n2 = n1 + round(len(split) * ratio2)\n",
    "\n",
    "    metadata_train.extend(split[:n1])\n",
    "    metadata_val.extend(split[n1:n2])\n",
    "    metadata_test.extend(split[n2:])\n",
    "\n",
    "random.shuffle(metadata_train)\n",
    "random.shuffle(metadata_val)\n",
    "random.shuffle(metadata_test)\n",
    "\n",
    "print('Train data size\\t\\t:', len(metadata_train))\n",
    "print('Validation data size\\t:', len(metadata_val))\n",
    "print('Test data size\\t\\t:', len(metadata_test))\n",
    "\n",
    "print('\\n', '--- Recheck ---', '\\n')\n",
    "train_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_train:\n",
    "    train_class_nums[m['class_label']] += 1\n",
    "\n",
    "val_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_val:\n",
    "    val_class_nums[m['class_label']] += 1\n",
    "\n",
    "test_class_nums = np.zeros((len(class_label_to_type)), dtype=np.int32)\n",
    "for m in metadata_test:\n",
    "    test_class_nums[m['class_label']] += 1\n",
    "\n",
    "print('Train data label distribution\\t:', train_class_nums, train_class_nums.sum())\n",
    "print('Val data label distribution\\t:', val_class_nums, val_class_nums.sum())\n",
    "print('Test data label distribution\\t:', test_class_nums, test_class_nums.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Test TorchVision Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-22. -25. -26.]\n",
      " [ -9.  -9.  -7.]\n",
      " [ -5.  -4.  -1.]\n",
      " [  3.   3.   4.]\n",
      " [  0.   0.  -2.]\n",
      " [  4.   9.  10.]\n",
      " [  9.   7.   7.]\n",
      " [  1.   2.  -1.]\n",
      " [  6.   7.   5.]\n",
      " [  6.   6.   5.]\n",
      " [ -8.  -8.  -7.]\n",
      " [-10.  -8.  -3.]\n",
      " [ -4.  -3.  -2.]\n",
      " [ -5.  -7.  -5.]\n",
      " [  5.   8.   7.]\n",
      " [  4.   6.   4.]\n",
      " [ -3.  -4.  -4.]\n",
      " [  1.  -2.  -2.]\n",
      " [  9.   7.   3.]\n",
      " [-21. -21. -18.]\n",
      " [  0.   1.   2.]]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "[[ 29.  27.  26.]\n",
      " [  3.   2.   0.]\n",
      " [ -1.  -1.  -2.]\n",
      " [  2.   2.   1.]\n",
      " [ -7.  -7.  -5.]\n",
      " [-24. -24. -27.]\n",
      " [ -6.  -7.  -7.]\n",
      " [ -4.  -4.  -2.]\n",
      " [ -7.  -6.  -6.]\n",
      " [ -8.  -8.  -9.]\n",
      " [ 28.  27.  28.]\n",
      " [ -1.   1.   0.]\n",
      " [ -5.  -5.  -3.]\n",
      " [ 19.  18.  20.]\n",
      " [ -9.  -8.  -5.]\n",
      " [ -3.  -4.  -5.]\n",
      " [  6.   4.   1.]\n",
      " [ -1.   0.  -2.]\n",
      " [ -6.  -4.  -5.]\n",
      " [ 58. 177. 197.]\n",
      " [ -1.  -1.  -1.]]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    dataset = EEGDataset(root_path, metadata_train, EEGRandomCrop(3))\n",
    "    print(dataset[0]['signal'])\n",
    "    print('\\n')\n",
    "    print('-' * 100)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization per signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'signal': array([[-9.4980073e-01, -7.6441890e-01, -5.1724309e-01, ...,\n",
      "        -9.9099672e-01, -1.0527906e+00, -1.0321927e+00],\n",
      "       [ 5.3544533e-01,  1.2499492e+00,  1.2499492e+00, ...,\n",
      "        -7.1493649e-01, -7.1493649e-01, -5.3631055e-01],\n",
      "       [ 1.5363599e+00,  1.7070744e+00,  8.5350204e-01, ...,\n",
      "        -7.0310736e-05,  1.7064416e-01,  5.1207310e-01],\n",
      "       ...,\n",
      "       [ 8.6157030e-01,  5.1706815e-01,  1.7256606e-01, ...,\n",
      "        -1.0331913e+00,  3.1498959e-04,  8.6157030e-01],\n",
      "       [-4.7229922e-01,  1.2586252e-01,  1.5734471e-01, ...,\n",
      "         1.6527491e+00,  1.5740435e+00,  1.5110791e+00],\n",
      "       [ 1.2456863e-03,  1.4145758e+00,  1.4145758e+00, ...,\n",
      "         1.2456863e-03,  7.0791072e-01, -1.4120845e+00]], dtype=float32), 'age': 56, 'class_label': 0, 'metadata': {'serial': '00774', 'edfname': '01063145_220915', 'birth': '1958-12-09', 'record': '2015-09-22T13:11:12', 'age': 56, 'dx1': 'cb_normal', 'label': ['normal', 'cb_normal'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Pz Montage'], [352, 'Eyes Open'], [5948, 'Eyes Closed'], [13634, 'Eyes Open'], [18212, 'Eyes Closed'], [24386, 'Eyes Open'], [30139, 'Eyes Closed'], [36398, 'Eyes Open'], [42152, 'Eyes Closed'], [48662, 'Eyes Open'], [54541, 'Eyes Closed'], [60254, 'Eyes Open'], [66344, 'Eyes Closed'], [72224, 'Eyes Open'], [79364, 'Eyes Closed'], [84698, 'Eyes Open'], [90074, 'Eyes Closed'], [96290, 'Eyes Open'], [102338, 'Eyes Closed'], [108470, 'Eyes Open'], [114811, 'Eyes Closed'], [120859, 'Eyes Open'], [121400, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Mean: [-2.6394940e-09  2.5138039e-10 -5.5303682e-09  7.0386506e-09\n",
      "  8.0441724e-09 -2.5138039e-10  1.4077301e-08 -2.5138038e-09\n",
      " -7.7927922e-09 -1.2569019e-10 -1.5082823e-09  5.6560587e-09\n",
      " -3.7707057e-10  4.0220862e-09 -1.2757554e-08  1.4580062e-08\n",
      "  9.8038351e-09  1.1186427e-08 -8.7983132e-09 -4.0967145e-09\n",
      "  1.5397049e-08]\n",
      "Std: [1.         0.99999994 0.99999994 1.         0.99999994 1.\n",
      " 1.         0.99999994 0.99999994 1.         0.99999994 0.99999994\n",
      " 1.         1.         1.         0.99999994 0.99999994 1.\n",
      " 0.99999994 1.         0.99999994]\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, EEGNormalizePerSignal())\n",
    "print(dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('Mean:', np.mean(dataset[0]['signal'], axis=1))\n",
    "print('Std:', np.std(dataset[0]['signal'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age mean and standard deviation:\t 70.2202166064982 ,\t 9.617642005132124\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "before:\n",
      "56\n",
      "58\n",
      "76\n",
      "59\n",
      "56\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after:\n",
      "-1.4785554072530993\n",
      "-1.2706042278628438\n",
      "0.6009563866494568\n",
      "-1.166628638167716\n",
      "-1.4785554072530993\n"
     ]
    }
   ],
   "source": [
    "ages = []\n",
    "for m in metadata_train:\n",
    "    ages.append(m['age'])\n",
    "\n",
    "ages = np.array(ages)\n",
    "age_mean = np.mean(ages)\n",
    "age_std = np.std(ages)\n",
    "\n",
    "print('Age mean and standard deviation:\\t', age_mean, ',\\t', age_std)\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('before:')\n",
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "for i in range(5):\n",
    "    print(dataset[i]['age'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print('after:')\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGNormalizeAge(mean=age_mean, std=age_std))\n",
    "for i in range(5):\n",
    "    print(dataset[i]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop EKG channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (21, 120200)\n",
      "[[-39.  -9.  -9. ...   3.   3.   1.]\n",
      " [ 36.  40.  40. ...  11.  11.   9.]\n",
      " [-32. -34. -33. ...  -6.  -7.  -6.]\n",
      " ...\n",
      " [ 13.  14.  15. ...  -2.  -1.   0.]\n",
      " [-23. -16.   7. ...   6.  35. -27.]\n",
      " [  0.   0.  -2. ...   0.   2.   2.]]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after: (20, 120200)\n",
      "[[-39.  -9.  -9. ...   3.   3.   1.]\n",
      " [ 36.  40.  40. ...  11.  11.   9.]\n",
      " [-32. -34. -33. ...  -6.  -7.  -6.]\n",
      " ...\n",
      " [ 26.  25.  26. ...   2.   3.   3.]\n",
      " [ 13.  14.  15. ...  -2.  -1.   0.]\n",
      " [  0.   0.  -2. ...   0.   2.   2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "print('before:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGDropEKGChannel())\n",
    "print('after:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop photic stimulation channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (21, 121400)\n",
      "[[-46. -37. -25. ... -48. -51. -50.]\n",
      " [  6.  14.  14. ...  -8.  -8.  -6.]\n",
      " [  9.  10.   5. ...   0.   1.   3.]\n",
      " ...\n",
      " [  5.   3.   1. ...  -6.   0.   5.]\n",
      " [-30.   8.  10. ... 105. 100.  96.]\n",
      " [  0.   2.   2. ...   0.   1.  -2.]]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "after: (20, 121400)\n",
      "[[-46. -37. -25. ... -48. -51. -50.]\n",
      " [  6.  14.  14. ...  -8.  -8.  -6.]\n",
      " [  9.  10.   5. ...   0.   1.   3.]\n",
      " ...\n",
      " [  4.   7.   5. ...  -6.  -1.  -1.]\n",
      " [  5.   3.   1. ...  -6.   0.   5.]\n",
      " [-30.   8.  10. ... 105. 100.  96.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "print('before:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGDropPhoticChannel())\n",
    "print('after:', dataset[0]['signal'].shape)\n",
    "print(dataset[0]['signal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "{'signal': array([[-46., -37., -25., ..., -48., -51., -50.],\n",
      "       [  6.,  14.,  14., ...,  -8.,  -8.,  -6.],\n",
      "       [  9.,  10.,   5., ...,   0.,   1.,   3.],\n",
      "       ...,\n",
      "       [  5.,   3.,   1., ...,  -6.,   0.,   5.],\n",
      "       [-30.,   8.,  10., ..., 105., 100.,  96.],\n",
      "       [  0.,   2.,   2., ...,   0.,   1.,  -2.]], dtype=float32), 'age': 56, 'class_label': 0, 'metadata': {'serial': '00774', 'edfname': '01063145_220915', 'birth': '1958-12-09', 'record': '2015-09-22T13:11:12', 'age': 56, 'dx1': 'cb_normal', 'label': ['normal', 'cb_normal'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Pz Montage'], [352, 'Eyes Open'], [5948, 'Eyes Closed'], [13634, 'Eyes Open'], [18212, 'Eyes Closed'], [24386, 'Eyes Open'], [30139, 'Eyes Closed'], [36398, 'Eyes Open'], [42152, 'Eyes Closed'], [48662, 'Eyes Open'], [54541, 'Eyes Closed'], [60254, 'Eyes Open'], [66344, 'Eyes Closed'], [72224, 'Eyes Open'], [79364, 'Eyes Closed'], [84698, 'Eyes Open'], [90074, 'Eyes Closed'], [96290, 'Eyes Open'], [102338, 'Eyes Closed'], [108470, 'Eyes Open'], [114811, 'Eyes Closed'], [120859, 'Eyes Open'], [121400, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "before:\n",
      "{'signal': tensor([[-46., -37., -25.,  ..., -48., -51., -50.],\n",
      "        [  6.,  14.,  14.,  ...,  -8.,  -8.,  -6.],\n",
      "        [  9.,  10.,   5.,  ...,   0.,   1.,   3.],\n",
      "        ...,\n",
      "        [  5.,   3.,   1.,  ...,  -6.,   0.,   5.],\n",
      "        [-30.,   8.,  10.,  ..., 105., 100.,  96.],\n",
      "        [  0.,   2.,   2.,  ...,   0.,   1.,  -2.]]), 'age': tensor(56.), 'class_label': tensor(0), 'metadata': {'serial': '00774', 'edfname': '01063145_220915', 'birth': '1958-12-09', 'record': '2015-09-22T13:11:12', 'age': 56, 'dx1': 'cb_normal', 'label': ['normal', 'cb_normal'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Pz Montage'], [352, 'Eyes Open'], [5948, 'Eyes Closed'], [13634, 'Eyes Open'], [18212, 'Eyes Closed'], [24386, 'Eyes Open'], [30139, 'Eyes Closed'], [36398, 'Eyes Open'], [42152, 'Eyes Closed'], [48662, 'Eyes Open'], [54541, 'Eyes Closed'], [60254, 'Eyes Open'], [66344, 'Eyes Closed'], [72224, 'Eyes Open'], [79364, 'Eyes Closed'], [84698, 'Eyes Open'], [90074, 'Eyes Closed'], [96290, 'Eyes Open'], [102338, 'Eyes Closed'], [108470, 'Eyes Open'], [114811, 'Eyes Closed'], [120859, 'Eyes Open'], [121400, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGDataset(root_path, metadata_train, None)\n",
    "print('before:')\n",
    "print(dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "dataset = EEGDataset(root_path, metadata_train, EEGToTensor())\n",
    "print('before:')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short time Fourier transform (STFT or spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 101, 2429]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[ 7.9370e+03,  5.7561e+03,  1.6963e+03,  ..., -4.3464e+00,\n",
      "         -2.4824e+00, -9.0000e+00],\n",
      "        [ 2.5200e+02,  7.9001e+02,  2.5220e+02,  ...,  6.7221e-01,\n",
      "         -1.7404e+00,  6.0000e+00],\n",
      "        [ 1.6000e+02, -8.1314e+01,  1.3739e+01,  ..., -2.4583e+00,\n",
      "          6.5933e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.2316e+02,  4.2250e+02,  ..., -3.9330e+00,\n",
      "          3.8433e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.9486e+03,  1.7935e+03,  ...,  3.8528e+00,\n",
      "          7.7595e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -5.7612e+00,  2.5108e+01,  ..., -3.8516e+00,\n",
      "          2.0368e+00,  0.0000e+00]])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([21, 101, 2429]) torch.complex64 <class 'torch.Tensor'>\n",
      "tensor([[ 7.9370e+03+0.0000e+00j,  5.7561e+03-4.0513e+03j,\n",
      "          1.6963e+03-5.5050e+03j,  ...,\n",
      "         -4.3464e+00-3.1604e-01j, -2.4824e+00+4.2163e-01j,\n",
      "         -9.0000e+00+0.0000e+00j],\n",
      "        [ 2.5200e+02+0.0000e+00j,  7.9001e+02-9.7517e+02j,\n",
      "          2.5220e+02-6.1933e+02j,  ...,\n",
      "          6.7221e-01+9.5947e-01j, -1.7404e+00+2.8607e+00j,\n",
      "          6.0000e+00+0.0000e+00j],\n",
      "        [ 1.6000e+02+0.0000e+00j, -8.1314e+01-2.4641e+02j,\n",
      "          1.3739e+01+8.7839e+01j,  ...,\n",
      "         -2.4583e+00+2.3257e+00j,  6.5933e+00-2.9987e+00j,\n",
      "          0.0000e+00+0.0000e+00j],\n",
      "        ...,\n",
      "        [ 4.6000e+01+0.0000e+00j, -3.5320e+02+1.2316e+02j,\n",
      "         -5.1123e+01+4.2250e+02j,  ...,\n",
      "         -4.4971e+00-3.9330e+00j,  8.0109e-01+3.8433e+00j,\n",
      "          0.0000e+00+0.0000e+00j],\n",
      "        [-8.1300e+02+0.0000e+00j, -1.7143e+03-1.9486e+03j,\n",
      "         -2.5812e+02+1.7935e+03j,  ...,\n",
      "         -4.6965e+01+3.8528e+00j, -5.5285e+01+7.7595e+00j,\n",
      "         -5.1000e+01+0.0000e+00j],\n",
      "        [ 1.7000e+01+0.0000e+00j, -1.2294e+01-5.7612e+00j,\n",
      "          8.2524e+00+2.5108e+01j,  ...,\n",
      "         -6.5654e+00-3.8516e+00j,  2.1084e+00+2.0368e+00j,\n",
      "         -7.0000e+00+0.0000e+00j]])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "torch.Size([21, 101, 2429]) torch.float32 <class 'torch.Tensor'>\n",
      "tensor([[ 7.9370e+03,  5.7561e+03,  1.6963e+03,  ..., -4.3464e+00,\n",
      "         -2.4824e+00, -9.0000e+00],\n",
      "        [ 2.5200e+02,  7.9001e+02,  2.5220e+02,  ...,  6.7221e-01,\n",
      "         -1.7404e+00,  6.0000e+00],\n",
      "        [ 1.6000e+02, -8.1314e+01,  1.3739e+01,  ..., -2.4583e+00,\n",
      "          6.5933e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 4.6000e+01, -3.5320e+02, -5.1123e+01,  ..., -4.4971e+00,\n",
      "          8.0109e-01,  0.0000e+00],\n",
      "        [-8.1300e+02, -1.7143e+03, -2.5812e+02,  ..., -4.6965e+01,\n",
      "         -5.5285e+01, -5.1000e+01],\n",
      "        [ 1.7000e+01, -1.2294e+01,  8.2524e+00,  ..., -6.5654e+00,\n",
      "          2.1084e+00, -7.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='as_real')])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='complex')])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='remove')])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(107175)\n",
      "tensor(633266)\n",
      "tensor(636218)\n",
      "tensor(2549400)\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGToTensor(), EEGSpectrogram(n_fft=200, complex_mode='complex')])\n",
    "dataset1 = EEGDataset(root_path, metadata_train, composed)\n",
    "\n",
    "composed = transforms.Compose([EEGToTensor()])\n",
    "dataset2 = EEGDataset(root_path, metadata_train, composed)\n",
    "\n",
    "diff = torch.istft(dataset1[0]['signal'], n_fft=200) - dataset2[0]['signal']\n",
    "print(torch.sum(diff > 1e-4))\n",
    "print(torch.sum(diff > 1e-6))\n",
    "print(torch.sum(diff > 1e-8))\n",
    "print(torch.mul(*diff.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 101, 119]) torch.complex64 <class 'torch.Tensor'>\n",
      "tensor([[-1.2514e+02+0.0000e+00j, -1.2403e+00-5.1921e+01j,\n",
      "          5.4556e+00-2.6447e+01j,  ...,\n",
      "          1.0979e+00+6.1226e-04j,  1.0772e+00-3.6137e-02j,\n",
      "          1.0872e+00+0.0000e+00j],\n",
      "        [-1.6511e+02+0.0000e+00j, -2.4113e+00+1.6019e+01j,\n",
      "         -1.3783e+01-6.9509e-01j,  ...,\n",
      "          1.1358e+00+5.4370e-02j,  9.1314e-01-5.6526e-01j,\n",
      "          1.0455e+00+0.0000e+00j],\n",
      "        [ 2.6962e+01+0.0000e+00j, -2.5124e+00-1.7830e+01j,\n",
      "         -4.1437e+01-1.7742e+01j,  ...,\n",
      "          1.5830e+00-2.2078e-02j, -9.3204e-01-4.1704e-01j,\n",
      "          1.7951e-01+0.0000e+00j],\n",
      "        ...,\n",
      "        [-6.1104e+01+0.0000e+00j, -2.2689e+01-2.1311e+01j,\n",
      "         -2.7893e-01-1.4965e+01j,  ...,\n",
      "          3.6751e-01+2.7100e-01j, -1.2049e-01+5.2408e-01j,\n",
      "          7.9562e-01+0.0000e+00j],\n",
      "        [ 1.7993e+01+0.0000e+00j,  4.1547e+01+6.5779e+00j,\n",
      "         -1.4255e+01+2.6794e+01j,  ...,\n",
      "         -1.5551e+00-1.3285e-01j, -1.6725e+00+2.3074e-01j,\n",
      "         -1.2809e+00+0.0000e+00j],\n",
      "        [ 3.8817e+00+0.0000e+00j,  1.4607e+01-2.7779e+01j,\n",
      "         -5.9121e+00+1.1079e+00j,  ...,\n",
      "         -7.1392e-01+2.5562e-02j, -6.3075e-01-1.1058e-03j,\n",
      "         -4.8926e-01+0.0000e+00j]])\n"
     ]
    }
   ],
   "source": [
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=200*60), # 1 minutes\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGToTensor(),\n",
    "                               EEGSpectrogram(n_fft=200, complex_mode='complex', hop_length=200 // 2)])\n",
    "dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "print(dataset[0]['signal'].shape, dataset[0]['signal'].dtype, type(dataset[0]['signal']))\n",
    "print(dataset[0]['signal'][:, :, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose some at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([EEGNormalizeAge(mean=age_mean, std=age_std),\n",
    "                               EEGDropPhoticChannel(),\n",
    "                               EEGRandomCrop(crop_length=200*60), # 1 minutes\n",
    "                               EEGNormalizePerSignal(),\n",
    "                               EEGToTensor()])\n",
    "\n",
    "train_dataset = EEGDataset(root_path, metadata_train, composed)\n",
    "val_dataset = EEGDataset(root_path, metadata_val, composed)\n",
    "test_dataset = EEGDataset(root_path, metadata_test, composed)\n",
    "\n",
    "print(train_dataset[0]['signal'].shape)\n",
    "print(train_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(val_dataset[0]['signal'].shape)\n",
    "print(val_dataset[0])\n",
    "\n",
    "print()\n",
    "print('-' * 100)\n",
    "print()\n",
    "\n",
    "print(test_dataset[0]['signal'].shape)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current PyTorch device:', device)\n",
    "if device.type == 'cuda':\n",
    "    num_workers = 0 # A number other than 0 causes an error\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    sample_batched['signal'].to(device)\n",
    "    sample_batched['age'].to(device)\n",
    "    sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(i_batch, \n",
    "          sample_batched['signal'].shape, \n",
    "          sample_batched['age'].shape, \n",
    "          sample_batched['class_label'].shape, \n",
    "          len(sample_batched['metadata']))\n",
    "    \n",
    "    if i_batch > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, validation, test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=pin_memory,\n",
    "                          collate_fn=eeg_collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=32, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False,\n",
    "                        num_workers=num_workers, \n",
    "                        pin_memory=pin_memory,\n",
    "                        collate_fn=eeg_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=32, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False,\n",
    "                         num_workers=num_workers, \n",
    "                         pin_memory=pin_memory,\n",
    "                         collate_fn=eeg_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i, sample_batched in enumerate(train_loader):\n",
    "    # pull up the batch data\n",
    "    x = sample_batched['signal'].to(device)\n",
    "    age = sample_batched['age'].to(device)\n",
    "    target = sample_batched['class_label'].to(device)\n",
    "    \n",
    "    print(x)\n",
    "    print(age)\n",
    "    print(target)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
