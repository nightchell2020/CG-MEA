{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Networks\n",
    "\n",
    "- Three-way SoftMax or Multi-BCE classifier of normal, non-vascular MCI, and non-vascular dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some packages\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pprint\n",
    "import wandb\n",
    "\n",
    "# custom package\n",
    "from models import *\n",
    "from utils.eeg_dataset import *\n",
    "from utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook name\n",
    "def get_notebook_name():\n",
    "    import ipynbname\n",
    "    return ipynbname.name()\n",
    "nb_fname = get_notebook_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.9.0\n",
      "cuda is available.\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available(): print('cuda is available.')\n",
    "else: print('cuda is unavailable.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Set the default configuration for building datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_data = {}\n",
    "cfg_data['device'] = device\n",
    "cfg_data['dataset'] = 'CAUHS'\n",
    "cfg_data['data_path'] = r'dataset/02_Curated_Data/'\n",
    "cfg_data['meta_path'] = os.path.join(cfg_data['data_path'], 'metadata_debug.json')\n",
    "cfg_data['target_task'] = 'Normal, MCI, Dementia' # 'Norml, MCI, Dementia'\n",
    "cfg_data['vascular'] = 'X'\n",
    "cfg_data['segment'] = 'no' # 'train', 'all', 'no'\n",
    "cfg_data['seed'] = 0\n",
    "cfg_data['crop_length'] = 200 * 10 # 10 seconds\n",
    "cfg_data['longer_crop_length'] = 200 * 10 * 10 # 100 seconds\n",
    "cfg_data['input_norm'] = 'dataset' # 'datatset', 'datapoint', 'no'\n",
    "cfg_data['EKG'] = 'O'\n",
    "cfg_data['photic'] = 'X'\n",
    "cfg_data['awgn'] = 5e-2\n",
    "cfg_data['awgn_age'] = 5e-2\n",
    "cfg_data['minibatch'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label_to_type: ['Normal', 'Non-vascular MCI', 'Non-vascular dementia']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "- There are 463 data belonging to Normal\n",
      "- There are 347 data belonging to Non-vascular MCI\n",
      "- There are 229 data belonging to Non-vascular dementia\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Train data label distribution\t: [370, 278, 183] 831\n",
      "Train data label distribution\t: [46, 35, 23] 104\n",
      "Train data label distribution\t: [47, 34, 23] 104\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "composed_train: Compose(\n",
      "    <utils.eeg_dataset.EEGRandomCrop object at 0x000002859816B700>\n",
      "    <utils.eeg_dataset.EEGNormalizeMeanStd object at 0x0000028598114A60>\n",
      "    <utils.eeg_dataset.EEGNormalizeAge object at 0x00000285A9F87A30>\n",
      "    <utils.eeg_dataset.EEGDropPhoticChannel object at 0x00000285A9F8DB50>\n",
      "    <utils.eeg_dataset.EEGAddGaussianNoise object at 0x00000285A9F8D5B0>\n",
      "    <utils.eeg_dataset.EEGAddGaussianNoiseAge object at 0x00000285A9F8DEE0>\n",
      "    <utils.eeg_dataset.EEGToTensor object at 0x00000285A9F7E340>\n",
      ")\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "composed_test: Compose(\n",
      "    <utils.eeg_dataset.EEGRandomCrop object at 0x00000285A16D0130>\n",
      "    <utils.eeg_dataset.EEGNormalizeMeanStd object at 0x000002859D5539A0>\n",
      "    <utils.eeg_dataset.EEGNormalizeAge object at 0x00000285A9F8D3A0>\n",
      "    <utils.eeg_dataset.EEGDropPhoticChannel object at 0x00000285A9F8DAF0>\n",
      "    <utils.eeg_dataset.EEGToTensor object at 0x00000285A9F7E2B0>\n",
      ")\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "longer_composed_test: Compose(\n",
      "    <utils.eeg_dataset.EEGRandomCrop object at 0x00000285A9F7E2E0>\n",
      "    <utils.eeg_dataset.EEGNormalizeMeanStd object at 0x00000285A9F7ED90>\n",
      "    <utils.eeg_dataset.EEGNormalizeAge object at 0x00000285A9F7EE50>\n",
      "    <utils.eeg_dataset.EEGDropPhoticChannel object at 0x00000285A9F7E7C0>\n",
      "    <utils.eeg_dataset.EEGToTensor object at 0x00000285A9F7E970>\n",
      ")\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "train_dataset[0]:\n",
      "torch.Size([20, 2000])\n",
      "{'signal': tensor([[ 0.9572,  0.9734,  0.9011,  ..., -3.3696, -3.4371, -3.4756],\n",
      "        [-0.5940, -0.5815, -0.5409,  ..., -2.2248, -2.0616, -1.7482],\n",
      "        [-4.8655, -4.9288, -4.9172,  ...,  1.2718,  1.4462,  1.6930],\n",
      "        ...,\n",
      "        [ 0.8947,  0.8955,  0.7543,  ...,  0.3574, -0.0099, -0.1255],\n",
      "        [ 1.3226,  1.1928,  1.0263,  ...,  0.4778,  0.3345,  0.3324],\n",
      "        [ 0.3508,  0.4633,  0.6065,  ...,  0.5152,  0.4601,  0.6867]]), 'age': tensor(-1.1559), 'class_label': tensor(0), 'metadata': {'serial': '01012', 'edfname': '01212635_270515', 'birth': '1956-06-01', 'record': '2015-05-27T09:37:24', 'age': 58, 'dx1': 'cb_normal', 'label': ['normal', 'cb_normal'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [400, 'Eyes Open'], [7918, 'Eyes Closed'], [14091, 'Eyes Open'], [18208, 'Eyes Closed'], [24256, 'Eyes Open'], [30724, 'Eyes Closed'], [36562, 'Eyes Open'], [42190, 'Eyes Closed'], [48910, 'Eyes Open'], [55126, 'Eyes Closed'], [60417, 'Eyes Open'], [66004, 'Eyes Closed'], [71968, 'Eyes Open'], [78310, 'Eyes Closed'], [84442, 'Eyes Open'], [90070, 'Eyes Closed'], [96076, 'Eyes Open'], [102082, 'Eyes Closed'], [108844, 'Eyes Open'], [113674, 'Eyes Closed'], [120000, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "val_dataset[0]:\n",
      "torch.Size([20, 2000])\n",
      "{'signal': tensor([[-5.6515e-01, -5.8776e-01, -5.8776e-01,  ...,  1.1530e+00,\n",
      "          1.1530e+00,  1.1756e+00],\n",
      "        [-5.9381e-01, -6.4323e-01, -6.9264e-01,  ...,  1.1851e+00,\n",
      "          1.1357e+00,  1.0369e+00],\n",
      "        [ 1.7862e-01,  1.7862e-01,  2.6818e-01,  ...,  7.1598e-01,\n",
      "          8.9510e-01,  1.0742e+00],\n",
      "        ...,\n",
      "        [-3.7449e-01, -4.6824e-01, -5.6198e-01,  ..., -2.8075e-01,\n",
      "         -4.6824e-01, -6.5573e-01],\n",
      "        [ 1.8666e-01,  5.5689e-03, -1.7552e-01,  ...,  9.6115e-02,\n",
      "         -1.7552e-01, -3.5661e-01],\n",
      "        [-2.2368e-01, -3.6201e-01, -3.7266e-01,  ..., -7.8854e+00,\n",
      "         -7.0979e+00, -3.7353e+00]]), 'age': tensor(0.7204), 'class_label': tensor(1), 'metadata': {'serial': '00700', 'edfname': '00985401_011117', 'birth': '1940-09-09', 'record': '2017-11-01T14:20:48', 'age': 77, 'dx1': 'mci amnestic', 'label': ['mci', 'mci_amnestic'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 002'], [1705, 'Eyes Open'], [5402, 'Eyes Closed'], [13046, 'Eyes Open'], [17666, 'Eyes Closed'], [30308, 'Eyes Closed'], [36272, 'Eyes Open'], [41774, 'Eyes Closed'], [48958, 'Eyes Open'], [55510, 'Eyes Closed'], [61641, 'Eyes Open'], [66766, 'Eyes Closed'], [72730, 'Eyes Open'], [78988, 'Eyes Closed'], [87770, 'Eyes Open'], [90542, 'Eyes Closed'], [97096, 'Eyes Open'], [102178, 'Eyes Closed'], [110872, 'Eyes Open'], [113728, 'Eyes Closed'], [122052, 'Photic On - 3.0 Hz'], [122428, 'Eyes Open'], [123309, 'Eyes Closed'], [124068, 'Photic Off'], [126126, 'Photic On - 6.0 Hz'], [128142, 'Photic Off'], [130158, 'Photic On - 9.0 Hz'], [132216, 'Photic Off'], [132718, 'Eyes Open'], [133600, 'Eyes Closed'], [134232, 'Photic On - 12.0 Hz'], [136248, 'Photic Off'], [138306, 'Photic On - 15.0 Hz'], [140322, 'Photic Off'], [142380, 'Photic On - 18.0 Hz'], [142630, 'Eyes Open'], [143302, 'Eyes Closed'], [144396, 'Photic Off'], [146412, 'Photic On - 21.0 Hz'], [148428, 'Photic Off'], [150486, 'Photic On - 24.0 Hz'], [152502, 'Photic Off'], [152710, 'Eyes Open'], [153550, 'Eyes Closed'], [154560, 'Photic On - 27.0 Hz'], [156576, 'Photic Off'], [158592, 'Photic On - 30.0 Hz'], [160608, 'Photic Off'], [160942, 'Eyes Open'], [161698, 'Eyes Closed'], [169132, 'Eyes Open'], [170098, 'Eyes Closed'], [173600, 'Paused']], 'class_type': 'Non-vascular MCI', 'class_label': 1}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "test_dataset[0]:\n",
      "torch.Size([20, 2000])\n",
      "{'signal': tensor([[-1.5599, -1.6051, -1.8086,  ..., -0.2260, -0.2260, -0.2260],\n",
      "        [-0.8409, -0.9397, -1.1374,  ..., -0.0503, -0.0997, -0.0503],\n",
      "        [ 0.7160,  0.6264,  0.5369,  ...,  0.7160,  0.5369,  0.4473],\n",
      "        ...,\n",
      "        [ 0.9379,  0.8442,  0.5630,  ...,  1.1254,  1.1254,  1.1254],\n",
      "        [ 1.2732,  1.3638,  1.2732,  ...,  0.9110,  0.9110,  1.0016],\n",
      "        [ 0.7766,  0.7340,  0.6915,  ...,  0.5957,  0.2871,  1.3193]]), 'age': tensor(1.0259), 'class_label': tensor(0), 'metadata': {'serial': '00299', 'edfname': '00671212_160819', 'birth': '1938-08-17', 'record': '2019-08-16T10:57:03', 'age': 80, 'dx1': 'smi', 'label': ['normal', 'smi'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 005'], [1773, 'Eyes Closed'], [6000, 'Cz check'], [7612, 'Eyes Open'], [12912, 'Eyes Closed'], [18078, 'Eyes Open'], [23958, 'Eyes Closed'], [29288, 'Eyes Open'], [35934, 'Eyes Closed'], [41856, 'Eyes Open'], [47862, 'Eyes Closed'], [54460, 'Eyes Open'], [59962, 'Eyes Closed'], [66178, 'Eyes Open'], [71008, 'Eyes Closed'], [73948, 'Photic On - 3.0 Hz'], [74158, 'Eyes Open'], [75166, 'Eyes Closed'], [75964, 'Photic Off'], [77980, 'Photic On - 6.0 Hz'], [78358, 'Eyes Open'], [79282, 'Eyes Closed'], [80038, 'Photic Off'], [82054, 'Photic On - 9.0 Hz'], [84070, 'Photic Off'], [86128, 'Photic On - 12.0 Hz'], [87640, 'Eyes Open'], [88144, 'Photic Off'], [88396, 'Eyes Closed'], [90202, 'Photic On - 15.0 Hz'], [92218, 'Photic Off'], [92722, 'Eyes Open'], [93772, 'Eyes Closed'], [94234, 'Photic On - 18.0 Hz'], [96250, 'Photic Off'], [98308, 'Photic On - 21.0 Hz'], [100324, 'Photic Off'], [102382, 'Photic On - 24.0 Hz'], [104398, 'Photic Off'], [106414, 'Photic On - 27.0 Hz'], [108430, 'Photic Off'], [110488, 'Photic On - 30.0 Hz'], [111580, 'Eyes Open'], [111790, 'Photic Off'], [112420, 'Eyes Closed'], [113200, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "test_dataset_longer[0]:\n",
      "torch.Size([20, 20000])\n",
      "{'signal': tensor([[ 0.1809,  0.1131,  0.1357,  ...,  0.1583,  0.1809,  0.1357],\n",
      "        [ 0.5921,  0.6415,  0.4439,  ...,  0.0486,  0.0980,  0.0980],\n",
      "        [ 0.4473,  0.5369,  0.5369,  ...,  0.3577,  0.1786,  0.2682],\n",
      "        ...,\n",
      "        [-0.3745, -0.3745, -0.1870,  ...,  0.6567,  0.5630,  0.6567],\n",
      "        [-0.2661, -0.0850,  0.1867,  ...,  0.6394,  0.5488,  0.6394],\n",
      "        [-0.9047, -1.0111, -0.7877,  ...,  5.8419,  6.5123,  7.2465]]), 'age': tensor(1.0259), 'class_label': tensor(0), 'metadata': {'serial': '00299', 'edfname': '00671212_160819', 'birth': '1938-08-17', 'record': '2019-08-16T10:57:03', 'age': 80, 'dx1': 'smi', 'label': ['normal', 'smi'], 'events': [[0, 'Start Recording'], [0, 'New Montage - Montage 005'], [1773, 'Eyes Closed'], [6000, 'Cz check'], [7612, 'Eyes Open'], [12912, 'Eyes Closed'], [18078, 'Eyes Open'], [23958, 'Eyes Closed'], [29288, 'Eyes Open'], [35934, 'Eyes Closed'], [41856, 'Eyes Open'], [47862, 'Eyes Closed'], [54460, 'Eyes Open'], [59962, 'Eyes Closed'], [66178, 'Eyes Open'], [71008, 'Eyes Closed'], [73948, 'Photic On - 3.0 Hz'], [74158, 'Eyes Open'], [75166, 'Eyes Closed'], [75964, 'Photic Off'], [77980, 'Photic On - 6.0 Hz'], [78358, 'Eyes Open'], [79282, 'Eyes Closed'], [80038, 'Photic Off'], [82054, 'Photic On - 9.0 Hz'], [84070, 'Photic Off'], [86128, 'Photic On - 12.0 Hz'], [87640, 'Eyes Open'], [88144, 'Photic Off'], [88396, 'Eyes Closed'], [90202, 'Photic On - 15.0 Hz'], [92218, 'Photic Off'], [92722, 'Eyes Open'], [93772, 'Eyes Closed'], [94234, 'Photic On - 18.0 Hz'], [96250, 'Photic Off'], [98308, 'Photic On - 21.0 Hz'], [100324, 'Photic Off'], [102382, 'Photic On - 24.0 Hz'], [104398, 'Photic Off'], [106414, 'Photic On - 27.0 Hz'], [108430, 'Photic Off'], [110488, 'Photic On - 30.0 Hz'], [111580, 'Eyes Open'], [111790, 'Photic Off'], [112420, 'Eyes Closed'], [113200, 'Paused']], 'class_type': 'Normal', 'class_label': 0}}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "0 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "1 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "2 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "3 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "4 torch.Size([32, 20, 2000]) torch.Size([32]) torch.Size([32]) 32\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = build_dataset(cfg_data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Define Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_common_model = {'in_channels': _[0].dataset[0]['signal'].shape[0], \n",
    "                    'out_dims': len(_[-1])}\n",
    "cfg_model_pool = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Model config:'\n",
      "{'LR': 0.001,\n",
      " 'base_channels': 256,\n",
      " 'dropout': 0.2,\n",
      " 'fc_stages': 2,\n",
      " 'final_pool': 'max',\n",
      " 'generator': <class 'models.transformer.CNNTransformer'>,\n",
      " 'in_channels': 20,\n",
      " 'model': '1D-CNN-Transformer',\n",
      " 'n_encoders': 4,\n",
      " 'n_heads': 4,\n",
      " 'out_dims': 3,\n",
      " 'use_age': 'fc'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CNNTransformer(\n",
      "  (conv1): Conv1d(20, 256, kernel_size=(21,), stride=(9,))\n",
      "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(9,), stride=(3,))\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv1d(256, 512, kernel_size=(9,), stride=(3,))\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv1d(512, 512, kernel_size=(9,), stride=(3,))\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (final_pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc_stage): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=513, out_features=256, bias=False)\n",
      "      (1): Dropout(p=0.2, inplace=False)\n",
      "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=False)\n",
      "      (1): Dropout(p=0.2, inplace=False)\n",
      "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_model = {}\n",
    "cfg_model.update(cfg_common_model)\n",
    "cfg_model['model'] = '1D-CNN-Transformer'\n",
    "cfg_model['generator'] = CNNTransformer\n",
    "cfg_model['activation'] = 'relu'\n",
    "cfg_model['fc_stages'] = 2\n",
    "cfg_model['use_age'] = 'fc'\n",
    "cfg_model['final_pool'] = 'max'\n",
    "cfg_model['base_channels'] = 256\n",
    "cfg_model['n_encoders'] = 4\n",
    "cfg_model['n_heads'] = 4\n",
    "cfg_model['dropout'] = 0.2\n",
    "cfg_model['LR'] = 1e-3\n",
    "\n",
    "pprint.pprint('Model config:')\n",
    "pprint.pprint(cfg_model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "model = cfg_model['generator'](**cfg_model).to(device, dtype=torch.float32)\n",
    "print(model)\n",
    "print('\\n' + '-' * 100 + '\\n')\n",
    "\n",
    "# tensorboard visualization\n",
    "# visualize_network_tensorboard(model, train_loader, device, nb_fname, '1D-Tiny-CNN-fc-age')\n",
    "\n",
    "del model\n",
    "cfg_model_pool.append(cfg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_channels': 20,\n",
      " 'out_dims': 3,\n",
      " 'model': '1D-CNN-Transformer',\n",
      " 'generator': <class 'models.transformer.CNNTransformer'>,\n",
      " 'fc_stages': 2,\n",
      " 'use_age': 'fc',\n",
      " 'final_pool': 'max',\n",
      " 'base_channels': 256,\n",
      " 'n_encoders': 4,\n",
      " 'n_heads': 4,\n",
      " 'dropout': 0.2,\n",
      " 'LR': 0.001}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cfg_model in cfg_model_pool:\n",
    "    pprint.pp(cfg_model, width=150)\n",
    "    print('\\n' + '-' * 100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Default Configurations for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configurations\n",
    "cfg_train = {}\n",
    "cfg_train['iterations'] = 100000\n",
    "cfg_train['history_interval'] = cfg_train['iterations'] // 500\n",
    "cfg_train['lr_decay_step'] = round(cfg_train['iterations'] * 0.8)\n",
    "cfg_train['lr_decay_gamma'] = 0.1\n",
    "cfg_train['weight_decay'] = 1e-2\n",
    "cfg_train['mixup'] = 0.0 # 0 for no usage\n",
    "cfg_train['criterion'] = 'cross-entropy' # 'cross-entropy', 'multi-bce'\n",
    "\n",
    "cfg_train['device'] = device\n",
    "cfg_train['save_model'] = True\n",
    "cfg_train['save_temporary'] = False\n",
    "cfg_train['draw_result'] = True\n",
    "cfg_train['watch_model'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_wandb(config, train_loader, val_loader, test_loader, test_loader_longer, class_label_to_type):\n",
    "    print('*'*120)\n",
    "    print(f'{\"*\"*30}{config[\"model\"] + \" train starts\":^60}{\"*\"*30}')\n",
    "    print('*'*120)\n",
    "\n",
    "    # generate model and its trainer        \n",
    "    model = config['generator'](**config).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=config['LR'], \n",
    "                            weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                          step_size=config['lr_decay_step'], \n",
    "                                          gamma=config['lr_decay_gamma'])\n",
    "    \n",
    "    tr_ms = train_multistep if config.get('mixup', 0) < 1e-12 else train_mixup_multistep\n",
    "    \n",
    "    # track granients and weights statistics\n",
    "    if config.get('watch_model', None):\n",
    "        wandb.watch(model, log='all', \n",
    "                    log_freq=config['history_interval'], \n",
    "                    log_graph=True)\n",
    "\n",
    "    # train and validation routine\n",
    "    best_val_acc = 0\n",
    "    for i in range(0, config[\"iterations\"], config[\"history_interval\"]):\n",
    "        # train 'history_interval' steps\n",
    "        loss, train_acc = tr_ms(model, train_loader, optimizer, scheduler, \n",
    "                                config, config[\"history_interval\"])\n",
    "\n",
    "        # validation\n",
    "        val_acc, _, _, _, _ = check_accuracy(model, val_loader, config, repeat=10)\n",
    "\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "            if config['save_model'] and config['save_temporary']:\n",
    "                save_path = f'checkpoint_temp/{wandb.run.name}/'\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                path = os.path.join(save_path, f'{config[\"model\"]}')\n",
    "                torch.save(best_model_state, path)\n",
    "\n",
    "        # log\n",
    "        wandb.log({'Loss': loss, \n",
    "                   'Train Accuracy': train_acc, \n",
    "                   'Validation Accuracy': val_acc}, step=i)\n",
    "\n",
    "    # calculate the test accuracies for best and last models\n",
    "    last_model_state = deepcopy(model.state_dict())\n",
    "    last_test_result = check_accuracy(model, test_loader, config, repeat=30)\n",
    "    last_test_acc = last_test_result[0]\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    best_test_result = check_accuracy(model, test_loader, config, repeat=30)\n",
    "    best_test_acc = best_test_result[0]\n",
    "\n",
    "    if last_test_acc < best_test_acc:\n",
    "        model_state = best_model_state\n",
    "        test_result = best_test_result\n",
    "    else:\n",
    "        model_state = last_model_state\n",
    "        test_result = last_test_result\n",
    "\n",
    "    model.load_state_dict(model_state)\n",
    "    test_acc, test_confusion, test_debug, score, target = test_result\n",
    "\n",
    "    # calculate the test accuracies for final model on much longer sequence\n",
    "    last_test_result = check_accuracy(model, test_loader_longer, config, repeat=30)\n",
    "    longer_test_acc = last_test_result[0]\n",
    "\n",
    "    # save the model\n",
    "    if config['save_model']:\n",
    "        save_path = f'checkpoint_temp/{wandb.run.name}/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        path = os.path.join(save_path, f'{config[\"model\"]}')\n",
    "        torch.save(model_state, path)\n",
    "\n",
    "    # leave the message\n",
    "    wandb.config.final_shape = model.get_final_shape()\n",
    "    wandb.config.num_params = count_parameters(model)\n",
    "    wandb.log({'Test Accuracy': test_acc,\n",
    "               '(Best / Last) Test Accuracy': ('Best' if last_test_acc < best_test_acc else 'Last', \n",
    "                                               round(best_test_acc, 2), round(last_test_acc, 2)),\n",
    "               'Confusion Matrix (Array)': test_confusion,\n",
    "               'Test Accuracy (Longer)': longer_test_acc, \n",
    "               'Test Debug Table/Serial': test_debug[0], \n",
    "               'Test Debug Table/EDF': test_debug[1], \n",
    "               'Test Debug Table/Pred': test_debug[2], \n",
    "               'Test Debug Table/GT': test_debug[3]})\n",
    "\n",
    "    if 'lr_search' in config:\n",
    "        draw_learning_rate_record(config['lr_search'], use_wandb=True)\n",
    "\n",
    "    if config['draw_result']:\n",
    "        draw_roc_curve(score, target, class_label_to_type, use_wandb=True)\n",
    "        draw_confusion(test_confusion, class_label_to_type, use_wandb=True)\n",
    "        draw_debug_table(test_debug, use_wandb=True)\n",
    "        wandb.log({\"Confusion Matrix\": wandb.plot.confusion_matrix(y_true=target, \n",
    "                                                                   preds=score.argmax(axis=-1), \n",
    "                                                                   class_names=class_label_to_type)})\n",
    "        wandb.log({\"ROC Curve\": wandb.plot.roc_curve(target, score, labels=class_label_to_type)})\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep(cfg_data, cfg_train, cfg_model_pool):\n",
    "    wandb_run = wandb.init()\n",
    "    wandb.run.name = wandb.run.id\n",
    "    with wandb_run:\n",
    "        # wandb config update\n",
    "        cfg_model = cfg_model_pool[wandb.config.model_index]\n",
    "        config = {}\n",
    "        for k, v in {**cfg_data, **cfg_train, **cfg_model}.items():\n",
    "            if k not in wandb.config:\n",
    "                config[k] = v\n",
    "        \n",
    "        # to prevent callables from type-conversion to str\n",
    "        wandb.config.update(config)\n",
    "        for k, v in wandb.config.items():\n",
    "            if k not in config:\n",
    "                config[k] = v\n",
    "                \n",
    "        # build dataset\n",
    "        train_loader, val_loader, test_loader, test_loader_longer, class_label_to_type = build_dataset(config)\n",
    "        \n",
    "        config['in_channels'] = train_loader.dataset[0]['signal'].shape[0]\n",
    "        config['out_dims'] = len(class_label_to_type)\n",
    "        \n",
    "        # learning rate search if needed\n",
    "        if config[\"LR\"] is None:\n",
    "            config['LR'], config['lr_search'] = learning_rate_search(config, train_loader, \n",
    "                                                                     min_log_lr=-4.5, max_log_lr=-3.0, \n",
    "                                                                     trials=100, steps=100)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n",
    "        # train the model\n",
    "        train_with_wandb(config, train_loader, val_loader, test_loader, test_loader_longer, class_label_to_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_data = {}\n",
    "sweep_data['crop_length'] = {\n",
    "    'values': [200 * 10, # 10 sec\n",
    "               200 * 20, # 20 sec\n",
    "              ],\n",
    "}\n",
    "\n",
    "sweep_data['EKG'] = {\n",
    "    'values': ['O', 'X'],\n",
    "}\n",
    "\n",
    "sweep_data['photic'] = {\n",
    "    'values': ['O', 'X'],\n",
    "}\n",
    "\n",
    "sweep_data['awgn'] = {\n",
    "    'distribution': 'uniform',\n",
    "    'min': 0,\n",
    "    'max': 0.3,\n",
    "}\n",
    "\n",
    "sweep_data['awgn_age'] = {\n",
    "    'distribution': 'uniform',\n",
    "    'min': 0,\n",
    "    'max': 0.3,\n",
    "}\n",
    "\n",
    "sweep_data['minibatch'] = {\n",
    "    'values': [32, ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_model = {}\n",
    "sweep_model['model_index'] = { \n",
    "    'values' : [i for i in range(len(cfg_model_pool))] \n",
    "}\n",
    "\n",
    "sweep_model['fc_stages'] = { \n",
    "    'distribution' : 'int_uniform',\n",
    "    'min': 1,\n",
    "    'max': 3,\n",
    "}\n",
    "\n",
    "sweep_model['use_age'] = { \n",
    "    'values' : ['fc', 'conv']\n",
    "}\n",
    "\n",
    "sweep_model['final_pool'] = { \n",
    "    'values' : ['max', 'average']\n",
    "}\n",
    "\n",
    "sweep_model['first_dilation'] = { \n",
    "    'distribution' : 'int_uniform',\n",
    "    'min': 1,\n",
    "    'max': 3,\n",
    "}\n",
    "\n",
    "sweep_model['base_stride'] = { \n",
    "    'distribution' : 'int_uniform',\n",
    "    'min': 2,\n",
    "    'max': 5,\n",
    "}\n",
    "\n",
    "sweep_model['dropout'] = {\n",
    "    'distribution': 'uniform',\n",
    "    'min': 0.0,\n",
    "    'max': 0.5\n",
    "}\n",
    "\n",
    "sweep_model['base_channels'] = {\n",
    "    'values' : [128, 192, 256]\n",
    "}\n",
    "\n",
    "sweep_model['n_encoders'] = {\n",
    "    'distribution': 'int_uniform',\n",
    "    'min': 2,\n",
    "    'max': 5\n",
    "}\n",
    "\n",
    "sweep_model['n_heads'] = {\n",
    "    'values' : [2, 4, 8]\n",
    "}\n",
    "\n",
    "sweep_model['LR'] = {\n",
    "    'distribution': 'log_uniform',\n",
    "    'min': math.log(5e-5),\n",
    "    'max': math.log(1e-3)\n",
    "}\n",
    "\n",
    "sweep_model['activation'] = {\n",
    "    'values' : ['relu', 'gelu', 'mish']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_train = {}\n",
    "sweep_train['iterations'] = {\n",
    "    'values' : [100000, 150000, 200000]\n",
    "}\n",
    "\n",
    "sweep_train['lr_decay_gamma'] = {\n",
    "    'distribution' : 'uniform',\n",
    "    'min': 0.1,\n",
    "    'max': 0.5,\n",
    "}\n",
    "\n",
    "# sweep_train['lr_decay_step'] = {\n",
    "#     'values' : [80000]\n",
    "# }\n",
    "\n",
    "sweep_train['weight_decay'] = {\n",
    "    'distribution' : 'log_uniform',\n",
    "    'min': math.log(1e-5),\n",
    "    'max': math.log(1e-1)\n",
    "}\n",
    "\n",
    "sweep_train['mixup'] = {\n",
    "    'values': [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "sweep_train['criterion'] = {\n",
    "    'values': ['cross-entropy', 'multi-bce']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: enqavg1n\n",
      "Sweep URL: https://wandb.ai/ipis-mjkim/eeg-analysis/sweeps/enqavg1n\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    \"entity\": \"ipis-mjkim\",\n",
    "    \"name\" : \"my-sweep\",\n",
    "    \"method\" : \"random\",\n",
    "    \"parameters\" : \n",
    "    {\n",
    "        **sweep_data,\n",
    "        **sweep_model,\n",
    "        **sweep_train,\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"eeg-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0m2tmew5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tEKG: X\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLR: 0.00045065578408069675\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tawgn: 0.17193040492994616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tawgn_age: 0.2504089164293522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_channels: 192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_stride: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: cross-entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcrop_length: 2000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.30105002199690745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_stages: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfinal_pool: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfirst_dilation: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \titerations: 100000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_decay_gamma: 0.15239761252230455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tminibatch: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_index: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_encoders: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphotic: O\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_age: fc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00017226845522933776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mipis-mjkim\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ipis-mjkim/eeg-analysis/runs/0m2tmew5\" target=\"_blank\">fresh-sweep-1</a></strong> to <a href=\"https://wandb.ai/ipis-mjkim/eeg-analysis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/ipis-mjkim/eeg-analysis/sweeps/enqavg1n\" target=\"_blank\">https://wandb.ai/ipis-mjkim/eeg-analysis/sweeps/enqavg1n</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************************************************************************\n",
      "******************************              1D-CNN-Transformer train starts               ******************************\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=lambda: train_sweep(cfg_data, cfg_train, cfg_model_pool), count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
